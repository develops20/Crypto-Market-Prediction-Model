{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# DRW Crypto Market Prediction Competition Pipeline\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os # Import os for path checking\n\n# Memory optimization and data processing\nimport gc\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\n# Removed Dask: import dask.dataframe as dd \n\n# Deep learning and modeling \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (LSTM, ConvLSTM2D, Dense, Dropout, \n                                     BatchNormalization, Input, Conv1D, MaxPooling1D,\n                                     Flatten, Reshape, TimeDistributed)\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\n# Tree-based models for ensemble\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\n# Set memory growth for GPU (if available) - still good practice\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\nclass CryptoMarketPredictor:\n    def __init__(self, sequence_length=30, top_features=100, top_X_features_to_preselect=30): \n        self.sequence_length = sequence_length \n        self.top_features = top_features \n        self.top_X_features_to_preselect = top_X_features_to_preselect \n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None # Stores final selected feature names\n        self.models = {}\n        # Re-added checkpoint path for robustness, now for Pandas DataFrames\n        self._checkpoint_path = './processed_train_data_checkpoint.parquet' \n        \n    def optimize_memory(self, df):\n        \"\"\"\n        Optimize Pandas DataFrame memory usage and clean data.\n        \"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        \n        # Clean data first\n        df = self.clean_data(df)\n        \n        # Optimize numeric columns (Pandas directly)\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col == 'timestamp' or col == 'ID' or col == 'label':\n                continue # Don't downcast timestamp, ID, or label\n            df[col] = pd.to_numeric(df[col], downcast='float')\n            \n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n    \n    def clean_data(self, df):\n        \"\"\"\n        Clean Pandas DataFrame by handling inf, -inf, and extreme values.\n        \"\"\"\n        print(\"Cleaning data...\")\n        \n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        # Replace inf and -inf with NaN first\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n        # Fill NaN values with forward fill, then backward fill, then 0\n        for col in numeric_cols:\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        # Handle extreme outliers (values beyond 3*IQR)\n        for col in df.select_dtypes(include=[np.float32, np.float64]).columns:\n            if col in ['timestamp', 'ID', 'label']: continue \n            if df[col].nunique() > 1:\n                q25 = df[col].quantile(0.25)\n                q75 = df[col].quantile(0.75)\n                iqr = q75 - q25\n                \n                if iqr != 0 and not pd.isna(iqr):\n                    lower_bound = q25 - 3 * iqr\n                    upper_bound = q75 + 3 * iqr\n                    df[col] = df[col].clip(lower_bound, upper_bound)\n        print(\"Data cleaning applied.\")\n        return df\n    \n    def create_time_features(self, df):\n        \"\"\"\n        Create time-based features with robust calculations using Pandas.\n        Significantly reduced complexity for faster execution.\n        \"\"\"\n        print(\"Creating time-based features...\")\n        \n        # Basic market features\n        df['mid_price'] = (df['bid_qty'] + df['ask_qty']) / 2\n        df['spread'] = df['ask_qty'] - df['bid_qty']\n        \n        # Safe division for imbalance\n        denominator = df['bid_qty'] + df['ask_qty'] + 1e-10\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n        \n        # Safe division for buy/sell ratio\n        df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-10)\n        \n        # Rolling statistics - significantly reduced windows for speed\n        windows = [10, 30] \n        base_cols_for_rolling = ['volume', 'mid_price', 'buy_qty', 'sell_qty', 'imbalance'] \n\n        for col in base_cols_for_rolling:\n            for window in windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0) \n        \n        # Lagged features - significantly reduced lags for speed\n        lags = [1, 5] \n        base_cols_for_lag = ['mid_price', 'imbalance'] \n\n        for col in base_cols_for_lag:\n            for lag in lags:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n        \n        # Technical indicators - reduced\n        df['rsi_proxy'] = self.calculate_rsi_proxy(df['mid_price'], window=10) \n        df['momentum'] = df['mid_price'] - df['mid_price'].shift(5) \n        \n        # Final check for any inf/nan values that might have been introduced\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        print(f\"Time-based features created. Current shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n        return df\n    \n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate RSI-like indicator with safe operations for Pandas Series.\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n        \n        rs = gain / (loss + 1e-10)\n        rsi = 100 - (100 / (1 + rs))\n        \n        rsi = rsi.replace([np.inf, -np.inf], np.nan).fillna(50)\n        \n        return rsi\n    \n    def select_features(self, X_df, y_df, method='mutual_info'):\n        \"\"\"\n        Feature selection to reduce dimensionality with robust handling.\n        Operates directly on Pandas DataFrames.\n        \"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n        \n        print(\"Validating data before final feature selection...\")\n        \n        # Check for any remaining inf/nan values\n        inf_mask = np.isinf(X_df).any(axis=1)\n        nan_mask = np.isnan(X_df).any(axis=1)\n        invalid_mask = inf_mask | nan_mask\n        \n        if invalid_mask.sum() > 0:\n            print(f\"Removing {invalid_mask.sum()} rows with invalid values before final selection.\")\n            X_df = X_df[~invalid_mask]\n            y_df = y_df[~invalid_mask]\n        \n        # Check for constant or near-constant features (can cause issues for some selectors)\n        feature_std = X_df.std()\n        constant_features_mask = feature_std < 1e-8 \n        \n        if constant_features_mask.all():\n            print(\"Warning: All features are constant. Cannot perform feature selection.\")\n            non_constant_features = X_df.columns.tolist() \n        else:\n            non_constant_features = X_df.columns[~constant_features_mask].tolist()\n            if constant_features_mask.sum() > 0:\n                print(f\"Removing {constant_features_mask.sum()} constant features.\")\n            X_df = X_df[non_constant_features]\n\n\n        print(f\"Final data shape for feature selection: {X_df.shape}\")\n        \n        n_features_to_select = min(self.top_features, X_df.shape[1])\n        \n        if method == 'mutual_info':\n            selector = SelectKBest(score_func=mutual_info_regression, k=n_features_to_select)\n        else:\n            selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n        \n        X_selected = selector.fit_transform(X_df, y_df)\n        self.feature_selector = selector\n        self.selected_features = X_df.columns[selector.get_support()].tolist()\n        \n        print(f\"\\n--- Selected Features ({len(self.selected_features)}) ---\")\n        for feature in self.selected_features:\n            print(f\"- {feature}\")\n        print(\"---------------------------------------\\n\")\n\n        # Return as NumPy array and Pandas Index to maintain alignment with y\n        return X_selected.astype(np.float32), X_df.index\n    \n    def prepare_sequences(self, data, target=None):\n        \"\"\"Prepare sequences for time series models\"\"\"\n        sequences = []\n        targets = []\n        \n        for i in range(self.sequence_length, len(data)):\n            sequences.append(data[i-self.sequence_length:i])\n            if target is not None:\n                targets.append(target[i])\n        \n        # Ensure outputs are float32 for TensorFlow models to save memory\n        return np.array(sequences).astype(np.float32), np.array(targets).astype(np.float32) if target is not None else None\n    \n    def build_convlstm_model(self, input_shape):\n        \"\"\"Build ConvLSTM model for spatial-temporal patterns\"\"\"\n        model = Sequential([\n            # Reshape for ConvLSTM (samples, time_steps, rows, cols, channels)\n            # input_shape is (sequence_length, num_features)\n            # We need (sequence_length, 1, num_features, 1)\n            Reshape((self.sequence_length, 1, input_shape[1], 1), input_shape=input_shape),\n            \n            ConvLSTM2D(filters=64, kernel_size=(1, 3), \n                       activation='tanh', recurrent_activation='sigmoid',\n                       return_sequences=True, dropout=0.2, padding='same'), # Added padding='same'\n            BatchNormalization(),\n            \n            ConvLSTM2D(filters=32, kernel_size=(1, 3),\n                       activation='tanh', recurrent_activation='sigmoid',\n                       return_sequences=False, dropout=0.2, padding='same'), # Added padding='same'\n            BatchNormalization(),\n            \n            Flatten(),\n            Dense(50, activation='relu'),\n            Dropout(0.3),\n            Dense(1, activation='linear')\n        ])\n        \n        model.compile(optimizer=Adam(learning_rate=0.001), \n                      loss='mae', metrics=['mae'])\n        return model\n    \n    def build_lstm_model(self, input_shape):\n        \"\"\"Build standard LSTM model\"\"\"\n        model = Sequential([\n            LSTM(100, return_sequences=True, input_shape=input_shape, dropout=0.2),\n            BatchNormalization(),\n            LSTM(50, return_sequences=False, dropout=0.2),\n            BatchNormalization(),\n            Dense(50, activation='relu'),\n            Dropout(0.3),\n            Dense(1, activation='linear')\n        ])\n        \n        model.compile(optimizer=Adam(learning_rate=0.001), \n                      loss='mae', metrics=['mae'])\n        return model\n\n    def build_cnn_lstm_model(self, input_shape):\n        print(\"CNN-LSTM model building skipped for current speed optimization, but can be reinstated later.\")\n        return None \n    \n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train LightGBM model\"\"\"\n        print(\"Training LightGBM model...\")\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n        \n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 1000 # Increased estimators for potentially better learning\n        }\n        \n        model = lgb.train(\n            params,\n            train_data,\n            valid_sets=[val_data],\n            num_boost_round=params['n_estimators'], \n            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n        )\n        \n        return model\n    \n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance\"\"\"\n        mae = mean_absolute_error(y_true, y_pred)\n        correlation, _ = pearsonr(y_true, y_pred)\n        \n        print(f\"{model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n    \n    def fit(self, train_data_raw_initial_load): \n        \"\"\"Main training pipeline with robust error handling\"\"\"\n        print(\"Starting training pipeline...\")\n        \n        # Determine raw X_n columns from the initial load structure\n        X_n_cols_raw = [col for col in train_data_raw_initial_load.columns if col.startswith('X') and col != 'label']    \n        basic_features_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n        \n        # --- Aggressive initial column selection for Pandas memory management ---\n        # We will ONLY select these columns from the already loaded raw DataFrame\n        preselected_X_n_features = X_n_cols_raw[:min(self.top_X_features_to_preselect, len(X_n_cols_raw))]\n        print(f\"Initially selected {len(preselected_X_n_features)} X_n features for direct Pandas load (due to memory constraints without Dask).\")\n\n        # Ensure 'timestamp' and 'label' are in columns_to_process for the fit method\n        columns_to_process_raw_for_fit = basic_features_cols + preselected_X_n_features + ['label']\n        \n        train_df = None\n        if os.path.exists(self._checkpoint_path):\n            print(f\"Checkpoint found. Loading processed data from {self._checkpoint_path}...\")\n            train_df = pd.read_parquet(self._checkpoint_path)\n            # Ensure timestamp is datetime for Pandas DataFrame if not already\n            if 'timestamp' in train_df.columns:\n                train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n        else:\n            print(f\"Selecting columns from initial raw training data...\")\n            # Select columns directly from the already loaded train_data_raw_initial_load\n            # This is where the fix is: train_data_raw_initial_load is already full, so 'timestamp' is a column here\n            train_df = train_data_raw_initial_load[columns_to_process_raw_for_fit].copy()\n            \n            # Ensure 'timestamp' is a column if it was index (less likely with initial full load, but safe check)\n            if 'timestamp' not in train_df.columns and train_df.index.name == 'timestamp':\n                train_df = train_df.reset_index()\n\n            # Ensure timestamp is datetime\n            train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n            \n            # Cleanup initial raw load (as it's now copied/subsetted)\n            # No need to del train_data_raw_initial_load here, it's passed as arg\n            gc.collect()\n\n            # Memory optimization and data cleaning (now on Pandas DataFrame)\n            train_df = self.optimize_memory(train_df)    \n            \n            # Feature engineering (now on Pandas DataFrame)\n            train_df = self.create_time_features(train_df)\n            \n            print(f\"Saving processed data to checkpoint: {self._checkpoint_path}...\")\n            train_df.to_parquet(self._checkpoint_path, index=False) # Save checkpoint, index=False to avoid writing Pandas index\n            print(\"Checkpoint saved.\")\n            \n        print(f\"Data shape after feature engineering: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n        \n        # Prepare features and target for final selection and modeling\n        feature_cols_final = [col for col in train_df.columns \n                              if col not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols_final]\n        y_df = train_df['label']\n        \n        print(f\"Features shape before final selection: {X_df.shape[0]} rows, {X_df.shape[1]} columns\")\n        print(f\"Target shape: {y_df.shape[0]} rows\")\n        \n        # Cleanup df reference\n        del train_df # Only delete the train_df here as X_df and y_df are derived from it\n        gc.collect()\n\n        # Feature selection with robust handling (operates on Pandas DataFrames)\n        X_selected, valid_idx = self.select_features(X_df, y_df)    \n        y_for_training = y_df.loc[valid_idx].astype(np.float32)\n\n        del X_df, y_df # Clean up X_df and y_df after feature selection\n        gc.collect()\n\n        # Scale features\n        X_scaled = self.scaler.fit_transform(X_selected)\n        \n        # Final validation of X_scaled\n        print(\"Final data validation (after scaling)...\")\n        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n            print(\"ERROR: Still have invalid values after preprocessing! Applying emergency cleanup.\")\n            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        \n        print(f\"Final training data shape: {X_scaled.shape}\")\n        \n        # Time-based split (use last 2-3 months for validation)\n        # We need original timestamps to split; use the main train file's timestamps\n        temp_train_full_timestamps = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet', columns=['timestamp'])\n        if 'timestamp' not in temp_train_full_timestamps.columns and temp_train_full_timestamps.index.name == 'timestamp':\n            temp_train_full_timestamps = temp_train_full_timestamps.reset_index()\n        temp_train_full_timestamps['timestamp'] = pd.to_datetime(temp_train_full_timestamps['timestamp'])\n\n        # Global VALIDATION_SPLIT_DATE for consistency\n        VALIDATION_SPLIT_DATE = '2024-01-01'\n\n        # Filter the original indices based on the validation split date\n        original_train_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] < VALIDATION_SPLIT_DATE].index\n        original_val_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] >= VALIDATION_SPLIT_DATE].index\n\n        # Create a temporary DataFrame from X_scaled to use .loc with original indices\n        # This uses X's original index (valid_idx from select_features)\n        X_scaled_temp_df = pd.DataFrame(X_scaled, index=valid_idx, columns=self.selected_features)\n\n        # Intersect indices to ensure we only select data that exists in X_scaled_temp_df\n        actual_train_indices = original_train_indices.intersection(X_scaled_temp_df.index)\n        actual_val_indices = original_val_indices.intersection(X_scaled_temp_df.index)\n\n        X_train = X_scaled_temp_df.loc[actual_train_indices].values.astype(np.float32)\n        y_train = y_for_training.loc[actual_train_indices].values.astype(np.float32)\n        X_val = X_scaled_temp_df.loc[actual_val_indices].values.astype(np.float32)\n        y_val = y_for_training.loc[actual_val_indices].values.astype(np.float32)\n\n        del temp_train_full_timestamps, X_scaled_temp_df, y_for_training # Clean up more\n        gc.collect() # Aggressive GC\n\n        print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n        \n        # Train LightGBM (baseline)\n        try:\n            lgb_model = self.train_lightgbm(X_train, y_train, X_val, y_val)\n            lgb_pred = lgb_model.predict(X_val)\n            lgb_score = self.evaluate_model(y_val, lgb_pred, \"LightGBM\")\n            self.models['lightgbm'] = lgb_model\n        except Exception as e:\n            print(f\"LightGBM training failed: {e}\")\n            lgb_score = 0\n        \n        # Deep learning model training and ensemble logic (reinstated)\n        try:\n            X_train_seq, y_train_seq = self.prepare_sequences(X_train, y_train)\n            X_val_seq, y_val_seq = self.prepare_sequences(X_val, y_val)\n            \n            if len(X_train_seq) > 0:\n                print(f\"Sequence data - Train: {X_train_seq.shape}, Val: {X_val_seq.shape}\")\n                \n                callbacks = [\n                    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_mae'),\n                    ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6, monitor='val_mae')\n                ]\n                \n                # Train ConvLSTM\n                convlstm_score = 0 # Initialize score\n                try:\n                    print(\"Training ConvLSTM model...\")\n                    convlstm_model = self.build_convlstm_model(X_train_seq.shape[1:])\n                    if convlstm_model: # Check if model was actually built\n                        convlstm_model.fit(\n                            X_train_seq, y_train_seq,\n                            validation_data=(X_val_seq, y_val_seq),\n                            epochs=50, batch_size=64,\n                            callbacks=callbacks, verbose=1 # Changed verbose to 1 for logging\n                        )\n                        convlstm_pred = convlstm_model.predict(X_val_seq).flatten()\n                        convlstm_score = self.evaluate_model(y_val_seq, convlstm_pred, \"ConvLSTM\")\n                        self.models['convlstm'] = convlstm_model\n                except Exception as e:\n                    print(f\"ConvLSTM training failed: {e}\")\n                    convlstm_score = 0\n                \n                # Train LSTM\n                lstm_score = 0 # Initialize score\n                try:\n                    print(\"Training LSTM model...\")\n                    lstm_model = self.build_lstm_model(X_train_seq.shape[1:])\n                    if lstm_model: # Check if model was actually built\n                        lstm_model.fit(\n                            X_train_seq, y_train_seq,\n                            validation_data=(X_val_seq, y_val_seq),\n                            epochs=50, batch_size=64,\n                            callbacks=callbacks, verbose=1 # Changed verbose to 1 for logging\n                        )\n                        lstm_pred = lstm_model.predict(X_val_seq).flatten()\n                        lstm_score = self.evaluate_model(y_val_seq, lstm_pred, \"LSTM\")\n                        self.models['lstm'] = lstm_model\n                except Exception as e:\n                    print(f\"LSTM training failed: {e}\")\n                    lstm_score = 0\n                \n                # Ensemble predictions if we have multiple models (including DL)\n                if len(self.models) > 1:\n                    # Align lgb_pred to match the length of DL predictions\n                    lgb_pred_aligned = lgb_pred[self.sequence_length:]\n\n                    ensemble_pred_dl = []\n                    dl_weights = []\n\n                    if 'convlstm' in self.models and convlstm_score > 0: # Only include if successful\n                        ensemble_pred_dl.append(convlstm_pred)\n                        dl_weights.append(0.35)\n                    if 'lstm' in self.models and lstm_score > 0: # Only include if successful\n                        ensemble_pred_dl.append(lstm_pred) \n                        dl_weights.append(0.25)\n\n                    if ensemble_pred_dl:\n                        # Combine DL predictions first, then ensemble with LGBM\n                        dl_ensemble_weighted = np.average(ensemble_pred_dl, axis=0, weights=dl_weights)\n                        ensemble_pred = (lgb_pred_aligned * 0.4) + (dl_ensemble_weighted * 0.6)\n                        \n                        ensemble_score = self.evaluate_model(y_val_seq, ensemble_pred, \"Ensemble\")\n                        print(f\"\\nEnsemble score: {ensemble_score:.4f}\")\n                    else:\n                        print(\"No successful deep learning models to include in ensemble.\")\n                        ensemble_score = lgb_score # Fallback to LGBM score if DL failed\n\n            else:\n                print(\"Skipping deep learning models as no sequences could be prepared.\")\n                ensemble_score = lgb_score # Only LGBM was trained\n            \n            print(f\"\\nBest individual model score: {max(lgb_score, convlstm_score, lstm_score) if 'convlstm' in self.models else lgb_score:.4f}\")\n            print(f\"Ensemble score: {ensemble_score:.4f}\") \n        except Exception as e:\n            print(f\"Deep learning training failed during ensemble: {e}\")\n            ensemble_score = lgb_score # Fallback to LGBM score\n        \n        # Cleanup memory for sequence data\n        del X_train, y_train, X_val, y_val \n        if 'X_train_seq' in locals() and X_train_seq is not None:\n             del X_train_seq, y_train_seq, X_val_seq, y_val_seq\n        gc.collect()\n        \n        return self\n    \n    def predict(self, test_data_raw_initial_load): \n        \"\"\"Generate predictions for test data with robust error handling\"\"\"\n        print(\"Generating predictions...\")\n        \n        # --- Aggressive initial column selection for Pandas memory management ---\n        # We will ONLY select these columns from the already loaded raw DataFrame\n        # self.selected_features should be populated after fit()\n        \n        basic_features_cols_test = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'ID']\n        # The test set does NOT have a 'label' column during prediction\n        # Get the original X_n columns from the raw test data (matching what was selected in training)\n        X_n_cols_raw_test = [col for col in test_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        preselected_X_n_features_test = X_n_cols_raw_test[:min(self.top_X_features_to_preselect, len(X_n_cols_raw_test))]\n\n        # Select only the raw columns needed for feature engineering from the test set\n        columns_to_process_raw_for_predict = basic_features_cols_test + preselected_X_n_features_test\n        \n        print(f\"Selecting columns from initial raw test data (only {len(columns_to_process_raw_for_predict)} columns)...\")\n        # Select columns directly from the already loaded test_data_raw_initial_load\n        test_df = test_data_initial_load[columns_to_process_raw_for_predict].copy() # FIX: Use _initial_load\n        \n        # Ensure 'ID' is a column if it was index\n        if 'ID' not in test_df.columns and test_df.index.name == 'ID':\n            test_df = test_df.reset_index()\n\n        # Store original test IDs for final submission mapping\n        original_test_ids = test_df['ID'].copy()    \n        \n        # Ensure timestamp is datetime\n        if 'timestamp' in test_df.columns:\n            test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n\n        del test_data_raw_initial_load # Free up memory from initial test data load\n        gc.collect()\n\n        # Memory optimization and data cleaning (now on Pandas DataFrame)\n        test_df = self.optimize_memory(test_df)\n        \n        # Feature engineering (same as training, now on Pandas DataFrame)\n        test_df = self.create_time_features(test_df)\n        \n        print(f\"Test data shape after feature engineering: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n        \n        # Prepare features for prediction\n        if self.selected_features is None:\n            raise ValueError(\"Model must be fitted before making predictions (selected_features is None)\")\n        \n        # Filter test_data to include only the final selected features (from training)\n        # This is now safe as test_df has all engineered features\n        X_test_df_final = test_df[self.selected_features] # Ensure order matches training features\n\n        # Handle any remaining invalid values before scaling\n        X_test_df_final = X_test_df_final.replace([np.inf, -np.inf], np.nan)\n        X_test_df_final = X_test_df_final.fillna(method='ffill').fillna(method='bfill').fillna(0)\n        \n        # Scale features\n        X_test_scaled = self.scaler.transform(X_test_df_final)\n        \n        # Final validation\n        if np.any(np.isnan(X_test_scaled)) or np.any(np.isinf(X_test_scaled)):\n            print(\"WARNING: Invalid values in test data after scaling, applying emergency cleanup.\")\n            X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        \n        # Create a DataFrame from X_test_scaled to maintain original indices for prediction mapping\n        X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test_df_final.index, columns=self.selected_features)\n        \n        del X_test_df_final, test_df # Clean up\n        gc.collect()\n\n        predictions = np.zeros(len(original_test_ids), dtype=np.float32) # Initialize full predictions\n\n        # Dictionary to store predictions mapped to original DataFrame indices\n        indexed_predictions = {}\n        \n        # LightGBM predictions (baseline/fallback)\n        if 'lightgbm' in self.models:\n            try:\n                lgb_pred_full = self.models['lightgbm'].predict(X_test_scaled_df)\n                for i, idx in enumerate(X_test_scaled_df.index):\n                    indexed_predictions[idx] = lgb_pred_full[i]\n                print(\"LightGBM predictions generated.\")\n            except Exception as e:\n                print(f\"LightGBM prediction failed: {e}\")\n                \n        # Deep learning prediction logic for ensemble\n        if ('convlstm' in self.models and self.models['convlstm'] is not None) or \\\n           ('lstm' in self.models and self.models['lstm'] is not None):\n            try:\n                X_test_seq, _ = self.prepare_sequences(X_test_scaled) \n                \n                if len(X_test_seq) > 0:\n                    dl_predictions_list = []\n                    dl_weights_list = []\n                    \n                    if 'convlstm' in self.models and self.models['convlstm'] is not None:\n                        try:\n                            convlstm_pred = self.models['convlstm'].predict(X_test_seq).flatten()\n                            dl_predictions_list.append(convlstm_pred)\n                            dl_weights_list.append(0.6) # Weight for ConvLSTM in DL ensemble\n                        except Exception as e:\n                            print(f\"ConvLSTM prediction during test failed: {e}\")\n                    \n                    if 'lstm' in self.models and self.models['lstm'] is not None:\n                        try:\n                            lstm_pred = self.models['lstm'].predict(X_test_seq).flatten()\n                            dl_predictions_list.append(lstm_pred)\n                            dl_weights_list.append(0.4) # Weight for LSTM in DL ensemble\n                        except Exception as e:\n                            print(f\"LSTM prediction during test failed: {e}\")\n                    \n                    # Combine deep learning predictions\n                    if dl_predictions_list:\n                        dl_ensemble_weighted = np.average(dl_predictions_list, axis=0, weights=dl_weights_list)\n                        \n                        # Apply ensemble to the part of the test data covered by sequences\n                        # These predictions map to indices starting from `self.sequence_length`\n                        sequence_covered_indices = X_test_scaled_df.index[self.sequence_length:].tolist()\n                        \n                        for i, original_idx in enumerate(sequence_covered_indices):\n                            # Combine LGBM's initial prediction for this index with DL ensemble\n                            # If LGBM didn't run or failed, its value might be missing,\n                            # so get it safely or default to 0.0\n                            lgbm_part = indexed_predictions.get(original_idx, 0.0)    \n                            indexed_predictions[original_idx] = (lgbm_part * 0.4) + (dl_ensemble_weighted[i] * 0.6)\n                        \n                        print(\"Deep learning models included in predictions.\")\n                    else:\n                        print(\"No successful deep learning models to include in test predictions.\")\n                else:\n                    print(\"Test data too short for deep learning sequences. Using LightGBM only.\")\n            except Exception as e:\n                print(f\"Deep learning prediction setup failed: {e}\")\n                print(\"Falling back to LightGBM predictions only for test set.\")\n        \n        # Populate the final predictions array based on original_test_ids order\n        # Get the original IDs and their corresponding internal DataFrame indices from the full raw test data\n        temp_test_raw_full = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet', columns=['ID'])\n        if 'ID' not in temp_test_raw_full.columns and temp_test_raw_full.index.name == 'ID':\n            temp_test_raw_full = temp_test_raw_full.reset_index()\n\n        id_to_original_index_map = pd.Series(temp_test_raw_full.index.values, index=temp_test_raw_full['ID'])\n\n        # Iterate through the original_test_ids to populate predictions\n        for i, current_id in enumerate(original_test_ids):\n            original_idx_in_raw_df = id_to_original_index_map.get(current_id)\n            \n            if original_idx_in_raw_df is not None and original_idx_in_raw_df in indexed_predictions:\n                predictions[i] = indexed_predictions[original_idx_in_raw_df]\n            else:\n                predictions[i] = 0.0    \n\n        del temp_test_raw_full, id_to_original_index_map\n        gc.collect()\n\n        # Final validation of predictions\n        if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n            print(\"WARNING: Invalid predictions detected (NaN/Inf), cleaning...\")\n            predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n        \n        print(f\"Generated {len(predictions)} predictions\")\n        print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n        \n        return predictions\n\n# Main execution function\ndef run_competition_pipeline():\n    \"\"\"Run the complete competition pipeline\"\"\"\n    \n    # Load data (initial raw load to get shape and display head)\n    print(\"Loading data...\")\n    # Loading full dataset for shape check (this could be memory intensive)\n    train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n    # Explicitly convert index to column if 'timestamp' is in index name\n    if 'timestamp' in train_full_raw.index.names:\n        train_full_raw = train_full_raw.reset_index(names=['timestamp']) # Ensure it becomes a named column\n    # Ensure it's datetime type\n    train_full_raw['timestamp'] = pd.to_datetime(train_full_raw['timestamp'])\n\n    test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n    # Explicitly convert index to column if 'ID' or 'timestamp' is in index name for test\n    if 'ID' in test_full_raw.index.names:\n        test_full_raw = test_full_raw.reset_index(names=['ID']) # Ensure it becomes a named column\n    elif 'timestamp' in test_full_raw.index.names: # Also check for timestamp as index in test\n        test_full_raw = test_full_raw.reset_index(names=['timestamp'])\n    # Ensure timestamp is datetime if it exists (for features)\n    if 'timestamp' in test_full_raw.columns:\n        test_full_raw['timestamp'] = pd.to_datetime(test_full_raw['timestamp'])\n    \n    print(f\"\\nTrain shape: {train_full_raw.shape}\")\n    print(f\"Test shape: {test_full_raw.shape}\")\n    \n    # Initialize and train model\n    predictor = CryptoMarketPredictor(\n        sequence_length=30,    \n        top_features=100,      \n        top_X_features_to_preselect=30 \n    )    \n    # Pass the full raw DataFrame to allow determining X_n_cols_raw\n    predictor.fit(train_full_raw) \n    \n    # Generate predictions\n    # Pass the full raw DataFrame to allow initial column selection\n    predictions = predictor.predict(test_full_raw) \n    \n    # Create submission\n    submission = pd.DataFrame({\n        'ID': test_full_raw['ID'], # Use the 'ID' column from the original test DataFrame\n        'Prediction': predictions\n    })\n    \n    # Save submission\n    submission.to_csv('submission.csv', index=False)\n    print(f\"Submission saved with {len(submission)} predictions\")\n    print(f\"Prediction statistics - Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n    \n    return submission\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    submission = run_competition_pipeline()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T05:10:39.663508Z","iopub.execute_input":"2025-06-23T05:10:39.663736Z","execution_failed":"2025-06-23T05:13:11.438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}