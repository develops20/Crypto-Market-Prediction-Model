{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-nb153?scriptVersionId=248907206\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# DRW Crypto Market Prediction Competition Pipeline\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os # Import os for path checking\nimport gc # Import garbage collector\nimport math # For ceil in data generator\n\n# Memory optimization and data processing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\n# Deep learning and modeling (TensorFlow import remains for GPU memory setting)\nimport tensorflow as tf \n# Removed: from tensorflow.keras.models import Sequential, Model\n# Removed: from tensorflow.keras.layers import (BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Reshape, TimeDistributed)\n# Removed: from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n# Removed: from tensorflow.keras.optimizers import Adam\n# Removed: from tensorflow.keras.utils import Sequence\n\n# Tree-based models for ensemble\nimport lightgbm as lgb\nimport xgboost as xgb # Import XGBoost\nfrom scipy.stats import pearsonr\n\n# Set memory growth for GPU (if available) - still good practice\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n\n# Removed TimeSeriesSequence class as it's no longer needed for Keras DL models\n# class TimeSeriesSequence(Sequence):\n#     ...\n\n\nclass CryptoMarketPredictor:\n    def __init__(self, sequence_length=30, top_features=100, top_X_features_to_preselect=30):\n        self.sequence_length = sequence_length\n        self.top_features = top_features\n        self.top_X_features_to_preselect = top_X_features_to_preselect\n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None # Stores final selected feature names\n        self.models = {}\n        # Path for the initial processed data after feature engineering\n        self._engineered_data_checkpoint_path = './engineered_train_data_checkpoint.parquet'\n        # Paths for scaled and feature-selected data (now loaded fully into memory for LGBM/XGBoost)\n        # No longer using separate _scaled_X_path, _scaled_y_path for generator\n        # The data will be loaded into memory for LGBM and XGBoost directly.\n\n\n    def optimize_memory(self, df):\n        \"\"\"\n        Optimize Pandas DataFrame memory usage and clean data.\n        \"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n        # Clean data first\n        df = self.clean_data(df)\n\n        # Optimize numeric columns (Pandas directly)\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col == 'timestamp' or col == 'ID' or col == 'label':\n                continue # Don't downcast timestamp, ID, or label\n            df[col] = pd.to_numeric(df[col], downcast='float')\n\n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n\n    def clean_data(self, df):\n        \"\"\"\n        Clean Pandas DataFrame by handling inf, -inf, and extreme values.\n        \"\"\"\n        print(\"Cleaning data...\")\n\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        # Replace inf and -inf with NaN first\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n        # Fill NaN values with forward fill, then backward fill, then 0\n        for col in numeric_cols:\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        # Handle extreme outliers (values beyond 3*IQR)\n        for col in df.select_dtypes(include=[np.float32, np.float64]).columns:\n            if col in ['timestamp', 'ID', 'label']: continue\n            if df[col].nunique() > 1:\n                q25 = df[col].quantile(0.25)\n                q75 = df[col].quantile(0.75)\n                iqr = q75 - q25\n\n                if iqr != 0 and not pd.isna(iqr):\n                    lower_bound = q25 - 3 * iqr\n                    upper_bound = q75 + 3 * iqr\n                    df[col] = df[col].clip(lower_bound, upper_bound)\n        print(\"Data cleaning applied.\")\n        return df\n\n    def create_time_features(self, df):\n        \"\"\"\n        Create time-based features with robust calculations using Pandas.\n        Significantly reduced complexity for faster execution.\n        \"\"\"\n        print(\"Creating time-based features...\")\n\n        # Basic market features\n        df['mid_price'] = (df['bid_qty'] + df['ask_qty']) / 2\n        df['spread'] = df['ask_qty'] - df['bid_qty']\n\n        # Safe division for imbalance\n        denominator = df['bid_qty'] + df['ask_qty'] + 1e-10\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n\n        # Safe division for buy/sell ratio\n        df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-10)\n\n        # Rolling statistics - significantly reduced windows for speed\n        windows = [10, 30]\n        base_cols_for_rolling = ['volume', 'mid_price', 'buy_qty', 'sell_qty', 'imbalance']\n\n        for col in base_cols_for_rolling:\n            for window in windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)\n\n        # Lagged features - significantly reduced lags for speed\n        lags = [1, 5]\n        base_cols_for_lag = ['mid_price', 'imbalance']\n\n        for col in base_cols_for_lag:\n            for lag in lags:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n\n        # Technical indicators - reduced\n        df['rsi_proxy'] = self.calculate_rsi_proxy(df['mid_price'], window=10)\n        df['momentum'] = df['mid_price'] - df['mid_price'].shift(5)\n\n        # Final check for any inf/nan values that might have been introduced\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        print(f\"Time-based features created. Current shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n        return df\n\n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate RSI-like indicator with safe operations for Pandas Series.\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n\n        rs = gain / (loss + 1e-10)\n        rsi = 100 - (100 / (1 + rs))\n\n        rsi = rsi.replace([np.inf, -np.inf], np.nan).fillna(50)\n\n        return rsi\n\n    def select_features(self, X_df, y_df, method='mutual_info'):\n        \"\"\"\n        Feature selection to reduce dimensionality with robust handling.\n        Operates directly on Pandas DataFrames.\n        \"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n\n        print(\"Validating data before final feature selection...\")\n\n        # Check for any remaining inf/nan values\n        inf_mask = np.isinf(X_df).any(axis=1)\n        nan_mask = np.isnan(X_df).any(axis=1)\n        invalid_mask = inf_mask | nan_mask\n\n        if invalid_mask.sum() > 0:\n            print(f\"Removing {invalid_mask.sum()} rows with invalid values before final selection.\")\n            X_df = X_df[~invalid_mask]\n            y_df = y_df[~invalid_mask]\n\n        # Check for constant or near-constant features (can cause issues for some selectors)\n        feature_std = X_df.std()\n        constant_features_mask = feature_std < 1e-8\n\n        if constant_features_mask.all():\n            print(\"Warning: All features are constant. Cannot perform feature selection.\")\n            non_constant_features = X_df.columns.tolist()\n        else:\n            non_constant_features = X_df.columns[~constant_features_mask].tolist()\n            if constant_features_mask.sum() > 0:\n                print(f\"Removing {constant_features_mask.sum()} constant features.\")\n            X_df = X_df[non_constant_features]\n\n        print(f\"Final data shape for feature selection: {X_df.shape}\")\n\n        n_features_to_select = min(self.top_features, X_df.shape[1])\n\n        if method == 'mutual_info':\n            selector = SelectKBest(score_func=mutual_info_regression, k=n_features_to_select)\n        else:\n            selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n\n        X_selected = selector.fit_transform(X_df, y_df)\n        self.feature_selector = selector\n        self.selected_features = X_df.columns[selector.get_support()].tolist()\n\n        print(f\"\\n--- Selected Features ({len(self.selected_features)}) ---\")\n        for feature in self.selected_features:\n            print(f\"- {feature}\")\n        print(\"---------------------------------------\\n\")\n\n        # Return as NumPy array and Pandas Index to maintain alignment with y\n        return X_selected.astype(np.float32), X_df.index\n\n    # Removed build_convlstm_model, build_lstm_model, build_conv1d_model\n    # All deep learning models are replaced by XGBoost\n    # def build_convlstm_model(self, input_shape): ...\n    # def build_lstm_model(self, input_shape): ...\n    # def build_conv1d_model(self, input_shape): ...\n\n    def build_cnn_lstm_model(self, input_shape):\n        print(\"CNN-LSTM model building skipped for current speed optimization, but can be reinstated later.\")\n        return None\n\n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train LightGBM model\"\"\"\n        print(\"Training LightGBM model...\")\n\n        # Robust NaN/Inf handling just before LightGBM training\n        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n            print(\"WARNING: NaNs/Infs found in X_train for LightGBM. Applying emergency cleanup.\")\n            X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(X_val)) or np.any(np.isinf(X_val)):\n            print(\"WARNING: NaNs/Infs found in X_val for LightGBM. Applying emergency cleanup.\")\n            X_val = np.nan_to_num(X_val, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_train)) or np.any(np.isinf(y_train)):\n            print(\"WARNING: NaNs/Infs found in y_train for LightGBM. Applying emergency cleanup.\")\n            y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_val)) or np.any(np.isinf(y_val)):\n            print(\"WARNING: NaNs/Infs found in y_val for LightGBM. Applying emergency cleanup.\")\n            y_val = np.nan_to_num(y_val, nan=0.0, posinf=1e6, neginf=-1e6)\n\n\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 1000\n        }\n\n        model = lgb.train(\n            params,\n            train_data,\n            valid_sets=[val_data],\n            num_boost_round=params['n_estimators'],\n            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n        )\n\n        return model\n\n    def train_xgboost(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train XGBoost model\"\"\"\n        print(\"Training XGBoost model...\")\n\n        # Robust NaN/Inf handling just before XGBoost training\n        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n            print(\"WARNING: NaNs/Infs found in X_train for XGBoost. Applying emergency cleanup.\")\n            X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(X_val)) or np.any(np.isinf(X_val)):\n            print(\"WARNING: NaNs/Infs found in X_val for XGBoost. Applying emergency cleanup.\")\n            X_val = np.nan_to_num(X_val, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_train)) or np.any(np.isinf(y_train)):\n            print(\"WARNING: NaNs/Infs found in y_train for XGBoost. Applying emergency cleanup.\")\n            y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_val)) or np.any(np.isinf(y_val)):\n            print(\"WARNING: NaNs/Infs found in y_val for XGBoost. Applying emergency cleanup.\")\n            y_val = np.nan_to_num(y_val, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        model = xgb.XGBRegressor(\n            objective='reg:squarederror', # Regression objective\n            eval_metric='mae', # Mean Absolute Error\n            n_estimators=1000, # Number of boosting rounds\n            learning_rate=0.05,\n            max_depth=6, # Maximum depth of a tree\n            subsample=0.8, # Subsample ratio of the training instance\n            colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n            random_state=42,\n            n_jobs=-1 # Use all available CPU cores\n        )\n\n        model.fit(X_train, y_train,\n                  eval_set=[(X_val, y_val)],\n                  early_stopping_rounds=50, # Stop if no improvement for 50 rounds\n                  verbose=False) # Suppress verbose output during training\n\n        return model\n\n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance\"\"\"\n        # Ensure y_true and y_pred are clean before evaluation\n        y_true_clean = np.nan_to_num(y_true, nan=0.0, posinf=1e6, neginf=-1e6)\n        y_pred_clean = np.nan_to_num(y_pred, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        mae = mean_absolute_error(y_true_clean, y_pred_clean)\n\n        # Pearson correlation requires at least 2 non-constant values for both arrays\n        correlation = 0.0\n        if len(np.unique(y_true_clean)) > 1 and len(np.unique(y_pred_clean)) > 1:\n            try:\n                correlation, _ = pearsonr(y_true_clean, y_pred_clean)\n            except ValueError:\n                # Can happen if inputs have 0 variance after cleaning\n                correlation = 0.0\n\n        print(f\"{model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n\n    def fit(self, train_data_raw_initial_load):\n        \"\"\"Main training pipeline with robust error handling\"\"\"\n        print(\"Starting training pipeline...\")\n\n        # Determine raw X_n columns from the initial load structure\n        X_n_cols_raw = [col for col in train_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        basic_features_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n\n        preselected_X_n_features = X_n_cols_raw[:min(self.top_X_features_to_preselect, len(X_n_cols_raw))]\n        print(f\"Initially selected {len(preselected_X_n_features)} X_n features for direct Pandas load.\")\n\n        columns_to_process_raw_for_fit = basic_features_cols + preselected_X_n_features + ['label']\n\n        train_df = None\n        if os.path.exists(self._engineered_data_checkpoint_path):\n            print(f\"Checkpoint found. Loading processed data from {self._engineered_data_checkpoint_path}...\")\n            train_df = pd.read_parquet(self._engineered_data_checkpoint_path)\n            if 'timestamp' in train_df.columns:\n                train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n        else:\n            print(f\"Selecting columns from initial raw training data...\")\n            train_df = train_data_raw_initial_load[columns_to_process_raw_for_fit].copy()\n            if 'timestamp' not in train_df.columns and train_df.index.name == 'timestamp':\n                train_df = train_df.reset_index(names=['timestamp'])\n            train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n            gc.collect()\n            train_df = self.optimize_memory(train_df)\n            train_df = self.create_time_features(train_df)\n            print(f\"Saving engineered data to checkpoint: {self._engineered_data_checkpoint_path}...\")\n            train_df.to_parquet(self._engineered_data_checkpoint_path, index=False)\n            print(\"Engineered data checkpoint saved.\")\n\n        print(f\"Data shape after feature engineering: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n        feature_cols_final = [col for col in train_df.columns\n                              if col not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols_final]\n        y_df = train_df['label']\n        print(f\"Features shape before final selection: {X_df.shape[0]} rows, {X_df.shape[1]} columns\")\n        print(f\"Target shape: {y_df.shape[0]} rows\")\n        \n        X_selected, valid_idx = self.select_features(X_df, y_df)\n        y_for_training = y_df.loc[valid_idx].astype(np.float32)\n\n        del X_df, y_df\n        gc.collect()\n\n        X_scaled = self.scaler.fit_transform(X_selected)\n        print(\"Final data validation (after scaling)...\")\n        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n            print(\"ERROR: Still have invalid values after preprocessing! Applying emergency cleanup.\")\n            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        print(f\"Final training data shape: {X_scaled.shape}\")\n        \n        del X_selected\n        gc.collect()\n\n        # Split data into training and validation sets\n        temp_train_full_timestamps = train_data_raw_initial_load[['timestamp']].copy()\n        if 'timestamp' not in temp_train_full_timestamps.columns and temp_train_full_timestamps.index.name == 'timestamp':\n            temp_train_full_timestamps = temp_train_full_timestamps.reset_index(names=['timestamp'])\n        temp_train_full_timestamps['timestamp'] = pd.to_datetime(temp_train_full_timestamps['timestamp'])\n        \n        VALIDATION_SPLIT_DATE = '2024-01-01'\n        original_train_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] < VALIDATION_SPLIT_DATE].index\n        original_val_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] >= VALIDATION_SPLIT_DATE].index\n        \n        del temp_train_full_timestamps\n        gc.collect()\n\n        # Create a temporary DataFrame from X_scaled to map to original indices for splitting\n        X_scaled_temp_df = pd.DataFrame(X_scaled, index=valid_idx, columns=self.selected_features)\n        \n        del X_scaled, y_for_training\n        gc.collect()\n\n        actual_train_indices = original_train_indices.intersection(X_scaled_temp_df.index)\n        actual_val_indices = original_val_indices.intersection(X_scaled_temp_df.index)\n\n        # Extract X_train, y_train, X_val, y_val as NumPy arrays for direct use by LGBM/XGBoost\n        X_train_final = X_scaled_temp_df.loc[actual_train_indices].values.astype(np.float32)\n        y_train_final = pd.Series(train_df['label'].loc[actual_train_indices].values).astype(np.float32)\n        X_val_final = X_scaled_temp_df.loc[actual_val_indices].values.astype(np.float32)\n        y_val_final = pd.Series(train_df['label'].loc[actual_val_indices].values).astype(np.float32)\n        \n        # Free up train_df and X_scaled_temp_df\n        del train_df, X_scaled_temp_df, valid_idx\n        gc.collect()\n\n        print(f\"Training set shapes: X={X_train_final.shape}, Y={y_train_final.shape}\")\n        print(f\"Validation set shapes: X={X_val_final.shape}, Y={y_val_final.shape}\")\n\n        # Train LightGBM model\n        try:\n            lgb_model = self.train_lightgbm(X_train_final, y_train_final, X_val_final, y_val_final)\n            lgb_pred = lgb_model.predict(X_val_final)\n            lgb_score = self.evaluate_model(y_val_final, lgb_pred, \"LightGBM\")\n            self.models['lightgbm'] = lgb_model\n        except Exception as e:\n            print(f\"LightGBM training or prediction failed: {e}\")\n            lgb_score = 0\n            lgb_pred = np.zeros_like(y_val_final) # Ensure lgb_pred is defined even on failure\n\n        # Train XGBoost model\n        try:\n            xgb_model = self.train_xgboost(X_train_final, y_train_final, X_val_final, y_val_final)\n            xgb_pred = xgb_model.predict(X_val_final)\n            xgb_score = self.evaluate_model(y_val_final, xgb_pred, \"XGBoost\")\n            self.models['xgboost'] = xgb_model\n        except Exception as e:\n            print(f\"XGBoost training or prediction failed: {e}\")\n            xgb_score = 0\n            xgb_pred = np.zeros_like(y_val_final) # Ensure xgb_pred is defined even on failure\n\n        # Ensemble logic\n        ensemble_predictions_list = []\n        ensemble_weights = []\n\n        if 'lightgbm' in self.models and lgb_score > 0:\n            ensemble_predictions_list.append(lgb_pred)\n            ensemble_weights.append(0.5) # Initial equal weight\n\n        if 'xgboost' in self.models and xgb_score > 0:\n            ensemble_predictions_list.append(xgb_pred)\n            ensemble_weights.append(0.5) # Initial equal weight\n\n        if ensemble_predictions_list:\n            # Ensure all predictions have the same length before ensembling\n            min_len = min(len(p) for p in ensemble_predictions_list)\n            ensemble_predictions_list = [p[:min_len] for p in ensemble_predictions_list]\n            y_val_ensemble_aligned = y_val_final[:min_len] # Align true labels for evaluation\n\n            ensemble_pred = np.average(ensemble_predictions_list, axis=0, weights=ensemble_weights)\n            ensemble_score = self.evaluate_model(y_val_ensemble_aligned, ensemble_pred, \"Ensemble (LGBM + XGBoost)\")\n            print(f\"\\nEnsemble score: {ensemble_score:.4f}\")\n        else:\n            print(\"No successful models to include in ensemble. Ensemble score defaulting to 0.\")\n            ensemble_score = 0\n\n        print(f\"\\nBest individual model score: {max(lgb_score, xgb_score):.4f}\")\n        print(f\"Final overall ensemble score: {ensemble_score:.4f}\")\n        \n        # Clean up memory after training\n        del X_train_final, y_train_final, X_val_final, y_val_final\n        gc.collect()\n        return self\n\n    def predict(self, test_data_raw_initial_load):\n        \"\"\"Generate predictions for test data with robust error handling\"\"\"\n        print(\"Generating predictions...\")\n\n        temp_df_for_id_map = test_data_raw_initial_load.copy()\n        if 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index.name == 'ID':\n            temp_df_for_id_map = temp_df_for_id_map.reset_index(names=['ID'])\n        elif 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index is not None and temp_df_for_id_map.index.name is None and len(temp_df_for_id_map.index) == len(temp_df_for_id_map):\n            temp_df_for_id_map = temp_df_for_id_map.reset_index()\n            temp_df_for_id_map.rename(columns={'index': 'ID'}, inplace=True)\n\n        id_to_original_index_map = pd.Series(temp_df_for_id_map.index.values, index=temp_df_for_id_map['ID'])\n        original_test_ids = temp_df_for_id_map['ID'].copy()\n\n        del temp_df_for_id_map\n        gc.collect()\n\n        basic_features_cols_test = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'ID']\n        X_n_cols_raw_test = [col for col in test_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        preselected_X_n_features_test = X_n_cols_raw_test[:min(self.top_X_features_to_preselect, len(X_n_cols_raw_test))]\n\n        columns_to_process_raw_for_predict = basic_features_cols_test + preselected_X_n_features_test\n\n        missing_columns = [col for col in columns_to_process_raw_for_predict if col not in test_data_raw_initial_load.columns]\n        if missing_columns:\n            raise KeyError(f\"The following required columns are missing from the test data: {missing_columns}. Please ensure your test.parquet file contains these columns.\")\n\n        print(f\"Selecting columns from initial raw test data (only {len(columns_to_process_raw_for_predict)} columns)...\")\n        test_df = test_data_raw_initial_load[columns_to_process_raw_for_predict].copy()\n\n        del test_data_raw_initial_load\n        gc.collect()\n\n        if 'ID' not in test_df.columns and test_df.index.name == 'ID':\n            test_df = test_df.reset_index()\n\n        if 'timestamp' in test_df.columns:\n            test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n\n        test_df = self.optimize_memory(test_df)\n        test_df = self.create_time_features(test_df)\n\n        print(f\"Test data shape after feature engineering: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n\n        if self.selected_features is None:\n            raise ValueError(\"Model must be fitted before making predictions (selected_features is None)\")\n\n        X_test_df_final = test_df[self.selected_features]\n\n        X_test_df_final = X_test_df_final.replace([np.inf, -np.inf], np.nan)\n        X_test_df_final = X_test_df_final.ffill().bfill().fillna(0)\n\n        X_test_scaled = self.scaler.transform(X_test_df_final)\n\n        if np.any(np.isnan(X_test_scaled)) or np.any(np.isinf(X_test_scaled)):\n            print(\"WARNING: Invalid values in test data after scaling, applying emergency cleanup.\")\n            X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test_df_final.index, columns=self.selected_features)\n\n        del X_test_df_final, test_df\n        gc.collect()\n\n        predictions = np.zeros(len(original_test_ids), dtype=np.float32)\n        indexed_predictions_by_internal_idx = {}\n\n        # LightGBM prediction\n        if 'lightgbm' in self.models:\n            try:\n                X_test_scaled_for_lgbm = X_test_scaled_df.copy()\n                if np.any(np.isnan(X_test_scaled_for_lgbm.values)) or np.any(np.isinf(X_test_scaled_for_lgbm.values)):\n                    print(\"WARNING: NaNs/Infs found in X_test_scaled_for_lgbm. Applying emergency cleanup.\")\n                    X_test_scaled_for_lgbm = pd.DataFrame(\n                        np.nan_to_num(X_test_scaled_for_lgbm.values, nan=0.0, posinf=1e6, neginf=-1e6),\n                        columns=X_test_scaled_for_lgbm.columns, index=X_test_scaled_for_lgbm.index\n                    )\n                lgb_pred_full = self.models['lightgbm'].predict(X_test_scaled_for_lgbm)\n                for i, idx in enumerate(X_test_scaled_for_lgbm.index):\n                    indexed_predictions_by_internal_idx[idx] = lgb_pred_full[i]\n                print(\"LightGBM predictions generated.\")\n                del X_test_scaled_for_lgbm\n                gc.collect()\n            except Exception as e:\n                print(f\"LightGBM prediction failed: {e}\")\n\n        # XGBoost prediction\n        if 'xgboost' in self.models:\n            try:\n                X_test_scaled_for_xgb = X_test_scaled_df.copy()\n                if np.any(np.isnan(X_test_scaled_for_xgb.values)) or np.any(np.isinf(X_test_scaled_for_xgb.values)):\n                    print(\"WARNING: NaNs/Infs found in X_test_scaled_for_xgb. Applying emergency cleanup.\")\n                    X_test_scaled_for_xgb = pd.DataFrame(\n                        np.nan_to_num(X_test_scaled_for_xgb.values, nan=0.0, posinf=1e6, neginf=-1e6),\n                        columns=X_test_scaled_for_xgb.columns, index=X_test_scaled_for_xgb.index\n                    )\n                xgb_pred_full = self.models['xgboost'].predict(X_test_scaled_for_xgb)\n                # If LGBM already populated, we need to store both for ensemble\n                for i, idx in enumerate(X_test_scaled_for_xgb.index):\n                    # Store XGBoost predictions separately for ensemble calculation\n                    # Or update if only one model is used\n                    if idx in indexed_predictions_by_internal_idx:\n                        indexed_predictions_by_internal_idx[idx] = (indexed_predictions_by_internal_idx[idx] * 0.5) + (xgb_pred_full[i] * 0.5)\n                    else:\n                        indexed_predictions_by_internal_idx[idx] = xgb_pred_full[i]\n                print(\"XGBoost predictions generated.\")\n                del X_test_scaled_for_xgb\n                gc.collect()\n            except Exception as e:\n                print(f\"XGBoost prediction failed: {e}\")\n\n        del X_test_scaled_df\n        gc.collect()\n\n        for i, current_id in enumerate(original_test_ids):\n            original_idx_in_raw_df = id_to_original_index_map.get(current_id)\n\n            if original_idx_in_raw_df is not None and original_idx_in_raw_df in indexed_predictions_by_internal_idx:\n                predictions[i] = indexed_predictions_by_internal_idx[original_idx_in_raw_df]\n            else:\n                predictions[i] = 0.0\n\n        if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n            print(\"WARNING: Invalid predictions detected (NaN/Inf), cleaning...\")\n            predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n\n        print(f\"Generated {len(predictions)} predictions\")\n        print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n\n        return predictions\n    \n    # Removed _prepare_sequences_for_inference as it's no longer needed\n    # def _prepare_sequences_for_inference(self, data): ...\n\n\n# Main execution function\ndef run_competition_pipeline():\n    \"\"\"Run the complete competition pipeline\"\"\"\n\n    print(\"Loading data...\")\n    train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n\n    if 'timestamp' not in train_full_raw.columns:\n        if train_full_raw.index.name == 'timestamp':\n            train_full_raw = train_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n        else:\n            train_full_raw = train_full_raw.reset_index()\n            if 'index' in train_full_raw.columns and 'timestamp' not in train_full_raw.columns:\n                 train_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                 print(f\"DEBUG: Renamed 'index' to 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in train_full_raw.columns:\n        train_full_raw['timestamp'] = pd.to_datetime(train_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in train_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in train_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: train_full_raw columns are: {train_full_raw.columns.tolist()}\")\n\n    test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n\n    if 'ID' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'ID':\n            test_full_raw = test_full_raw.reset_index(names=['ID'])\n            print(f\"DEBUG: Resetting index 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'ID' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'ID'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'timestamp':\n            test_full_raw = test_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'timestamp' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in test_full_raw.columns:\n        test_full_raw['timestamp'] = pd.to_datetime(test_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in test_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in test_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: test_full_raw columns are: {test_full_raw.columns.tolist()}\")\n\n    print(f\"\\nTrain shape: {train_full_raw.shape}\")\n    print(f\"Test shape: {test_full_raw.shape}\")\n\n    # Initialize and train model\n    predictor = CryptoMarketPredictor(\n        sequence_length=30, # Sequence length is still used for feature engineering, but not directly by XGBoost\n        top_features=100,\n        top_X_features_to_preselect=30\n    )\n    predictor.fit(train_full_raw)\n\n    predictions = predictor.predict(test_full_raw)\n\n    # Create submission\n    submission = pd.DataFrame({\n        'ID': test_full_raw['ID'],\n        'Prediction': predictions\n    })\n\n    # Save submission\n    submission.to_csv('/kaggle/working/submission.csv', index=False)\n    print(f\"Submission saved with {len(submission)} predictions\")\n    print(f\"Prediction statistics - Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n\n    return submission\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    submission = run_competition_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T07:29:24.305078Z","iopub.execute_input":"2025-07-05T07:29:24.305634Z","iopub.status.idle":"2025-07-05T07:36:04.938571Z","shell.execute_reply.started":"2025-07-05T07:29:24.30561Z","shell.execute_reply":"2025-07-05T07:36:04.937798Z"}},"outputs":[{"name":"stderr","text":"2025-07-05 07:29:27.516410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751700567.757777      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751700567.821380      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading data...\nDEBUG: Resetting index 'timestamp' for train_full_raw. New columns: ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X107', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X121', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X149', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X188', 'X189', 'X190', 'X191', 'X192', 'X193', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X233', 'X234', 'X235', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X268', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X289', 'X290', 'X291', 'X292', 'X293', 'X294', 'X295', 'X296', 'X297', 'X298', 'X299', 'X300', 'X301', 'X302', 'X303', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X330', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X347', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X381', 'X382', 'X383', 'X384', 'X385', 'X386', 'X387', 'X388', 'X389', 'X390', 'X391', 'X392', 'X393', 'X394', 'X395', 'X396', 'X397', 'X398', 'X399', 'X400', 'X401', 'X402', 'X403', 'X404', 'X405', 'X406', 'X407', 'X408', 'X409', 'X410', 'X411', 'X412', 'X413', 'X414', 'X415', 'X416', 'X417', 'X418', 'X419', 'X420', 'X421', 'X422', 'X423', 'X424', 'X425', 'X426', 'X427', 'X428', 'X429', 'X430', 'X431', 'X432', 'X433', 'X434', 'X435', 'X436', 'X437', 'X438', 'X439', 'X440', 'X441', 'X442', 'X443', 'X444', 'X445', 'X446', 'X447', 'X448', 'X449', 'X450', 'X451', 'X452', 'X453', 'X454', 'X455', 'X456', 'X457', 'X458', 'X459', 'X460', 'X461', 'X462', 'X463', 'X464', 'X465', 'X466', 'X467', 'X468', 'X469', 'X470', 'X471', 'X472', 'X473', 'X474', 'X475', 'X476', 'X477', 'X478', 'X479', 'X480', 'X481', 'X482', 'X483', 'X484', 'X485', 'X486', 'X487', 'X488', 'X489', 'X490', 'X491', 'X492', 'X493', 'X494', 'X495', 'X496', 'X497', 'X498', 'X499', 'X500', 'X501', 'X502', 'X503', 'X504', 'X505', 'X506', 'X507', 'X508', 'X509', 'X510', 'X511', 'X512', 'X513', 'X514', 'X515', 'X516', 'X517', 'X518', 'X519', 'X520', 'X521', 'X522', 'X523', 'X524', 'X525', 'X526', 'X527', 'X528', 'X529', 'X530', 'X531', 'X532', 'X533', 'X534', 'X535', 'X536', 'X537', 'X538', 'X539', 'X540', 'X541', 'X542', 'X543', 'X544', 'X545', 'X546', 'X547', 'X548', 'X549', 'X550', 'X551', 'X552', 'X553', 'X554', 'X555', 'X556', 'X557', 'X558', 'X559', 'X560', 'X561', 'X562', 'X563', 'X564', 'X565', 'X566', 'X567', 'X568', 'X569', 'X570', 'X571', 'X572', 'X573', 'X574', 'X575', 'X576', 'X577', 'X578', 'X579', 'X580', 'X581', 'X582', 'X583', 'X584', 'X585', 'X586', 'X587', 'X588', 'X589', 'X590', 'X591', 'X592', 'X593', 'X594', 'X595', 'X596', 'X597', 'X598', 'X599', 'X600', 'X601', 'X602', 'X603', 'X604', 'X605', 'X606', 'X607', 'X608', 'X609', 'X610', 'X611', 'X612', 'X613', 'X614', 'X615', 'X616', 'X617', 'X618', 'X619', 'X620', 'X621', 'X622', 'X623', 'X624', 'X625', 'X626', 'X627', 'X628', 'X629', 'X630', 'X631', 'X632', 'X633', 'X634', 'X635', 'X636', 'X637', 'X638', 'X639', 'X640', 'X641', 'X642', 'X643', 'X644', 'X645', 'X646', 'X647', 'X648', 'X649', 'X650', 'X651', 'X652', 'X653', 'X654', 'X655', 'X656', 'X657', 'X658', 'X659', 'X660', 'X661', 'X662', 'X663', 'X664', 'X665', 'X666', 'X667', 'X668', 'X669', 'X670', 'X671', 'X672', 'X673', 'X674', 'X675', 'X676', 'X677', 'X678', 'X679', 'X680', 'X681', 'X682', 'X683', 'X684', 'X685', 'X686', 'X687', 'X688', 'X689', 'X690', 'X691', 'X692', 'X693', 'X694', 'X695', 'X696', 'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X718', 'X719', 'X720', 'X721', 'X722', 'X723', 'X724', 'X725', 'X726', 'X727', 'X728', 'X729', 'X730', 'X731', 'X732', 'X733', 'X734', 'X735', 'X736', 'X737', 'X738', 'X739', 'X740', 'X741', 'X742', 'X743', 'X744', 'X745', 'X746', 'X747', 'X748', 'X749', 'X750', 'X751', 'X752', 'X753', 'X754', 'X755', 'X756', 'X757', 'X758', 'X759', 'X760', 'X761', 'X762', 'X763', 'X764', 'X765', 'X766', 'X767', 'X768', 'X769', 'X770', 'X771', 'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780', 'X781', 'X782', 'X783', 'X784', 'X785', 'X786', 'X787', 'X788', 'X789', 'X790', 'X791', 'X792', 'X793', 'X794', 'X795', 'X796', 'X797', 'X798', 'X799', 'X800', 'X801', 'X802', 'X803', 'X804', 'X805', 'X806', 'X807', 'X808', 'X809', 'X810', 'X811', 'X812', 'X813', 'X814', 'X815', 'X816', 'X817', 'X818', 'X819', 'X820', 'X821', 'X822', 'X823', 'X824', 'X825', 'X826', 'X827', 'X828', 'X829', 'X830', 'X831', 'X832', 'X833', 'X834', 'X835', 'X836', 'X837', 'X838', 'X839', 'X840', 'X841', 'X842', 'X843', 'X844', 'X845', 'X846', 'X847', 'X848', 'X849', 'X850', 'X851', 'X852', 'X853', 'X854', 'X855', 'X856', 'X857', 'X858', 'X859', 'X860', 'X861', 'X862', 'X863', 'X864', 'X865', 'X866', 'X867', 'X868', 'X869', 'X870', 'X871', 'X872', 'X873', 'X874', 'X875', 'X876', 'X877', 'X878', 'X879', 'X880', 'X881', 'X882', 'X883', 'X884', 'X885', 'X886', 'X887', 'X888', 'X889', 'X890', 'label']\nDEBUG: 'timestamp' column found and converted to datetime in train_full_raw.\nDEBUG: Resetting index 'ID' for test_full_raw. New columns: ['ID', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X107', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X121', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X149', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X188', 'X189', 'X190', 'X191', 'X192', 'X193', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X233', 'X234', 'X235', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X268', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X289', 'X290', 'X291', 'X292', 'X293', 'X294', 'X295', 'X296', 'X297', 'X298', 'X299', 'X300', 'X301', 'X302', 'X303', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X330', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X347', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X381', 'X382', 'X383', 'X384', 'X385', 'X386', 'X387', 'X388', 'X389', 'X390', 'X391', 'X392', 'X393', 'X394', 'X395', 'X396', 'X397', 'X398', 'X399', 'X400', 'X401', 'X402', 'X403', 'X404', 'X405', 'X406', 'X407', 'X408', 'X409', 'X410', 'X411', 'X412', 'X413', 'X414', 'X415', 'X416', 'X417', 'X418', 'X419', 'X420', 'X421', 'X422', 'X423', 'X424', 'X425', 'X426', 'X427', 'X428', 'X429', 'X430', 'X431', 'X432', 'X433', 'X434', 'X435', 'X436', 'X437', 'X438', 'X439', 'X440', 'X441', 'X442', 'X443', 'X444', 'X445', 'X446', 'X447', 'X448', 'X449', 'X450', 'X451', 'X452', 'X453', 'X454', 'X455', 'X456', 'X457', 'X458', 'X459', 'X460', 'X461', 'X462', 'X463', 'X464', 'X465', 'X466', 'X467', 'X468', 'X469', 'X470', 'X471', 'X472', 'X473', 'X474', 'X475', 'X476', 'X477', 'X478', 'X479', 'X480', 'X481', 'X482', 'X483', 'X484', 'X485', 'X486', 'X487', 'X488', 'X489', 'X490', 'X491', 'X492', 'X493', 'X494', 'X495', 'X496', 'X497', 'X498', 'X499', 'X500', 'X501', 'X502', 'X503', 'X504', 'X505', 'X506', 'X507', 'X508', 'X509', 'X510', 'X511', 'X512', 'X513', 'X514', 'X515', 'X516', 'X517', 'X518', 'X519', 'X520', 'X521', 'X522', 'X523', 'X524', 'X525', 'X526', 'X527', 'X528', 'X529', 'X530', 'X531', 'X532', 'X533', 'X534', 'X535', 'X536', 'X537', 'X538', 'X539', 'X540', 'X541', 'X542', 'X543', 'X544', 'X545', 'X546', 'X547', 'X548', 'X549', 'X550', 'X551', 'X552', 'X553', 'X554', 'X555', 'X556', 'X557', 'X558', 'X559', 'X560', 'X561', 'X562', 'X563', 'X564', 'X565', 'X566', 'X567', 'X568', 'X569', 'X570', 'X571', 'X572', 'X573', 'X574', 'X575', 'X576', 'X577', 'X578', 'X579', 'X580', 'X581', 'X582', 'X583', 'X584', 'X585', 'X586', 'X587', 'X588', 'X589', 'X590', 'X591', 'X592', 'X593', 'X594', 'X595', 'X596', 'X597', 'X598', 'X599', 'X600', 'X601', 'X602', 'X603', 'X604', 'X605', 'X606', 'X607', 'X608', 'X609', 'X610', 'X611', 'X612', 'X613', 'X614', 'X615', 'X616', 'X617', 'X618', 'X619', 'X620', 'X621', 'X622', 'X623', 'X624', 'X625', 'X626', 'X627', 'X628', 'X629', 'X630', 'X631', 'X632', 'X633', 'X634', 'X635', 'X636', 'X637', 'X638', 'X639', 'X640', 'X641', 'X642', 'X643', 'X644', 'X645', 'X646', 'X647', 'X648', 'X649', 'X650', 'X651', 'X652', 'X653', 'X654', 'X655', 'X656', 'X657', 'X658', 'X659', 'X660', 'X661', 'X662', 'X663', 'X664', 'X665', 'X666', 'X667', 'X668', 'X669', 'X670', 'X671', 'X672', 'X673', 'X674', 'X675', 'X676', 'X677', 'X678', 'X679', 'X680', 'X681', 'X682', 'X683', 'X684', 'X685', 'X686', 'X687', 'X688', 'X689', 'X690', 'X691', 'X692', 'X693', 'X694', 'X695', 'X696', 'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X718', 'X719', 'X720', 'X721', 'X722', 'X723', 'X724', 'X725', 'X726', 'X727', 'X728', 'X729', 'X730', 'X731', 'X732', 'X733', 'X734', 'X735', 'X736', 'X737', 'X738', 'X739', 'X740', 'X741', 'X742', 'X743', 'X744', 'X745', 'X746', 'X747', 'X748', 'X749', 'X750', 'X751', 'X752', 'X753', 'X754', 'X755', 'X756', 'X757', 'X758', 'X759', 'X760', 'X761', 'X762', 'X763', 'X764', 'X765', 'X766', 'X767', 'X768', 'X769', 'X770', 'X771', 'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780', 'X781', 'X782', 'X783', 'X784', 'X785', 'X786', 'X787', 'X788', 'X789', 'X790', 'X791', 'X792', 'X793', 'X794', 'X795', 'X796', 'X797', 'X798', 'X799', 'X800', 'X801', 'X802', 'X803', 'X804', 'X805', 'X806', 'X807', 'X808', 'X809', 'X810', 'X811', 'X812', 'X813', 'X814', 'X815', 'X816', 'X817', 'X818', 'X819', 'X820', 'X821', 'X822', 'X823', 'X824', 'X825', 'X826', 'X827', 'X828', 'X829', 'X830', 'X831', 'X832', 'X833', 'X834', 'X835', 'X836', 'X837', 'X838', 'X839', 'X840', 'X841', 'X842', 'X843', 'X844', 'X845', 'X846', 'X847', 'X848', 'X849', 'X850', 'X851', 'X852', 'X853', 'X854', 'X855', 'X856', 'X857', 'X858', 'X859', 'X860', 'X861', 'X862', 'X863', 'X864', 'X865', 'X866', 'X867', 'X868', 'X869', 'X870', 'X871', 'X872', 'X873', 'X874', 'X875', 'X876', 'X877', 'X878', 'X879', 'X880', 'X881', 'X882', 'X883', 'X884', 'X885', 'X886', 'X887', 'X888', 'X889', 'X890', 'label']\nDEBUG: Renamed 'index' to 'timestamp' for test_full_raw. New columns: ['timestamp', 'ID', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X107', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X121', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X149', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X188', 'X189', 'X190', 'X191', 'X192', 'X193', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X233', 'X234', 'X235', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X268', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X289', 'X290', 'X291', 'X292', 'X293', 'X294', 'X295', 'X296', 'X297', 'X298', 'X299', 'X300', 'X301', 'X302', 'X303', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X330', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X347', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X381', 'X382', 'X383', 'X384', 'X385', 'X386', 'X387', 'X388', 'X389', 'X390', 'X391', 'X392', 'X393', 'X394', 'X395', 'X396', 'X397', 'X398', 'X399', 'X400', 'X401', 'X402', 'X403', 'X404', 'X405', 'X406', 'X407', 'X408', 'X409', 'X410', 'X411', 'X412', 'X413', 'X414', 'X415', 'X416', 'X417', 'X418', 'X419', 'X420', 'X421', 'X422', 'X423', 'X424', 'X425', 'X426', 'X427', 'X428', 'X429', 'X430', 'X431', 'X432', 'X433', 'X434', 'X435', 'X436', 'X437', 'X438', 'X439', 'X440', 'X441', 'X442', 'X443', 'X444', 'X445', 'X446', 'X447', 'X448', 'X449', 'X450', 'X451', 'X452', 'X453', 'X454', 'X455', 'X456', 'X457', 'X458', 'X459', 'X460', 'X461', 'X462', 'X463', 'X464', 'X465', 'X466', 'X467', 'X468', 'X469', 'X470', 'X471', 'X472', 'X473', 'X474', 'X475', 'X476', 'X477', 'X478', 'X479', 'X480', 'X481', 'X482', 'X483', 'X484', 'X485', 'X486', 'X487', 'X488', 'X489', 'X490', 'X491', 'X492', 'X493', 'X494', 'X495', 'X496', 'X497', 'X498', 'X499', 'X500', 'X501', 'X502', 'X503', 'X504', 'X505', 'X506', 'X507', 'X508', 'X509', 'X510', 'X511', 'X512', 'X513', 'X514', 'X515', 'X516', 'X517', 'X518', 'X519', 'X520', 'X521', 'X522', 'X523', 'X524', 'X525', 'X526', 'X527', 'X528', 'X529', 'X530', 'X531', 'X532', 'X533', 'X534', 'X535', 'X536', 'X537', 'X538', 'X539', 'X540', 'X541', 'X542', 'X543', 'X544', 'X545', 'X546', 'X547', 'X548', 'X549', 'X550', 'X551', 'X552', 'X553', 'X554', 'X555', 'X556', 'X557', 'X558', 'X559', 'X560', 'X561', 'X562', 'X563', 'X564', 'X565', 'X566', 'X567', 'X568', 'X569', 'X570', 'X571', 'X572', 'X573', 'X574', 'X575', 'X576', 'X577', 'X578', 'X579', 'X580', 'X581', 'X582', 'X583', 'X584', 'X585', 'X586', 'X587', 'X588', 'X589', 'X590', 'X591', 'X592', 'X593', 'X594', 'X595', 'X596', 'X597', 'X598', 'X599', 'X600', 'X601', 'X602', 'X603', 'X604', 'X605', 'X606', 'X607', 'X608', 'X609', 'X610', 'X611', 'X612', 'X613', 'X614', 'X615', 'X616', 'X617', 'X618', 'X619', 'X620', 'X621', 'X622', 'X623', 'X624', 'X625', 'X626', 'X627', 'X628', 'X629', 'X630', 'X631', 'X632', 'X633', 'X634', 'X635', 'X636', 'X637', 'X638', 'X639', 'X640', 'X641', 'X642', 'X643', 'X644', 'X645', 'X646', 'X647', 'X648', 'X649', 'X650', 'X651', 'X652', 'X653', 'X654', 'X655', 'X656', 'X657', 'X658', 'X659', 'X660', 'X661', 'X662', 'X663', 'X664', 'X665', 'X666', 'X667', 'X668', 'X669', 'X670', 'X671', 'X672', 'X673', 'X674', 'X675', 'X676', 'X677', 'X678', 'X679', 'X680', 'X681', 'X682', 'X683', 'X684', 'X685', 'X686', 'X687', 'X688', 'X689', 'X690', 'X691', 'X692', 'X693', 'X694', 'X695', 'X696', 'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X718', 'X719', 'X720', 'X721', 'X722', 'X723', 'X724', 'X725', 'X726', 'X727', 'X728', 'X729', 'X730', 'X731', 'X732', 'X733', 'X734', 'X735', 'X736', 'X737', 'X738', 'X739', 'X740', 'X741', 'X742', 'X743', 'X744', 'X745', 'X746', 'X747', 'X748', 'X749', 'X750', 'X751', 'X752', 'X753', 'X754', 'X755', 'X756', 'X757', 'X758', 'X759', 'X760', 'X761', 'X762', 'X763', 'X764', 'X765', 'X766', 'X767', 'X768', 'X769', 'X770', 'X771', 'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780', 'X781', 'X782', 'X783', 'X784', 'X785', 'X786', 'X787', 'X788', 'X789', 'X790', 'X791', 'X792', 'X793', 'X794', 'X795', 'X796', 'X797', 'X798', 'X799', 'X800', 'X801', 'X802', 'X803', 'X804', 'X805', 'X806', 'X807', 'X808', 'X809', 'X810', 'X811', 'X812', 'X813', 'X814', 'X815', 'X816', 'X817', 'X818', 'X819', 'X820', 'X821', 'X822', 'X823', 'X824', 'X825', 'X826', 'X827', 'X828', 'X829', 'X830', 'X831', 'X832', 'X833', 'X834', 'X835', 'X836', 'X837', 'X838', 'X839', 'X840', 'X841', 'X842', 'X843', 'X844', 'X845', 'X846', 'X847', 'X848', 'X849', 'X850', 'X851', 'X852', 'X853', 'X854', 'X855', 'X856', 'X857', 'X858', 'X859', 'X860', 'X861', 'X862', 'X863', 'X864', 'X865', 'X866', 'X867', 'X868', 'X869', 'X870', 'X871', 'X872', 'X873', 'X874', 'X875', 'X876', 'X877', 'X878', 'X879', 'X880', 'X881', 'X882', 'X883', 'X884', 'X885', 'X886', 'X887', 'X888', 'X889', 'X890', 'label']\nDEBUG: 'timestamp' column found and converted to datetime in test_full_raw.\n\nTrain shape: (525887, 897)\nTest shape: (538150, 898)\nStarting training pipeline...\nInitially selected 30 X_n features for direct Pandas load.\nSelecting columns from initial raw training data...\nMemory usage before optimization: 148.45 MB\nCleaning data...\nData cleaning applied.\nMemory usage after optimization: 78.24 MB\nCreating time-based features...\nTime-based features created. Current shape: 525887 rows, 67 columns\nSaving engineered data to checkpoint: ./engineered_train_data_checkpoint.parquet...\nEngineered data checkpoint saved.\nData shape after feature engineering: 525887 rows, 67 columns\nFeatures shape before final selection: 525887 rows, 65 columns\nTarget shape: 525887 rows\nSelecting top 100 features from 65 features...\nValidating data before final feature selection...\nFinal data shape for feature selection: (525887, 65)\n\n--- Selected Features (65) ---\n- bid_qty\n- ask_qty\n- buy_qty\n- sell_qty\n- volume\n- X1\n- X2\n- X3\n- X4\n- X5\n- X6\n- X7\n- X8\n- X9\n- X10\n- X11\n- X12\n- X13\n- X14\n- X15\n- X16\n- X17\n- X18\n- X19\n- X20\n- X21\n- X22\n- X23\n- X24\n- X25\n- X26\n- X27\n- X28\n- X29\n- X30\n- mid_price\n- spread\n- imbalance\n- buy_sell_ratio\n- volume_ma_10\n- volume_std_10\n- volume_ma_30\n- volume_std_30\n- mid_price_ma_10\n- mid_price_std_10\n- mid_price_ma_30\n- mid_price_std_30\n- buy_qty_ma_10\n- buy_qty_std_10\n- buy_qty_ma_30\n- buy_qty_std_30\n- sell_qty_ma_10\n- sell_qty_std_10\n- sell_qty_ma_30\n- sell_qty_std_30\n- imbalance_ma_10\n- imbalance_std_10\n- imbalance_ma_30\n- imbalance_std_30\n- mid_price_lag_1\n- mid_price_lag_5\n- imbalance_lag_1\n- imbalance_lag_5\n- rsi_proxy\n- momentum\n---------------------------------------\n\nFinal data validation (after scaling)...\nFinal training data shape: (525887, 65)\nTraining set shapes: X=(439668, 65), Y=(439668,)\nValidation set shapes: X=(86219, 65), Y=(86219,)\nTraining LightGBM model...\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[6]\tvalid_0's l1: 0.703456\nLightGBM - MAE: 0.7035, Pearson Correlation: 0.0389\nTraining XGBoost model...\nXGBoost - MAE: 0.7029, Pearson Correlation: 0.0509\nEnsemble (LGBM + XGBoost) - MAE: 0.7031, Pearson Correlation: 0.0496\n\nEnsemble score: 0.0496\n\nBest individual model score: 0.0509\nFinal overall ensemble score: 0.0496\nGenerating predictions...\nSelecting columns from initial raw test data (only 37 columns)...\nMemory usage before optimization: 151.91 MB\nCleaning data...\nData cleaning applied.\nMemory usage after optimization: 80.06 MB\nCreating time-based features...\nTime-based features created. Current shape: 538150 rows, 67 columns\nTest data shape after feature engineering: 538150 rows, 67 columns\nLightGBM predictions generated.\nXGBoost predictions generated.\nGenerated 538150 predictions\nPrediction range: [-0.9657, 0.6798]\nSubmission saved with 538150 predictions\nPrediction statistics - Mean: 0.0322, Std: 0.0353\n","output_type":"stream"}],"execution_count":1}]}