{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom sklearn.metrics import make_scorer\nfrom scipy.stats import pearsonr\nimport gc\nimport datetime\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import ConvLSTM2D, Dense, Flatten, TimeDistributed, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\n\nimport lightgbm as lgb\n\n# --- Configuration ---\n# Set seed for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# Global configuration variables\nN_FEATURES_SELECT = 150  # Number of features to select using SelectKBest\nSEQUENCE_LENGTH = 60    # Timesteps for ConvLSTM input (e.g., 60 minutes)\nBATCH_SIZE = 2048       # Batch size for ConvLSTM training\nEPOCHS = 50             # Max epochs for ConvLSTM\nVALIDATION_SPLIT_DATE = '2024-01-01' # Split point for training and validation data (Jan-Feb 2024)\n\n# --- 1. Data Loading ---\nprint(\"Loading data...\")\ntry:\n    train_df = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n    test_df = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n    sample_submission_df = pd.read_csv('/kaggle/input/drw-crypto-market-prediction/sample_submission.csv')\n    print(\"Data loaded successfully.\")\nexcept FileNotFoundError as e:\n    print(f\"Error loading data: {e}. Make sure the parquet files are in the specified path.\")\n    # Exit or handle gracefully in a real competition\n    exit()\n\n# Check if 'timestamp' is an index in train_df and reset if so\nif 'timestamp' not in train_df.columns and train_df.index.name == 'timestamp':\n    print(\"Train: 'timestamp' is DataFrame index, resetting index...\")\n    train_df = train_df.reset_index()\nelif 'timestamp' not in train_df.columns:\n    print(\"Error: 'timestamp' column not found in train_df. Available columns:\", train_df.columns.tolist())\n    exit() # Exit to prevent further errors if timestamp is truly missing\n\n# Convert timestamp to datetime objects for easier time-based filtering\ntrain_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n\n# Check if 'ID' is an index in test_df and reset if so\nif 'ID' not in test_df.columns and test_df.index.name == 'ID':\n    print(\"Test: 'ID' is DataFrame index, resetting index...\")\n    test_df = test_df.reset_index()\nelif 'ID' not in test_df.columns:\n    print(\"Error: 'ID' column not found in test_df. Available columns:\", test_df.columns.tolist())\n    exit() # Exit to prevent further errors if ID is truly missing\n\n# Store original IDs for submission\ntest_ids = test_df['ID']\n\n# Drop 'ID' from test_df as it's not a feature\ntest_df = test_df.drop('ID', axis=1)\n\n# --- Memory Optimization: Downcast numerical columns to float32 ---\n# Identify all numerical columns excluding 'label' and 'ID' which will be handled separately or dropped.\ndef downcast_numerical_cols(df):\n    for col in df.select_dtypes(include=[np.float64, np.int64]).columns:\n        if col not in ['label', 'ID']: # Don't downcast target or ID columns prematurely\n            df[col] = df[col].astype(np.float32)\n    return df\n\nprint(\"Downcasting numerical columns to float32...\")\ntrain_df = downcast_numerical_cols(train_df)\ntest_df = downcast_numerical_cols(test_df)\nprint(\"Numerical columns downcasted.\")\n\n\nprint(f\"Train data shape: {train_df.shape}\")\nprint(f\"Test data shape: {test_df.shape}\")\n\n\n# --- 2. Data Preprocessing and Feature Engineering ---\n\ndef create_and_preprocess_features(df, is_train=True, scaler=None, selector=None):\n    \"\"\"\n    Applies feature engineering, handles missing values, and scales data.\n    \"\"\"\n    print(f\"Starting feature engineering and preprocessing for {'train' if is_train else 'test'} data...\")\n\n    # --- Handle Missing Values (Forward-fill for time-series) ---\n    # Apply ffill to all numerical columns\n    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n    # Exclude 'label' from ffill if it's the target in train_df\n    if 'label' in numerical_cols and not is_train:\n        numerical_cols.remove('label')\n\n    df[numerical_cols] = df[numerical_cols].ffill().bfill() # ffill then bfill to catch initial NaNs\n    print(f\"Missing values handled: {df.isnull().sum().sum()} remaining NaNs.\")\n\n    # --- Basic Feature Engineering ---\n    # Create a proxy for price from bid/ask quantities\n    # Note: bid_qty is total quantity buyers are willing to purchase at the BEST bid price\n    # ask_qty is total quantity sellers are offering to sell at the BEST ask price\n    # This 'mid_price' is a simple proxy based on quantities, not actual prices.\n    df['mid_qty_proxy'] = (df['bid_qty'] + df['ask_qty']) / 2\n\n    # Robust qty_imbalance calculation to prevent inf\n    denominator = np.maximum(df['bid_qty'] + df['ask_qty'], 1e-6) # Ensure denominator is never too small\n    df['qty_imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n\n\n    # Rolling features for key columns\n    rolling_windows = [10, 30, 60] # 10-min, 30-min, 60-min windows\n    base_cols_for_rolling = ['volume', 'buy_qty', 'sell_qty', 'mid_qty_proxy']\n\n    for col in base_cols_for_rolling:\n        for window in rolling_windows:\n            df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n            df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n            # Handle NaNs from rolling (e.g., at the beginning of the series)\n            df[f'{col}_roll_std_{window}'] = df[f'{col}_roll_std_{window}'].fillna(0) # Std dev can be NaN if window is 1 or constant\n\n    # Lagged features\n    lag_periods = [1, 2, 5] # Lag by 1, 2, 5 minutes\n    base_cols_for_lag = ['volume', 'buy_qty', 'sell_qty', 'mid_qty_proxy', 'qty_imbalance']\n\n    for col in base_cols_for_lag:\n        for lag in lag_periods:\n            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n\n    # Ensure no NaNs after rolling and lagging (fill with 0 or previous value)\n    df = df.ffill().bfill() # Apply again after feature engineering\n\n    print(f\"Engineered features added. Current shape: {df.shape}\")\n\n    # --- Explicitly handle inf values before scaling ---\n    # Replace any remaining inf/-inf with NaN and re-impute\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    print(f\"Inf values replaced with NaN. Remaining NaNs after inf replacement: {df.isnull().sum().sum()}\")\n    df = df.ffill().bfill() # Re-impute after handling inf\n    print(f\"NaNs re-imputed after inf replacement. Total NaNs: {df.isnull().sum().sum()}\")\n\n\n    # Identify features for scaling and selection\n    features_to_process = [col for col in df.columns if col not in ['timestamp', 'label', 'ID']]\n    X_processed = df[features_to_process].copy() # Ensure copy to avoid original df modification\n\n    # --- Normalization/Standardization ---\n    if is_train:\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X_processed)\n        print(\"Scaler fitted on training data.\")\n    else:\n        if scaler is None:\n            raise ValueError(\"Scaler must be provided for test data preprocessing.\")\n        X_scaled = scaler.transform(X_processed)\n        print(\"Scaler transformed test data.\")\n\n    # Convert X_scaled back to DataFrame with float32\n    X_scaled_df = pd.DataFrame(X_scaled, columns=features_to_process, index=df.index).astype(np.float32)\n\n    # --- Feature Selection (only on training data for fitting, then transform test) ---\n    if is_train:\n        y_label = df['label'].astype(np.float32) # Ensure label is float32 for consistency\n        selector = SelectKBest(mutual_info_regression, k=N_FEATURES_SELECT)\n        # Fit selector on scaled features and labels\n        selector.fit(X_scaled_df, y_label)\n        selected_features = X_scaled_df.columns[selector.get_support()].tolist()\n        print(f\"Selected {len(selected_features)} features for modeling.\")\n    else:\n        if selector is None:\n            raise ValueError(\"Selector must be provided for test data preprocessing.\")\n        selected_features = X_scaled_df.columns[selector.get_support()].tolist()\n        print(f\"Applying feature selection to test data, using {len(selected_features)} features.\")\n\n    # Return the processed dataframe with only selected features\n    return X_scaled_df[selected_features], scaler, selector, selected_features\n\n\n# --- Apply preprocessing and feature engineering to train and test data ---\n# Split train_df into training and validation sets based on timestamp\ntrain_data = train_df[train_df['timestamp'] < VALIDATION_SPLIT_DATE].copy()\nval_data = train_df[train_df['timestamp'] >= VALIDATION_SPLIT_DATE].copy()\n\nprint(f\"Training data size (before feature eng): {train_data.shape}\")\nprint(f\"Validation data size (before feature eng): {val_data.shape}\")\n\n# Process training data to fit scaler and selector\nX_train_processed, scaler, selector, selected_features = \\\n    create_and_preprocess_features(train_data, is_train=True)\ny_train = train_data['label'].astype(np.float32) # Ensure y_train is float32\n\n# Process validation data using the fitted scaler and selector\nX_val_processed, _, _, _ = \\\n    create_and_preprocess_features(val_data, is_train=False, scaler=scaler, selector=selector)\ny_val = val_data['label'].astype(np.float32) # Ensure y_val is float32\n\n# Process test data using the fitted scaler and selector\nX_test_processed, _, _, _ = \\\n    create_and_preprocess_features(test_df, is_train=False, scaler=scaler, selector=selector)\n\n# Clean up memory\ndel train_df, train_data, val_data\ngc.collect()\n\n\n# --- 3. ConvLSTM Model Input Preparation ---\n\ndef create_sequences(X, y=None, sequence_length=SEQUENCE_LENGTH):\n    \"\"\"\n    Creates sequences for ConvLSTM.\n    Input X is a DataFrame with selected features.\n    \"\"\"\n    xs, ys = [], []\n    num_features = X.shape[1]\n\n    for i in range(len(X) - sequence_length):\n        x_seq = X.iloc[i:(i + sequence_length)].values\n        # Reshape to (sequence_length, rows, cols, channels)\n        # Here, rows=1, cols=num_features, channels=1 for ConvLSTM2D with a single series.\n        x_seq = x_seq.reshape((sequence_length, 1, num_features, 1))\n        xs.append(x_seq)\n\n        if y is not None:\n            ys.append(y.iloc[i + sequence_length]) # Predict the label at the end of the sequence\n    \n    print(f\"Created {len(xs)} sequences with shape {xs[0].shape if xs else 'N/A'}\")\n    return np.array(xs).astype(np.float32), np.array(ys).astype(np.float32) if y is not None else None # Ensure output arrays are float32\n\nprint(\"Preparing ConvLSTM sequences for training...\")\nX_train_lstm, y_train_lstm = create_sequences(X_train_processed, y_train, SEQUENCE_LENGTH)\n\nprint(\"Preparing ConvLSTM sequences for validation...\")\nX_val_lstm, y_val_lstm = create_sequences(X_val_processed, y_val, SEQUENCE_LENGTH)\n\n# Clean up memory\ndel X_train_processed, y_train, X_val_processed, y_val\ngc.collect()\n\n\n# --- 4. Model Selection and Training ---\n\n# Pearson correlation as a metric for evaluation\ndef pearson_correlation_metric(y_true, y_pred):\n    if len(y_true.shape) > 1:\n        y_true = tf.squeeze(y_true)\n    if len(y_pred.shape) > 1:\n        y_pred = tf.squeeze(y_pred)\n\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    # Calculate Pearson correlation coefficient\n    mx = tf.reduce_mean(y_true)\n    my = tf.reduce_mean(y_pred)\n    xm = y_true - mx\n    ym = y_pred - my\n    r_num = tf.reduce_sum(xm * ym)\n    r_den = tf.sqrt(tf.reduce_sum(tf.square(xm)) * tf.reduce_sum(tf.square(ym)))\n    r = r_num / (r_den + 1e-8) # Add epsilon to prevent division by zero\n\n    # Ensure r is within [-1, 1] range\n    r = tf.maximum(tf.minimum(r, 1.0), -1.0)\n    return r\n\n# --- ConvLSTM Model ---\nprint(\"Building ConvLSTM model...\")\nnum_features_lstm = X_train_lstm.shape[-2] # The number of features in the sequence\nconvlstm_model = Sequential([\n    # Input shape: (timesteps, rows, cols, channels) -> (SEQUENCE_LENGTH, 1, num_features_lstm, 1)\n    ConvLSTM2D(filters=64, kernel_size=(1, 3), activation='relu',\n               input_shape=(SEQUENCE_LENGTH, 1, num_features_lstm, 1),\n               padding='same', return_sequences=True, kernel_regularizer=l2(0.001)),\n    Dropout(0.3),\n    ConvLSTM2D(filters=32, kernel_size=(1, 3), activation='relu',\n               padding='same', return_sequences=False, kernel_regularizer=l2(0.001)),\n    Dropout(0.3),\n    Flatten(), # Flatten the output of ConvLSTM to feed into Dense layers\n    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n    Dropout(0.2),\n    Dense(1, activation='linear') # Output layer for regression\n])\n\nconvlstm_model.compile(loss='mae', optimizer=Adam(learning_rate=0.001), metrics=[pearson_correlation_metric])\nconvlstm_model.summary()\n\nprint(\"Training ConvLSTM model...\")\nearly_stopping = EarlyStopping(monitor='val_pearson_correlation_metric', patience=10, mode='max', restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_pearson_correlation_metric', factor=0.5, patience=5, min_lr=1e-6, mode='max', verbose=1)\n\nhistory = convlstm_model.fit(\n    X_train_lstm, y_train_lstm,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val_lstm, y_val_lstm),\n    callbacks=[early_stopping, reduce_lr]\n)\nprint(\"ConvLSTM model training complete.\")\n\n# Clean up memory\ndel X_train_lstm, y_train_lstm, X_val_lstm, y_val_lstm\ngc.collect()\n\n\n# --- LightGBM Model ---\nprint(\"Building and training LightGBM model...\")\n# For LightGBM, we use the raw (scaled and selected) features without sequence creation.\n# Align indices for direct use in LightGBM.\nlgbm_train_X = X_train_processed.iloc[SEQUENCE_LENGTH:].copy() # Align with LSTM target\nlgbm_train_y = y_train.iloc[SEQUENCE_LENGTH:].copy()\n\nlgbm_val_X = X_val_processed.iloc[SEQUENCE_LENGTH:].copy() # Align with LSTM target\nlgbm_val_y = y_val.iloc[SEQUENCE_LENGTH:].copy()\n\nlgbm_model = lgb.LGBMRegressor(objective='mae',\n                               metric='mae', # LightGBM doesn't have native pearson, but MAE is a good proxy.\n                               n_estimators=1000,\n                               learning_rate=0.05,\n                               num_leaves=31,\n                               max_depth=-1,\n                               min_child_samples=20,\n                               subsample=0.8,\n                               colsample_bytree=0.8,\n                               random_state=42,\n                               n_jobs=-1)\n\nlgbm_model.fit(lgbm_train_X, lgbm_train_y,\n                eval_set=[(lgbm_val_X, lgbm_val_y)],\n                eval_metric='mae',\n                callbacks=[lgb.early_stopping(100, verbose=False)])\n\nprint(\"LightGBM model training complete.\")\n\n# Clean up memory\ndel lgbm_train_X, lgbm_train_y, lgbm_val_X, lgbm_val_y\ngc.collect()\n\n\n# --- 5. Prediction and Ensemble ---\n\n# Prepare test data for ConvLSTM predictions\nprint(\"Preparing ConvLSTM sequences for test data...\")\n# Test data needs to be lagged and rolled based on its own values up to the prediction point.\n# Since test timestamps are masked, we treat it as a continuous series for sequence creation.\nX_test_lstm, _ = create_sequences(X_test_processed, None, SEQUENCE_LENGTH)\n\n# Handle cases where test_df is too short for sequences\nif X_test_lstm.shape[0] == 0:\n    print(\"Test data is too short to create LSTM sequences. Skipping ConvLSTM prediction.\")\n    convlstm_preds = np.zeros(len(test_ids)) # Default to zeros\nelse:\n    print(\"Making ConvLSTM predictions...\")\n    convlstm_preds = convlstm_model.predict(X_test_lstm, batch_size=BATCH_SIZE).flatten()\n\n# The ConvLSTM predictions will be shorter than the original test_df because of sequence creation.\n# The predictions correspond to the label at `i + SEQUENCE_LENGTH`.\n# We need to map these predictions back to the original test_ids.\n# For simplicity, we'll align the predictions based on their index.\n# The first SEQUENCE_LENGTH rows of the test_df won't have ConvLSTM predictions.\n# We'll fill those with LightGBM predictions or zeros.\n\n# Pad ConvLSTM predictions to match original test_df length\n# The first `SEQUENCE_LENGTH` predictions will be missing from LSTM.\n# Let's fill those with the average of the ConvLSTM predictions or 0.\n# A more sophisticated approach would be to use LightGBM for these initial predictions.\npadded_convlstm_preds = np.zeros(len(test_ids))\nif X_test_lstm.shape[0] > 0:\n    padded_convlstm_preds[SEQUENCE_LENGTH:] = convlstm_preds\n\n\n# Make LightGBM predictions\nprint(\"Making LightGBM predictions...\")\n# LightGBM predicts for all rows where features are available.\nlgbm_preds = lgbm_model.predict(X_test_processed[selected_features])\n\n\n# --- Ensemble Predictions ---\n# Simple weighted average ensemble\n# You can tune these weights based on validation performance\nprint(\"Ensembling predictions...\")\nconvlstm_weight = 0.6\nlgbm_weight = 0.4\n\n# Apply ensemble only where ConvLSTM has predictions.\n# For the initial `SEQUENCE_LENGTH` rows, we'll rely solely on LightGBM.\nfinal_predictions = np.zeros(len(test_ids))\n\nif X_test_lstm.shape[0] > 0:\n    # Use ensemble for rows where ConvLSTM predictions exist\n    final_predictions[SEQUENCE_LENGTH:] = (convlstm_weight * padded_convlstm_preds[SEQUENCE_LENGTH:] +\n                                           lgbm_weight * lgbm_preds[SEQUENCE_LENGTH:])\n    # For the initial rows, use only LightGBM\n    final_predictions[:SEQUENCE_LENGTH] = lgbm_preds[:SEQUENCE_LENGTH]\nelse:\n    # If ConvLSTM predictions are not available, rely solely on LightGBM\n    final_predictions = lgbm_preds\n\n# Ensure predictions are float32\nfinal_predictions = final_predictions.astype(np.float32)\n\nprint(\"Predictions ensembled.\")\n\n# --- 6. Submission ---\nprint(\"Creating submission file...\")\nsubmission_df = pd.DataFrame({'ID': test_ids, 'Prediction': final_predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Submission file 'submission.csv' created successfully.\")\nprint(submission_df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:16:12.498612Z","iopub.execute_input":"2025-06-21T14:16:12.498896Z","execution_failed":"2025-06-21T14:18:51.350Z"}},"outputs":[{"name":"stderr","text":"2025-06-21 14:16:15.041310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750515375.240846      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750515375.300820      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading data...\nData loaded successfully.\nTrain: 'timestamp' is DataFrame index, resetting index...\nTest: 'ID' is DataFrame index, resetting index...\nDowncasting numerical columns to float32...\nNumerical columns downcasted.\nTrain data shape: (525887, 897)\nTest data shape: (538150, 896)\nTraining data size (before feature eng): (439668, 897)\nValidation data size (before feature eng): (86219, 897)\nStarting feature engineering and preprocessing for train data...\nMissing values handled: 0 remaining NaNs.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/1143487987.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['mid_qty_proxy'] = (df['bid_qty'] + df['ask_qty']) / 2\n/tmp/ipykernel_35/1143487987.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df['qty_imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_mean_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n/tmp/ipykernel_35/1143487987.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_roll_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n/tmp/ipykernel_35/1143487987.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n","output_type":"stream"},{"name":"stdout","text":"Engineered features added. Current shape: (439668, 938)\nInf values replaced with NaN. Remaining NaNs after inf replacement: 9233028\nNaNs re-imputed after inf replacement. Total NaNs: 9233028\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}