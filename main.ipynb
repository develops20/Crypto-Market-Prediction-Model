{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-nb153?scriptVersionId=248237309\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# DRW Crypto Market Prediction Competition Pipeline\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os # Import os for path checking\nimport gc # Import garbage collector\nimport math # For ceil in data generator\n\n# Memory optimization and data processing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\n# Deep learning and modeling\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (LSTM, ConvLSTM2D, Dense, Dropout,\n                                     BatchNormalization, Input, Conv1D, MaxPooling1D,\n                                     Flatten, Reshape, TimeDistributed)\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence # Import Keras Sequence for data generation\n\n# Tree-based models for ensemble\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\n# Set memory growth for GPU (if available) - still good practice\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n\nclass TimeSeriesSequence(Sequence):\n    \"\"\"\n    Keras Sequence for loading time-series data in batches from Parquet files,\n    preparing sequences for deep learning models.\n    \"\"\"\n    def __init__(self, X_filepath, y_filepath, sequence_length, batch_size, feature_cols, total_samples, original_indices):\n        self.X_filepath = X_filepath\n        self.y_filepath = y_filepath\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.feature_cols = feature_cols # List of feature column names\n        self.original_indices = original_indices # The actual Pandas Index used when saving to Parquet\n\n        # Calculate the number of samples that can form a full sequence\n        # A sequence for label at index `i` requires features from `i - sequence_length + 1` to `i`.\n        # So, the first label we can predict is at `sequence_length - 1` (0-indexed).\n        self.num_predictable_samples = max(0, len(self.original_indices) - self.sequence_length + 1)\n\n        print(f\"TimeSeriesSequence initialized for {X_filepath}: \"\n              f\"Total rows (from original_indices): {len(self.original_indices)}, \"\n              f\"Predictable samples: {self.num_predictable_samples}, \"\n              f\"Batch size: {self.batch_size}, Sequence length: {self.sequence_length}\")\n\n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch.\"\"\"\n        if self.num_predictable_samples == 0:\n            return 0\n        return math.ceil(self.num_predictable_samples / self.batch_size)\n\n    def __getitem__(self, idx):\n        \"\"\"Generates one batch of data.\"\"\"\n        # Calculate the global indices of the *labels* for this batch\n        # These are indices within the `self.original_indices` list.\n        batch_label_start_idx_in_original_indices = idx * self.batch_size + (self.sequence_length - 1)\n        batch_label_end_idx_in_original_indices = min(batch_label_start_idx_in_original_indices + self.batch_size, len(self.original_indices)) - 1\n\n        if batch_label_start_idx_in_original_indices > batch_label_end_idx_in_original_indices:\n            print(f\"Warning: Batch {idx} - calculated label range is empty. Returning empty.\")\n            return np.array([]), np.array([])\n\n        # Get the actual index values (from original Pandas Index) for the labels in this batch\n        actual_label_indices = self.original_indices[batch_label_start_idx_in_original_indices : batch_label_end_idx_in_original_indices + 1]\n\n        # Determine the full range of original indices needed to form sequences for these labels\n        # The earliest feature index needed is for the start of the first sequence in this batch\n        first_sequence_start_idx_in_original_indices = batch_label_start_idx_in_original_indices - (self.sequence_length - 1)\n\n        # The latest feature index needed is the end of the last sequence in this batch\n        last_sequence_end_idx_in_original_indices = batch_label_end_idx_in_original_indices\n\n        # Get the actual original index values for the full feature chunk needed\n        actual_feature_chunk_indices = self.original_indices[first_sequence_start_idx_in_original_indices : last_sequence_end_idx_in_original_indices + 1]\n\n        if len(actual_feature_chunk_indices) == 0 or len(actual_label_indices) == 0:\n            print(f\"Warning: Batch {idx} - insufficient data to form sequences. Returning empty.\")\n            return np.array([]), np.array([])\n\n        # Read the slice of X data using `filters` on the index column (named '__index_level_0__')\n        # This approach is more robust than trying to infer row numbers when data might be sparse or not perfectly contiguous.\n        X_batch_df = pd.read_parquet(\n            self.X_filepath,\n            columns=self.feature_cols,\n            engine='pyarrow',\n            filters=[('__index_level_0__', '>=', actual_feature_chunk_indices.min()),\n                     ('__index_level_0__', '<=', actual_feature_chunk_indices.max())]\n        )\n        # Re-index X_batch_df to ensure it matches the order of actual_feature_chunk_indices\n        # and contains only the rows explicitly needed, not extra rows between min/max filters.\n        X_batch_df = X_batch_df.loc[actual_feature_chunk_indices]\n\n        # Read the slice of Y data\n        y_batch_df = pd.read_parquet(\n            self.y_filepath,\n            columns=['label'],\n            engine='pyarrow',\n            filters=[('__index_level_0__', '>=', actual_label_indices.min()),\n                     ('__index_level_0__', '<=', actual_label_indices.max())]\n        )\n        # Re-index y_batch_df to ensure it matches the order of actual_label_indices\n        y_batch_df = y_batch_df.loc[actual_label_indices]\n\n        # Convert to numpy arrays\n        X_batch_np = X_batch_df.values.astype(np.float32)\n        y_batch_np = y_batch_df.values.flatten().astype(np.float32)\n\n        del X_batch_df, y_batch_df # Free memory\n        gc.collect()\n\n        # Prepare sequences from the loaded chunk (X_batch_np)\n        sequences = []\n        targets = []\n\n        # The labels (y_batch_np) directly correspond to the *end* of the sequences.\n        # The number of sequences will be len(y_batch_np).\n        # We need to construct sequences from X_batch_np such that each sequence corresponds\n        # to a label in y_batch_np.\n\n        # Calculate the starting offset within X_batch_np for the first sequence\n        # X_batch_np starts at `actual_feature_chunk_indices.min()`\n        # The first label is `actual_label_indices.min()`\n        # Its sequence starts at `actual_label_indices.min() - self.sequence_length + 1`\n        # So the offset is `(actual_label_indices.min() - self.sequence_length + 1) - actual_feature_chunk_indices.min()`\n        first_sequence_start_offset_in_chunk = (actual_label_indices.min() - self.sequence_length + 1) - actual_feature_chunk_indices.min()\n\n        if first_sequence_start_offset_in_chunk < 0:\n             # This indicates misalignment or not enough look-back data in the chunk\n             # Should be handled by `actual_feature_chunk_indices` construction, but as a safeguard:\n             print(f\"Warning: Negative offset for sequence start in batch {idx}. Adjusting to 0. This may indicate an issue.\")\n             first_sequence_start_offset_in_chunk = 0\n\n\n        for i in range(len(y_batch_np)):\n            # The current sequence starts at `first_sequence_start_offset_in_chunk + i`\n            # and ends at `first_sequence_start_offset_in_chunk + i + self.sequence_length - 1`\n            seq_start_in_chunk = first_sequence_start_offset_in_chunk + i\n            seq_end_in_chunk = seq_start_in_chunk + self.sequence_length\n\n            if seq_end_in_chunk > len(X_batch_np):\n                # Should not happen with correct slicing, but safety check\n                print(f\"Warning: Not enough feature data in chunk for sequence {i} in batch {idx}. Breaking.\")\n                break\n\n            sequences.append(X_batch_np[seq_start_in_chunk : seq_end_in_chunk])\n            targets.append(y_batch_np[i])\n\n        X_sequences = np.array(sequences).astype(np.float32)\n        y_sequences = np.array(targets).astype(np.float32)\n\n        if len(X_sequences) == 0:\n            print(f\"Warning: Batch {idx} resulted in 0 sequences. Returning empty.\")\n            return np.array([]).astype(np.float32).reshape(0, self.sequence_length, len(self.feature_cols)), \\\n                   np.array([]).astype(np.float32)\n\n        del X_batch_np, y_batch_np # Clear memory after forming sequences\n        gc.collect()\n\n        return X_sequences, y_sequences\n\n\nclass CryptoMarketPredictor:\n    def __init__(self, sequence_length=30, top_features=100, top_X_features_to_preselect=30):\n        self.sequence_length = sequence_length\n        self.top_features = top_features\n        self.top_X_features_to_preselect = top_X_features_to_preselect\n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None # Stores final selected feature names\n        self.models = {}\n        # Path for the initial processed data after feature engineering\n        self._engineered_data_checkpoint_path = './engineered_train_data_checkpoint.parquet'\n        # Paths for scaled and feature-selected data, to be read by the generator\n        self._scaled_X_path = './scaled_train_X.parquet'\n        self._scaled_y_path = './scaled_train_y.parquet'\n        self._scaled_val_X_path = './scaled_val_X.parquet'\n        self._scaled_val_y_path = './scaled_val_y.parquet'\n\n\n    def optimize_memory(self, df):\n        \"\"\"\n        Optimize Pandas DataFrame memory usage and clean data.\n        \"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n        # Clean data first\n        df = self.clean_data(df)\n\n        # Optimize numeric columns (Pandas directly)\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col == 'timestamp' or col == 'ID' or col == 'label':\n                continue # Don't downcast timestamp, ID, or label\n            df[col] = pd.to_numeric(df[col], downcast='float')\n\n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n\n    def clean_data(self, df):\n        \"\"\"\n        Clean Pandas DataFrame by handling inf, -inf, and extreme values.\n        \"\"\"\n        print(\"Cleaning data...\")\n\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        # Replace inf and -inf with NaN first\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n        # Fill NaN values with forward fill, then backward fill, then 0\n        for col in numeric_cols:\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        # Handle extreme outliers (values beyond 3*IQR)\n        for col in df.select_dtypes(include=[np.float32, np.float64]).columns:\n            if col in ['timestamp', 'ID', 'label']: continue\n            if df[col].nunique() > 1:\n                q25 = df[col].quantile(0.25)\n                q75 = df[col].quantile(0.75)\n                iqr = q75 - q25\n\n                if iqr != 0 and not pd.isna(iqr):\n                    lower_bound = q25 - 3 * iqr\n                    upper_bound = q75 + 3 * iqr\n                    df[col] = df[col].clip(lower_bound, upper_bound)\n        print(\"Data cleaning applied.\")\n        return df\n\n    def create_time_features(self, df):\n        \"\"\"\n        Create time-based features with robust calculations using Pandas.\n        Significantly reduced complexity for faster execution.\n        \"\"\"\n        print(\"Creating time-based features...\")\n\n        # Basic market features\n        df['mid_price'] = (df['bid_qty'] + df['ask_qty']) / 2\n        df['spread'] = df['ask_qty'] - df['bid_qty']\n\n        # Safe division for imbalance\n        denominator = df['bid_qty'] + df['ask_qty'] + 1e-10\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n\n        # Safe division for buy/sell ratio\n        df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-10)\n\n        # Rolling statistics - significantly reduced windows for speed\n        windows = [10, 30]\n        base_cols_for_rolling = ['volume', 'mid_price', 'buy_qty', 'sell_qty', 'imbalance']\n\n        for col in base_cols_for_rolling:\n            for window in windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)\n\n        # Lagged features - significantly reduced lags for speed\n        lags = [1, 5]\n        base_cols_for_lag = ['mid_price', 'imbalance']\n\n        for col in base_cols_for_lag:\n            for lag in lags:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n\n        # Technical indicators - reduced\n        df['rsi_proxy'] = self.calculate_rsi_proxy(df['mid_price'], window=10)\n        df['momentum'] = df['mid_price'] - df['mid_price'].shift(5)\n\n        # Final check for any inf/nan values that might have been introduced\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        print(f\"Time-based features created. Current shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n        return df\n\n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate RSI-like indicator with safe operations for Pandas Series.\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n\n        rs = gain / (loss + 1e-10)\n        rsi = 100 - (100 / (1 + rs))\n\n        rsi = rsi.replace([np.inf, -np.inf], np.nan).fillna(50)\n\n        return rsi\n\n    def select_features(self, X_df, y_df, method='mutual_info'):\n        \"\"\"\n        Feature selection to reduce dimensionality with robust handling.\n        Operates directly on Pandas DataFrames.\n        \"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n\n        print(\"Validating data before final feature selection...\")\n\n        # Check for any remaining inf/nan values\n        inf_mask = np.isinf(X_df).any(axis=1)\n        nan_mask = np.isnan(X_df).any(axis=1)\n        invalid_mask = inf_mask | nan_mask\n\n        if invalid_mask.sum() > 0:\n            print(f\"Removing {invalid_mask.sum()} rows with invalid values before final selection.\")\n            X_df = X_df[~invalid_mask]\n            y_df = y_df[~invalid_mask]\n\n        # Check for constant or near-constant features (can cause issues for some selectors)\n        feature_std = X_df.std()\n        constant_features_mask = feature_std < 1e-8\n\n        if constant_features_mask.all():\n            print(\"Warning: All features are constant. Cannot perform feature selection.\")\n            non_constant_features = X_df.columns.tolist()\n        else:\n            non_constant_features = X_df.columns[~constant_features_mask].tolist()\n            if constant_features_mask.sum() > 0:\n                print(f\"Removing {constant_features_mask.sum()} constant features.\")\n            X_df = X_df[non_constant_features]\n\n        print(f\"Final data shape for feature selection: {X_df.shape}\")\n\n        n_features_to_select = min(self.top_features, X_df.shape[1])\n\n        if method == 'mutual_info':\n            selector = SelectKBest(score_func=mutual_info_regression, k=n_features_to_select)\n        else:\n            selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n\n        X_selected = selector.fit_transform(X_df, y_df)\n        self.feature_selector = selector\n        self.selected_features = X_df.columns[selector.get_support()].tolist()\n\n        print(f\"\\n--- Selected Features ({len(self.selected_features)}) ---\")\n        for feature in self.selected_features:\n            print(f\"- {feature}\")\n        print(\"---------------------------------------\\n\")\n\n        # Return as NumPy array and Pandas Index to maintain alignment with y\n        return X_selected.astype(np.float32), X_df.index\n\n    def build_convlstm_model(self, input_shape):\n        \"\"\"Build ConvLSTM model for spatial-temporal patterns\"\"\"\n        model = Sequential([\n            # Reshape for ConvLSTM (samples, time_steps, rows, cols, channels)\n            # input_shape is (sequence_length, num_features)\n            # We need (sequence_length, 1, num_features, 1)\n            Reshape((self.sequence_length, 1, input_shape[1], 1), input_shape=input_shape),\n\n            ConvLSTM2D(filters=64, kernel_size=(1, 3), # Reverted filters\n                       activation='tanh', recurrent_activation='sigmoid',\n                       return_sequences=True, dropout=0.2, padding='same'),\n            BatchNormalization(),\n\n            ConvLSTM2D(filters=32, kernel_size=(1, 3), # Reverted filters\n                       activation='tanh', recurrent_activation='sigmoid',\n                       return_sequences=False, dropout=0.2, padding='same'),\n            BatchNormalization(),\n\n            Flatten(),\n            Dense(50, activation='relu'), # Reverted dense units\n            Dropout(0.3),\n            Dense(1, activation='linear')\n        ])\n\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss='mae', metrics=['mae'])\n        return model\n\n    def build_lstm_model(self, input_shape):\n        \"\"\"Build standard LSTM model\"\"\"\n        model = Sequential([\n            LSTM(100, return_sequences=True, input_shape=input_shape, dropout=0.2), # Reverted LSTM units\n            BatchNormalization(),\n            LSTM(50, return_sequences=False, dropout=0.2), # Reverted LSTM units\n            BatchNormalization(),\n            Dense(50, activation='relu'), # Reverted dense units\n            Dropout(0.3),\n            Dense(1, activation='linear')\n        ])\n\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss='mae', metrics=['mae'])\n        return model\n\n    def build_cnn_lstm_model(self, input_shape):\n        print(\"CNN-LSTM model building skipped for current speed optimization, but can be reinstated later.\")\n        return None\n\n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train LightGBM model\"\"\"\n        print(\"Training LightGBM model...\")\n\n        # Ensure no NaNs are passed to LightGBM\n        X_train_clean = np.nan_to_num(X_train.values, nan=0.0, posinf=1e6, neginf=-1e6)\n        y_train_clean = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        X_val_clean = np.nan_to_num(X_val.values, nan=0.0, posinf=1e6, neginf=-1e6)\n        y_val_clean = np.nan_to_num(y_val, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        train_data = lgb.Dataset(X_train_clean, label=y_train_clean)\n        val_data = lgb.Dataset(X_val_clean, label=y_val_clean, reference=train_data)\n\n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 1000\n        }\n\n        model = lgb.train(\n            params,\n            train_data,\n            valid_sets=[val_data],\n            num_boost_round=params['n_estimators'],\n            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n        )\n\n        return model\n\n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance\"\"\"\n        # Ensure y_true and y_pred are clean before evaluation\n        y_true_clean = np.nan_to_num(y_true, nan=0.0, posinf=1e6, neginf=-1e6)\n        y_pred_clean = np.nan_to_num(y_pred, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        mae = mean_absolute_error(y_true_clean, y_pred_clean)\n\n        # Pearson correlation requires at least 2 non-constant values for both arrays\n        correlation = 0.0\n        if len(np.unique(y_true_clean)) > 1 and len(np.unique(y_pred_clean)) > 1:\n            try:\n                correlation, _ = pearsonr(y_true_clean, y_pred_clean)\n            except ValueError:\n                # Can happen if inputs have 0 variance after cleaning\n                correlation = 0.0\n\n        print(f\"{model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n\n    def fit(self, train_data_raw_initial_load):\n        \"\"\"Main training pipeline with robust error handling\"\"\"\n        print(\"Starting training pipeline...\")\n\n        # Determine raw X_n columns from the initial load structure\n        X_n_cols_raw = [col for col in train_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        basic_features_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n\n        preselected_X_n_features = X_n_cols_raw[:min(self.top_X_features_to_preselect, len(X_n_cols_raw))]\n        print(f\"Initially selected {len(preselected_X_n_features)} X_n features for direct Pandas load.\")\n\n        columns_to_process_raw_for_fit = basic_features_cols + preselected_X_n_features + ['label']\n\n        train_df = None\n        if os.path.exists(self._engineered_data_checkpoint_path):\n            print(f\"Checkpoint found. Loading processed data from {self._engineered_data_checkpoint_path}...\")\n            train_df = pd.read_parquet(self._engineered_data_checkpoint_path)\n            if 'timestamp' in train_df.columns:\n                train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n        else:\n            print(f\"Selecting columns from initial raw training data...\")\n            train_df = train_data_raw_initial_load[columns_to_process_raw_for_fit].copy()\n            if 'timestamp' not in train_df.columns and train_df.index.name == 'timestamp':\n                train_df = train_df.reset_index(names=['timestamp'])\n            train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n            gc.collect()\n            train_df = self.optimize_memory(train_df)\n            train_df = self.create_time_features(train_df)\n            print(f\"Saving engineered data to checkpoint: {self._engineered_data_checkpoint_path}...\")\n            train_df.to_parquet(self._engineered_data_checkpoint_path, index=False)\n            print(\"Engineered data checkpoint saved.\")\n\n        print(f\"Data shape after feature engineering: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n        feature_cols_final = [col for col in train_df.columns\n                              if col not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols_final]\n        y_df = train_df['label']\n        print(f\"Features shape before final selection: {X_df.shape[0]} rows, {X_df.shape[1]} columns\")\n        print(f\"Target shape: {y_df.shape[0]} rows\")\n\n        X_selected, valid_idx = self.select_features(X_df, y_df)\n        y_for_training = y_df.loc[valid_idx].astype(np.float32)\n\n        del X_df, y_df\n        gc.collect()\n\n        X_scaled = self.scaler.fit_transform(X_selected)\n        print(\"Final data validation (after scaling)...\")\n        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n            print(\"ERROR: Still have invalid values after preprocessing! Applying emergency cleanup.\")\n            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        print(f\"Final training data shape: {X_scaled.shape}\")\n\n        del X_selected\n        gc.collect()\n\n        # Split data into training and validation sets\n        # IMPORTANT: Use the index from the `train_df` (which is already processed and filtered by `select_features`' `valid_idx`)\n        # to ensure alignment with X_scaled and y_for_training.\n        # train_data_raw_initial_load is too large and might have different index after cleanup if it was not reset.\n\n        # Determine actual indices for train/val split based on timestamp column of engineered data\n        # We need to re-create the full dataframe from X_scaled and y_for_training with its original indices\n        full_processed_data_for_split = pd.DataFrame(X_scaled, index=valid_idx, columns=self.selected_features)\n        full_processed_data_for_split['label'] = y_for_training\n\n        # Merge with timestamps from the engineered checkpoint data to retain alignment\n        # For simplicity, load the 'timestamp' column from the *engineered* checkpoint\n        # to align with its filtered/cleaned data indices.\n        # This assumes _engineered_data_checkpoint_path parquet file includes the index, or is sequential.\n        # If train_df was saved with index=False, this becomes problematic.\n        # Let's ensure _engineered_data_checkpoint_path always saves index for join.\n\n        # If train_df was saved without index, we need to load it fully just for timestamp and then merge.\n        # However, the previous approach of using original_train_indices based on `train_data_raw_initial_load`\n        # and then intersecting with `valid_idx` is robust for ensuring alignment.\n        # Let's stick with that for now, assuming `train_data_raw_initial_load`'s timestamp indices are sequential.\n\n        temp_original_timestamps_df = train_data_raw_initial_load[['timestamp']].copy()\n        # Ensure timestamp is datetime and index is sequential (if it was from original load)\n        if 'timestamp' not in temp_original_timestamps_df.columns and temp_original_timestamps_df.index.name == 'timestamp':\n            temp_original_timestamps_df = temp_original_timestamps_df.reset_index(names=['timestamp'])\n        temp_original_timestamps_df['timestamp'] = pd.to_datetime(temp_original_timestamps_df['timestamp'])\n\n        VALIDATION_SPLIT_DATE = '2024-01-01'\n        original_train_indices_based_on_timestamp = temp_original_timestamps_df[temp_original_timestamps_df['timestamp'] < VALIDATION_SPLIT_DATE].index\n        original_val_indices_based_on_timestamp = temp_original_timestamps_df[temp_original_timestamps_df['timestamp'] >= VALIDATION_SPLIT_DATE].index\n\n        del temp_original_timestamps_df # Free up memory\n        gc.collect()\n\n        # Intersect indices to ensure correct slicing after any data cleaning/filtering in select_features\n        # These are the *actual row indices* from the original full dataset for train/val splits\n        actual_train_indices = original_train_indices_based_on_timestamp.intersection(full_processed_data_for_split.index)\n        actual_val_indices = original_val_indices_based_on_timestamp.intersection(full_processed_data_for_split.index)\n\n        # Separate X and Y for train and validation using .loc to maintain alignment\n        X_train_df = full_processed_data_for_split.loc[actual_train_indices, self.selected_features]\n        y_train_series = full_processed_data_for_split.loc[actual_train_indices, 'label']\n        X_val_df = full_processed_data_for_split.loc[actual_val_indices, self.selected_features]\n        y_val_series = full_processed_data_for_split.loc[actual_val_indices, 'label']\n\n        # Save these as Parquet files for the generator to read. Important to save index=True.\n        X_train_df.to_parquet(self._scaled_X_path, index=True)\n        y_train_series.to_frame(name='label').to_parquet(self._scaled_y_path, index=True) # Save as DataFrame with 'label' column\n        X_val_df.to_parquet(self._scaled_val_X_path, index=True)\n        y_val_series.to_frame(name='label').to_parquet(self._scaled_val_y_path, index=True) # Save as DataFrame with 'label' column\n\n        print(\"Scaled training and validation data saved to separate Parquet files with original indices.\")\n\n        # Delete large in-memory objects after saving them to disk\n        del full_processed_data_for_split, X_train_df, y_train_series, X_val_df, y_val_series\n        gc.collect()\n\n        # Create Data Generators\n        train_generator = TimeSeriesSequence(\n            X_filepath=self._scaled_X_path,\n            y_filepath=self._scaled_y_path,\n            sequence_length=self.sequence_length,\n            batch_size=32,\n            feature_cols=self.selected_features,\n            total_samples=len(actual_train_indices), # Total samples for this generator's data\n            original_indices=actual_train_indices # Pass the actual Pandas Index\n        )\n        val_generator = TimeSeriesSequence(\n            X_filepath=self._scaled_val_X_path,\n            y_filepath=self._scaled_val_y_path,\n            sequence_length=self.sequence_length,\n            batch_size=32,\n            feature_cols=self.selected_features,\n            total_samples=len(actual_val_indices), # Total samples for this generator's data\n            original_indices=actual_val_indices # Pass the actual Pandas Index\n        )\n        print(\"Keras Data Generators created.\")\n\n        try:\n            # For LightGBM, we still need to load the full train/val sets into memory\n            # as it does not inherently support Keras-style generators.\n            # These are now loaded from the *scaled* parquet files.\n            lgb_X_train_mem = pd.read_parquet(self._scaled_X_path)\n            lgb_y_train_mem = pd.read_parquet(self._scaled_y_path)['label'].values\n            lgb_X_val_mem = pd.read_parquet(self._scaled_val_X_path)\n            lgb_y_val_mem = pd.read_parquet(self._scaled_val_y_path)['label'].values\n\n            lgb_model = self.train_lightgbm(\n                lgb_X_train_mem, lgb_y_train_mem, lgb_X_val_mem, lgb_y_val_mem\n            )\n            lgb_pred = lgb_model.predict(lgb_X_val_mem) # Predict on the already loaded val data\n            lgb_score = self.evaluate_model(lgb_y_val_mem, lgb_pred, \"LightGBM\")\n            self.models['lightgbm'] = lgb_model\n\n            del lgb_X_train_mem, lgb_y_train_mem, lgb_X_val_mem, lgb_y_val_mem, lgb_pred # Clear LGBM-specific data\n            gc.collect()\n\n        except Exception as e:\n            print(f\"LightGBM training or prediction failed: {e}\")\n            lgb_score = 0\n\n        # The y_val for evaluation of DL models needs to be derived from the actual labels\n        # that the generator will produce. For simplicity, we can load the full y_val from its checkpoint\n        # and then slice it by sequence_length for evaluation.\n        full_y_val_for_dl_eval = pd.read_parquet(self._scaled_val_y_path)['label'].values\n        # Ensure we only evaluate on the part where sequences can be formed (matching generator output)\n        y_val_dl_eval_aligned = full_y_val_for_dl_eval[self.sequence_length - 1:]\n\n        print(f\"Full Y_val for DL evaluation shape: {y_val_dl_eval_aligned.shape}\")\n\n\n        try:\n            # Check if generators are not empty before training DL models\n            if len(train_generator) > 0:\n                print(f\"Proceeding with Deep Learning models. Training generator has {len(train_generator)} batches.\")\n                callbacks = [\n                    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_mae'),\n                    ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6, monitor='val_mae')\n                ]\n                convlstm_score = 0\n                try:\n                    print(\"Training ConvLSTM model...\")\n                    # Get input shape from the generator's first item to ensure correct model build\n                    dummy_X, _ = train_generator.__getitem__(0)\n                    input_shape_convlstm = dummy_X.shape[1:] # (sequence_length, num_features)\n                    del dummy_X\n                    gc.collect()\n\n                    convlstm_model = self.build_convlstm_model(input_shape_convlstm)\n                    if convlstm_model:\n                        convlstm_model.fit(\n                            train_generator,\n                            validation_data=val_generator,\n                            epochs=5, # Initial conservative\n                            callbacks=callbacks, verbose=1,\n                            # workers=os.cpu_count(), use_multiprocessing=True # Enable for faster I/O if needed\n                        )\n                        convlstm_pred = convlstm_model.predict(val_generator).flatten()\n                        convlstm_score = self.evaluate_model(y_val_dl_eval_aligned, convlstm_pred, \"ConvLSTM\")\n                        self.models['convlstm'] = convlstm_model\n                except Exception as e:\n                    print(f\"ConvLSTM training or prediction failed: {e}\")\n                    convlstm_score = 0\n                lstm_score = 0\n                try:\n                    print(\"Training LSTM model...\")\n                    dummy_X, _ = train_generator.__getitem__(0)\n                    input_shape_lstm = dummy_X.shape[1:]\n                    del dummy_X\n                    gc.collect()\n\n                    lstm_model = self.build_lstm_model(input_shape_lstm)\n                    if lstm_model:\n                        lstm_model.fit(\n                            train_generator,\n                            validation_data=val_generator,\n                            epochs=5, # Initial conservative\n                            callbacks=callbacks, verbose=1,\n                            # workers=os.cpu_count(), use_multiprocessing=True\n                        )\n                        lstm_pred = lstm_model.predict(val_generator).flatten()\n                        lstm_score = self.evaluate_model(y_val_dl_eval_aligned, lstm_pred, \"LSTM\")\n                        self.models['lstm'] = lstm_model\n                except Exception as e:\n                    print(f\"LSTM training or prediction failed: {e}\")\n                    lstm_score = 0\n\n                if len(self.models) > 1:\n                    # Align LGBM predictions for the ensemble.\n                    # LGBM was trained on X_val, y_val. DL models were trained on sequences from X_val, y_val.\n                    # The DL predictions start from `sequence_length - 1` onwards in the original validation set.\n                    # lgb_pred is the full LGBM prediction on X_val (not sequence-aligned).\n                    # We need to ensure lgb_pred is defined before trying to slice it.\n                    if 'lgb_pred' in locals():\n                        lgb_pred_aligned = lgb_pred[self.sequence_length - 1:]\n                    else:\n                        print(\"LightGBM prediction not available for ensemble alignment.\")\n                        lgb_pred_aligned = np.array([]) # Empty array if LGBM failed\n\n                    if 'convlstm' in self.models and convlstm_score > 0:\n                        dl_predictions_list.append(convlstm_pred)\n                        dl_weights.append(0.35)\n                    if 'lstm' in self.models and lstm_score > 0:\n                        dl_predictions_list.append(lstm_pred)\n                        dl_weights.append(0.25)\n\n                    if dl_predictions_list and lgb_pred_aligned.size > 0 and len(dl_predictions_list[0]) == len(lgb_pred_aligned):\n                        dl_ensemble_weighted = np.average(dl_predictions_list, axis=0, weights=dl_weights)\n                        ensemble_pred = (lgb_pred_aligned * 0.4) + (dl_ensemble_weighted * 0.6)\n                        ensemble_score = self.evaluate_model(y_val_dl_eval_aligned, ensemble_pred, \"Ensemble\")\n                        print(f\"\\nEnsemble score: {ensemble_score:.4f}\")\n                    else:\n                        print(\"Not enough successful models or prediction lengths mismatch for ensemble.\")\n                        ensemble_score = lgb_score\n                else:\n                    print(\"Only LightGBM model was successfully trained or no DL models could be built. Skipping ensemble of DL models.\")\n                    ensemble_score = lgb_score\n                print(f\"\\nBest individual model score: {max(lgb_score, convlstm_score, lstm_score) if 'convlstm' in self.models else lgb_score:.4f}\")\n                print(f\"Final overall ensemble score: {ensemble_score:.4f}\")\n            else:\n                print(\"Skipping deep learning models as training generator is empty (e.g., due to insufficient data for sequence_length).\")\n                ensemble_score = lgb_score\n        except Exception as e:\n            print(f\"Deep learning training failed during overall process (likely model build or generator issue): {e}\")\n            ensemble_score = lgb_score\n\n        # Clean up temporary scaled data files\n        for path in [self._scaled_X_path, self._scaled_y_path, self._scaled_val_X_path, self._scaled_val_y_path]:\n            if os.path.exists(path):\n                os.remove(path)\n                print(f\"Cleaned up temporary file: {path}\")\n        gc.collect()\n        return self\n\n    def predict(self, test_data_raw_initial_load):\n        \"\"\"Generate predictions for test data with robust error handling\"\"\"\n        print(\"Generating predictions...\")\n\n        temp_df_for_id_map = test_data_raw_initial_load.copy()\n        if 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index.name == 'ID':\n            temp_df_for_id_map = temp_df_for_id_map.reset_index(names=['ID'])\n        elif 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index is not None and temp_df_for_id_map.index.name is None and len(temp_df_for_id_map.index) == len(temp_df_for_id_map):\n            temp_df_for_id_map = temp_df_for_id_map.reset_index()\n            temp_df_for_id_map.rename(columns={'index': 'ID'}, inplace=True)\n\n        id_to_original_index_map = pd.Series(temp_df_for_id_map.index.values, index=temp_df_for_id_map['ID'])\n        original_test_ids = temp_df_for_id_map['ID'].copy()\n\n        del temp_df_for_id_map\n        gc.collect()\n\n        basic_features_cols_test = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'ID']\n        X_n_cols_raw_test = [col for col in test_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        preselected_X_n_features_test = X_n_cols_raw_test[:min(self.top_X_features_to_preselect, len(X_n_cols_raw_test))]\n\n        columns_to_process_raw_for_predict = basic_features_cols_test + preselected_X_n_features_test\n\n        missing_columns = [col for col in columns_to_process_raw_for_predict if col not in test_data_raw_initial_load.columns]\n        if missing_columns:\n            raise KeyError(f\"The following required columns are missing from the test data: {missing_columns}. Please ensure your test.parquet file contains these columns.\")\n\n        print(f\"Selecting columns from initial raw test data (only {len(columns_to_process_raw_for_predict)} columns)...\")\n        test_df = test_data_raw_initial_load[columns_to_process_raw_for_predict].copy()\n\n        del test_data_raw_initial_load\n        gc.collect()\n\n        if 'ID' not in test_df.columns and test_df.index.name == 'ID':\n            test_df = test_df.reset_index()\n\n        if 'timestamp' in test_df.columns:\n            test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n\n        test_df = self.optimize_memory(test_df)\n        test_df = self.create_time_features(test_df)\n\n        print(f\"Test data shape after feature engineering: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n\n        if self.selected_features is None:\n            raise ValueError(\"Model must be fitted before making predictions (selected_features is None)\")\n\n        X_test_df_final = test_df[self.selected_features]\n\n        X_test_df_final = X_test_df_final.replace([np.inf, -np.inf], np.nan)\n        X_test_df_final = X_test_df_final.ffill().bfill().fillna(0)\n\n        X_test_scaled = self.scaler.transform(X_test_df_final)\n\n        if np.any(np.isnan(X_test_scaled)) or np.any(np.isinf(X_test_scaled)):\n            print(\"WARNING: Invalid values in test data after scaling, applying emergency cleanup.\")\n            X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test_df_final.index, columns=self.selected_features)\n\n        del X_test_df_final, test_df\n        gc.collect()\n\n        predictions = np.zeros(len(original_test_ids), dtype=np.float32)\n        indexed_predictions_by_internal_idx = {}\n\n        if 'lightgbm' in self.models:\n            try:\n                lgb_pred_full = self.models['lightgbm'].predict(X_test_scaled_df)\n                for i, idx in enumerate(X_test_scaled_df.index):\n                    indexed_predictions_by_internal_idx[idx] = lgb_pred_full[i]\n                print(\"LightGBM predictions generated.\")\n            except Exception as e:\n                print(f\"LightGBM prediction failed: {e}\")\n\n        if ('convlstm' in self.models and self.models['convlstm'] is not None) or \\\n           ('lstm' in self.models and self.models['lstm'] is not None):\n            try:\n                # For prediction, if test data is too large, you might need a generator here too.\n                # For now, we'll assume X_test_scaled can fit in memory for sequence prep for prediction.\n                # If this is still an OOM, we'd create a TimeSeriesSequence for prediction as well.\n                # We need a temporary full data array for prepare_sequences here.\n                X_test_seq_for_predict = self._prepare_sequences_for_inference(X_test_scaled)\n\n                if X_test_seq_for_predict is not None and X_test_seq_for_predict.size > 0:\n                    dl_predictions_list_test = []\n                    dl_weights_test = []\n\n                    if 'convlstm' in self.models and self.models['convlstm'] is not None:\n                        try:\n                            convlstm_pred = self.models['convlstm'].predict(X_test_seq_for_predict).flatten()\n                            dl_predictions_list_test.append(convlstm_pred)\n                            dl_weights_test.append(0.6)\n                        except Exception as e:\n                            print(f\"ConvLSTM prediction during test failed: {e}\")\n\n                    if 'lstm' in self.models and self.models['lstm'] is not None:\n                        try:\n                            lstm_pred = self.models['lstm'].predict(X_test_seq_for_predict).flatten()\n                            dl_predictions_list_test.append(lstm_pred)\n                            dl_weights_test.append(0.4)\n                        except Exception as e:\n                            print(f\"LSTM prediction during test failed: {e}\")\n\n                    if dl_predictions_list_test:\n                        dl_ensemble_weighted = np.average(dl_predictions_list_test, axis=0, weights=dl_weights_test)\n\n                        # Indices for DL predictions in the test set\n                        # These are derived from the X_test_scaled_df's index, offset by sequence_length-1\n                        sequence_covered_indices_in_X_scaled_df = X_test_scaled_df.index[self.sequence_length - 1:].tolist()\n\n\n                        for i, internal_idx in enumerate(sequence_covered_indices_in_X_scaled_df):\n                            lgbm_part = indexed_predictions_by_internal_idx.get(internal_idx, 0.0)\n                            indexed_predictions_by_internal_idx[internal_idx] = (lgbm_part * 0.4) + (dl_ensemble_weighted[i] * 0.6)\n\n                        print(\"Deep learning models included in predictions.\")\n                    else:\n                        print(\"No successful deep learning models to include in test predictions.\")\n                else:\n                    print(\"Test data too short for deep learning sequences. Using LightGBM only.\")\n            except Exception as e:\n                print(f\"Deep learning prediction setup failed: {e}\")\n                print(\"Falling back to LightGBM predictions only for test set.\")\n\n        del X_test_scaled_df # Clear prediction-specific data\n        if 'X_test_seq_for_predict' in locals() and X_test_seq_for_predict is not None:\n            del X_test_seq_for_predict\n        gc.collect()\n\n        for i, current_id in enumerate(original_test_ids):\n            original_idx_in_raw_df = id_to_original_index_map.get(current_id)\n\n            if original_idx_in_raw_df is not None and original_idx_in_raw_df in indexed_predictions_by_internal_idx:\n                predictions[i] = indexed_predictions_by_internal_idx[original_idx_in_raw_df]\n            else:\n                predictions[i] = 0.0\n\n        if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n            print(\"WARNING: Invalid predictions detected (NaN/Inf), cleaning...\")\n            predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n\n        print(f\"Generated {len(predictions)} predictions\")\n        print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n\n        return predictions\n\n    def _prepare_sequences_for_inference(self, data):\n        \"\"\"Helper to prepare sequences for inference, similar to original prepare_sequences.\"\"\"\n        sequences = []\n        data_np = np.asarray(data)\n\n        if len(data_np) < self.sequence_length:\n            print(f\"Warning: Not enough data ({len(data_np)} rows) to create sequences of length {self.sequence_length} for inference.\")\n            if data_np.shape and len(data_np.shape) > 1:\n                return np.array([]).astype(np.float32).reshape(0, self.sequence_length, data_np.shape[-1])\n            else: # Handle cases where data_np might be 1D or empty, resulting in 0 features\n                return np.array([]).astype(np.float32).reshape(0, self.sequence_length, 0)\n\n\n        # The loop range for inference is different because there are no explicit 'targets'\n        # We want to create all possible sequences.\n        # A sequence for `data_np[i]` means data from `data_np[i - sequence_length + 1 : i + 1]`.\n        # So, the first sequence ends at `sequence_length - 1`.\n        for i in range(self.sequence_length - 1, len(data_np)):\n            sequences.append(data_np[i - (self.sequence_length - 1) : i + 1])\n\n        return np.array(sequences).astype(np.float32)\n\n\n# Main execution function\ndef run_competition_pipeline():\n    \"\"\"Run the complete competition pipeline\"\"\"\n\n    print(\"Loading data...\")\n    train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n\n    if 'timestamp' not in train_full_raw.columns:\n        if train_full_raw.index.name == 'timestamp':\n            train_full_raw = train_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n        else:\n            train_full_raw = train_full_raw.reset_index()\n            if 'index' in train_full_raw.columns and 'timestamp' not in train_full_raw.columns:\n                 train_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                 print(f\"DEBUG: Renamed 'index' to 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in train_full_raw.columns:\n        train_full_raw['timestamp'] = pd.to_datetime(train_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in train_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in train_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: train_full_raw columns are: {train_full_raw.columns.tolist()}\")\n\n    test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n\n    if 'ID' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'ID':\n            test_full_raw = test_full_raw.reset_index(names=['ID'])\n            print(f\"DEBUG: Resetting index 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'ID' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'ID'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'timestamp':\n            test_full_raw = test_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'timestamp' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in test_full_raw.columns:\n        test_full_raw['timestamp'] = pd.to_datetime(test_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in test_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in test_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: test_full_raw columns are: {test_full_raw.columns.tolist()}\")\n\n    print(f\"\\nTrain shape: {train_full_raw.shape}\")\n    print(f\"Test shape: {test_full_raw.shape}\")\n\n    # Initialize and train model\n    predictor = CryptoMarketPredictor(\n        sequence_length=30,\n        top_features=100,\n        top_X_features_to_preselect=30\n    )\n    predictor.fit(train_full_raw)\n\n    predictions = predictor.predict(test_full_raw)\n\n    # Create submission\n    submission = pd.DataFrame({\n        'ID': test_full_raw['ID'],\n        'Prediction': predictions\n    })\n\n    # Save submission\n    submission.to_csv('/kaggle/working/submission.csv', index=False)\n    print(f\"Submission saved with {len(submission)} predictions\")\n    print(f\"Prediction statistics - Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n\n    return submission\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    submission = run_competition_pipeline()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}