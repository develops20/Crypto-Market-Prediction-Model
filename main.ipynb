{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-nb153?scriptVersionId=248986457\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# DRW Crypto Market Prediction Competition Pipeline\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os # Import os for path checking\nimport gc # Import garbage collector\nimport math # For ceil in data generator\n\n# Memory optimization and data processing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\n# Deep learning and modeling (TensorFlow import remains for GPU memory setting)\nimport tensorflow as tf \n# Removed: from tensorflow.keras.models import Sequential, Model\n# Removed: from tensorflow.keras.layers import (BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Reshape, TimeDistributed)\n# Removed: from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n# Removed: from tensorflow.keras.optimizers import Adam\n# Removed: from tensorflow.keras.utils import Sequence\n\n# Tree-based models for ensemble\nimport lightgbm as lgb\nimport xgboost as xgb # Import XGBoost\nimport catboost as cb # Import CatBoost\nfrom scipy.stats import pearsonr\n\n# Set memory growth for GPU (if available) - still good practice\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n\n# Removed TimeSeriesSequence class as it's no longer needed for Keras DL models\n# class TimeSeriesSequence(Sequence):\n#     ...\n\n\nclass CryptoMarketPredictor:\n    def __init__(self, sequence_length=30, top_features=100, top_X_features_to_preselect=30):\n        self.sequence_length = sequence_length\n        self.top_features = top_features\n        self.top_X_features_to_preselect = top_X_features_to_preselect\n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None # Stores final selected feature names\n        self.models = {}\n        # Path for the initial processed data after feature engineering\n        self._engineered_data_checkpoint_path = './engineered_train_data_checkpoint.parquet'\n        # Paths for scaled and feature-selected data (now loaded fully into memory for LGBM/XGBoost)\n        # No longer using separate _scaled_X_path, _scaled_y_path for generator\n        # The data will be loaded into memory for LGBM and XGBoost directly.\n\n\n    def optimize_memory(self, df):\n        \"\"\"\n        Optimize Pandas DataFrame memory usage and clean data.\n        \"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n        # Clean data first\n        df = self.clean_data(df)\n\n        # Optimize numeric columns (Pandas directly)\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col == 'timestamp' or col == 'ID' or col == 'label':\n                continue # Don't downcast timestamp, ID, or label\n            df[col] = pd.to_numeric(df[col], downcast='float')\n\n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n\n    def clean_data(self, df):\n        \"\"\"\n        Clean Pandas DataFrame by handling inf, -inf, and extreme values.\n        \"\"\"\n        print(\"Cleaning data...\")\n\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        # Replace inf and -inf with NaN first\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n        # Fill NaN values with forward fill, then backward fill, then 0\n        for col in numeric_cols:\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        # Handle extreme outliers (values beyond 3*IQR)\n        for col in df.select_dtypes(include=[np.float32, np.float64]).columns:\n            if col in ['timestamp', 'ID', 'label']: continue\n            if df[col].nunique() > 1:\n                q25 = df[col].quantile(0.25)\n                q75 = df[col].quantile(0.75)\n                iqr = q75 - q25\n\n                if iqr != 0 and not pd.isna(iqr):\n                    lower_bound = q25 - 3 * iqr\n                    upper_bound = q75 + 3 * iqr\n                    df[col] = df[col].clip(lower_bound, upper_bound)\n        print(\"Data cleaning applied.\")\n        return df\n\n    def create_time_features(self, df):\n        \"\"\"\n        Create time-based features with robust calculations using Pandas.\n        Significantly reduced complexity for faster execution.\n        \"\"\"\n        print(\"Creating time-based features...\")\n\n        # Basic market features\n        df['mid_price'] = (df['bid_qty'] + df['ask_qty']) / 2\n        df['spread'] = df['ask_qty'] - df['bid_qty']\n\n        # Safe division for imbalance\n        denominator = df['bid_qty'] + df['ask_qty'] + 1e-10\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n\n        # Safe division for buy/sell ratio\n        df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-10)\n\n        # New: Price-Volume Interaction\n        df['price_volume_interaction'] = df['mid_price'] * df['volume']\n\n        # Rolling statistics - EXPANDED WINDOWS AND STATS\n        windows = [5, 10, 20, 30, 60, 120] # Expanded windows\n        base_cols_for_rolling = ['volume', 'mid_price', 'buy_qty', 'sell_qty', 'imbalance', 'spread'] # Added spread\n\n        for col in base_cols_for_rolling:\n            for window in windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)\n                df[f'{col}_min_{window}'] = df[col].rolling(window, min_periods=1).min()\n                df[f'{col}_max_{window}'] = df[col].rolling(window, min_periods=1).max()\n                df[f'{col}_median_{window}'] = df[col].rolling(window, min_periods=1).median()\n                df[f'{col}_skew_{window}'] = df[col].rolling(window, min_periods=1).skew().fillna(0)\n                df[f'{col}_kurt_{window}'] = df[col].rolling(window, min_periods=1).kurt().fillna(0)\n                # Volatility (std/mean) - handle division by zero\n                df[f'{col}_volatility_{window}'] = (df[col].rolling(window, min_periods=1).std() / (df[col].rolling(window, min_periods=1).mean() + 1e-10)).fillna(0)\n\n\n        # Lagged features - EXPANDED LAGS\n        lags = [1, 2, 3, 5, 10, 20] # Expanded lags\n        base_cols_for_lag = ['mid_price', 'imbalance', 'volume', 'spread'] # Added volume, spread\n\n        for col in base_cols_for_lag:\n            for lag in lags:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n                # New: Lagged differences\n                if lag > 1:\n                    df[f'{col}_lag_diff_{lag}'] = df[col].diff(lag)\n\n\n        # Technical indicators - existing\n        df['rsi_proxy'] = self.calculate_rsi_proxy(df['mid_price'], window=10)\n        df['momentum'] = df['mid_price'] - df['mid_price'].shift(5)\n\n        # Final check for any inf/nan values that might have been introduced\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            df[col] = df[col].ffill().bfill().fillna(0) # Ensure no NaNs before feature selection/scaling\n\n        print(f\"Time-based features created. Current shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n        return df\n\n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate RSI-like indicator with safe operations for Pandas Series.\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n\n        rs = gain / (loss + 1e-10)\n        rsi = 100 - (100 / (1 + rs))\n\n        rsi = rsi.replace([np.inf, -np.inf], np.nan).fillna(50)\n\n        return rsi\n\n    def select_features(self, X_df, y_df, method='mutual_info'):\n        \"\"\"\n        Feature selection to reduce dimensionality with robust handling.\n        Operates directly on Pandas DataFrames.\n        \"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n\n        print(\"Validating data before final feature selection...\")\n\n        # Check for any remaining inf/nan values\n        inf_mask = np.isinf(X_df).any(axis=1)\n        nan_mask = np.isnan(X_df).any(axis=1)\n        invalid_mask = inf_mask | nan_mask\n\n        if invalid_mask.sum() > 0:\n            print(f\"Removing {invalid_mask.sum()} rows with invalid values before final selection.\")\n            X_df = X_df[~invalid_mask]\n            y_df = y_df[~invalid_mask]\n\n        # Check for constant or near-constant features (can cause issues for some selectors)\n        feature_std = X_df.std()\n        constant_features_mask = feature_std < 1e-8\n\n        if constant_features_mask.all():\n            print(\"Warning: All features are constant. Cannot perform feature selection.\")\n            non_constant_features = X_df.columns.tolist()\n        else:\n            non_constant_features = X_df.columns[~constant_features_mask].tolist()\n            if constant_features_mask.sum() > 0:\n                print(f\"Removing {constant_features_mask.sum()} constant features.\")\n            X_df = X_df[non_constant_features]\n\n        print(f\"Final data shape for feature selection: {X_df.shape}\")\n\n        n_features_to_select = min(self.top_features, X_df.shape[1])\n\n        if method == 'mutual_info':\n            selector = SelectKBest(score_func=mutual_info_regression, k=n_features_to_select)\n        else:\n            selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n\n        X_selected = selector.fit_transform(X_df, y_df)\n        self.feature_selector = selector\n        self.selected_features = X_df.columns[selector.get_support()].tolist()\n\n        print(f\"\\n--- Selected Features ({len(self.selected_features)}) ---\")\n        for feature in self.selected_features:\n            print(f\"- {feature}\")\n        print(\"---------------------------------------\\n\")\n\n        # Return as NumPy array and Pandas Index to maintain alignment with y\n        return X_selected.astype(np.float32), X_df.index\n\n    # Removed build_convlstm_model, build_lstm_model, build_conv1d_model\n    # All deep learning models are replaced by XGBoost\n    # def build_convlstm_model(self, input_shape): ...\n    # def build_lstm_model(self, input_shape): ...\n    # def build_conv1d_model(self, input_shape): ...\n\n    def build_cnn_lstm_model(self, input_shape):\n        print(\"CNN-LSTM model building skipped for current speed optimization, but can be reinstated later.\")\n        return None\n\n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train LightGBM model\"\"\"\n        print(\"Training LightGBM model...\")\n\n        # Robust NaN/Inf handling just before LightGBM training\n        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n            print(\"WARNING: NaNs/Infs found in X_train for LightGBM. Applying emergency cleanup.\")\n            X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(X_val)) or np.any(np.isinf(X_val)):\n            print(\"WARNING: NaNs/Infs found in X_val for LightGBM. Applying emergency cleanup.\")\n            X_val = np.nan_to_num(X_val, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_train)) or np.any(np.isinf(y_train)):\n            print(\"WARNING: NaNs/Infs found in y_train for LightGBM. Applying emergency cleanup.\")\n            y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_val)) or np.any(np.isinf(y_val)):\n            print(\"WARNING: NaNs/Infs found in y_val for LightGBM. Applying emergency cleanup.\")\n            y_val = np.nan_to_num(y_val, nan=0.0, posinf=1e6, neginf=-1e6)\n\n\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 1000\n        }\n\n        model = lgb.train(\n            params,\n            train_data,\n            valid_sets=[val_data],\n            num_boost_round=params['n_estimators'],\n            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n        )\n\n        return model\n\n    def train_xgboost(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train XGBoost model\"\"\"\n        print(\"Training XGBoost model...\")\n\n        # Robust NaN/Inf handling just before XGBoost training\n        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n            print(\"WARNING: NaNs/Infs found in X_train for XGBoost. Applying emergency cleanup.\")\n            X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(X_val)) or np.any(np.isinf(X_val)):\n            print(\"WARNING: NaNs/Infs found in X_val for XGBoost. Applying emergency cleanup.\")\n            X_val = np.nan_to_num(X_val, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_train)) or np.any(np.isinf(y_train)):\n            print(\"WARNING: NaNs/Infs found in y_train for XGBoost. Applying emergency cleanup.\")\n            y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_val)) or np.any(np.isinf(y_val)):\n            print(\"WARNING: NaNs/Infs found in y_val for XGBoost. Applying emergency cleanup.\")\n            y_val = np.nan_to_num(y_val, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        model = xgb.XGBRegressor(\n            objective='reg:squarederror', # Regression objective\n            eval_metric='mae', # Mean Absolute Error\n            n_estimators=1000, # Number of boosting rounds\n            learning_rate=0.05,\n            max_depth=6, # Maximum depth of a tree\n            subsample=0.8, # Subsample ratio of the training instance\n            colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n            random_state=42,\n            n_jobs=-1 # Use all available CPU cores\n        )\n\n        model.fit(X_train, y_train,\n                  eval_set=[(X_val, y_val)],\n                  early_stopping_rounds=50, # Stop if no improvement for 50 rounds\n                  verbose=False) # Suppress verbose output during training\n\n        return model\n\n    def train_catboost(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train CatBoost model\"\"\"\n        print(\"Training CatBoost model...\")\n\n        # Robust NaN/Inf handling just before CatBoost training\n        # CatBoost handles NaNs internally, but explicit conversion can prevent issues from other libs\n        X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        X_val = np.nan_to_num(X_val, nan=0.0, posinf=1e6, neginf=-1e6)\n        y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        y_val = np.nan_to_num(y_val, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        model = cb.CatBoostRegressor(\n            iterations=1000, # Number of boosting rounds\n            learning_rate=0.05,\n            depth=6, # Depth of the tree\n            loss_function='MAE', # Mean Absolute Error\n            eval_metric='MAE',\n            random_seed=42,\n            verbose=0, # Suppress verbose output during training\n            early_stopping_rounds=50, # Stop if no improvement for 50 rounds\n            allow_writing_files=False # Prevents writing files to disk\n        )\n\n        model.fit(X_train, y_train,\n                  eval_set=(X_val, y_val),\n                  verbose=0, # Suppress verbose output during training\n                  early_stopping_rounds=50)\n\n        return model\n\n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance\"\"\"\n        # Ensure y_true and y_pred are clean before evaluation\n        y_true_clean = np.nan_to_num(y_true, nan=0.0, posinf=1e6, neginf=-1e6)\n        y_pred_clean = np.nan_to_num(y_pred, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        mae = mean_absolute_error(y_true_clean, y_pred_clean)\n\n        # Pearson correlation requires at least 2 non-constant values for both arrays\n        correlation = 0.0\n        if len(np.unique(y_true_clean)) > 1 and len(np.unique(y_pred_clean)) > 1:\n            try:\n                correlation, _ = pearsonr(y_true_clean, y_pred_clean)\n            except ValueError:\n                # Can happen if inputs have 0 variance after cleaning\n                correlation = 0.0\n\n        print(f\"{model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n\n    def fit(self, train_data_raw_initial_load):\n        \"\"\"Main training pipeline with robust error handling\"\"\"\n        print(\"Starting training pipeline...\")\n\n        # Determine raw X_n columns from the initial load structure\n        X_n_cols_raw = [col for col in train_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        basic_features_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n\n        preselected_X_n_features = X_n_cols_raw[:min(self.top_X_features_to_preselect, len(X_n_cols_raw))]\n        print(f\"Initially selected {len(preselected_X_n_features)} X_n features for direct Pandas load.\")\n\n        columns_to_process_raw_for_fit = basic_features_cols + preselected_X_n_features + ['label']\n\n        train_df = None\n        if os.path.exists(self._engineered_data_checkpoint_path):\n            print(f\"Checkpoint found. Loading processed data from {self._engineered_data_checkpoint_path}...\")\n            train_df = pd.read_parquet(self._engineered_data_checkpoint_path)\n            if 'timestamp' in train_df.columns:\n                train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n        else:\n            print(f\"Selecting columns from initial raw training data...\")\n            train_df = train_data_raw_initial_load[columns_to_process_raw_for_fit].copy()\n            if 'timestamp' not in train_df.columns and train_df.index.name == 'timestamp':\n                train_df = train_df.reset_index(names=['timestamp'])\n            train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n            gc.collect()\n            train_df = self.optimize_memory(train_df)\n            train_df = self.create_time_features(train_df)\n            print(f\"Saving engineered data to checkpoint: {self._engineered_data_checkpoint_path}...\")\n            train_df.to_parquet(self._engineered_data_checkpoint_path, index=False)\n            print(\"Engineered data checkpoint saved.\")\n\n        print(f\"Data shape after feature engineering: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n        feature_cols_final = [col for col in train_df.columns\n                              if col not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols_final]\n        y_df = train_df['label']\n        print(f\"Features shape before final selection: {X_df.shape[0]} rows, {X_df.shape[1]} columns\")\n        print(f\"Target shape: {y_df.shape[0]} rows\")\n        \n        X_selected, valid_idx = self.select_features(X_df, y_df)\n        y_for_training = y_df.loc[valid_idx].astype(np.float32)\n\n        del X_df, y_df\n        gc.collect()\n\n        X_scaled = self.scaler.fit_transform(X_selected)\n        print(\"Final data validation (after scaling)...\")\n        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n            print(\"ERROR: Still have invalid values after preprocessing! Applying emergency cleanup.\")\n            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        print(f\"Final training data shape: {X_scaled.shape}\")\n        \n        del X_selected\n        gc.collect()\n\n        # Split data into training and validation sets\n        temp_train_full_timestamps = train_data_raw_initial_load[['timestamp']].copy()\n        if 'timestamp' not in temp_train_full_timestamps.columns and temp_train_full_timestamps.index.name == 'timestamp':\n            temp_train_full_timestamps = temp_train_full_timestamps.reset_index(names=['timestamp'])\n        temp_train_full_timestamps['timestamp'] = pd.to_datetime(temp_train_full_timestamps['timestamp'])\n        \n        VALIDATION_SPLIT_DATE = '2024-01-01'\n        original_train_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] < VALIDATION_SPLIT_DATE].index\n        original_val_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] >= VALIDATION_SPLIT_DATE].index\n        \n        del temp_train_full_timestamps\n        gc.collect()\n\n        # Create a temporary DataFrame from X_scaled to map to original indices for splitting\n        X_scaled_temp_df = pd.DataFrame(X_scaled, index=valid_idx, columns=self.selected_features)\n        \n        del X_scaled, y_for_training\n        gc.collect()\n\n        actual_train_indices = original_train_indices.intersection(X_scaled_temp_df.index)\n        actual_val_indices = original_val_indices.intersection(X_scaled_temp_df.index)\n\n        # Extract X_train, y_train, X_val, y_val as NumPy arrays for direct use by LGBM/XGBoost/CatBoost\n        X_train_final = X_scaled_temp_df.loc[actual_train_indices].values.astype(np.float32)\n        y_train_final = pd.Series(train_df['label'].loc[actual_train_indices].values).astype(np.float32)\n        X_val_final = X_scaled_temp_df.loc[actual_val_indices].values.astype(np.float32)\n        y_val_final = pd.Series(train_df['label'].loc[actual_val_indices].values).astype(np.float32)\n        \n        # Free up train_df and X_scaled_temp_df\n        del train_df, X_scaled_temp_df, valid_idx\n        gc.collect()\n\n        print(f\"Training set shapes: X={X_train_final.shape}, Y={y_train_final.shape}\")\n        print(f\"Validation set shapes: X={X_val_final.shape}, Y={y_val_final.shape}\")\n\n        # Train LightGBM model\n        lgb_score = 0\n        lgb_pred = np.zeros_like(y_val_final) # Initialize\n        try:\n            lgb_model = self.train_lightgbm(X_train_final, y_train_final, X_val_final, y_val_final)\n            lgb_pred = lgb_model.predict(X_val_final)\n            lgb_score = self.evaluate_model(y_val_final, lgb_pred, \"LightGBM\")\n            self.models['lightgbm'] = lgb_model\n        except Exception as e:\n            print(f\"LightGBM training or prediction failed: {e}\")\n            \n        # Train XGBoost model\n        xgb_score = 0\n        xgb_pred = np.zeros_like(y_val_final) # Initialize\n        try:\n            xgb_model = self.train_xgboost(X_train_final, y_train_final, X_val_final, y_val_final)\n            xgb_pred = xgb_model.predict(X_val_final)\n            xgb_score = self.evaluate_model(y_val_final, xgb_pred, \"XGBoost\")\n            self.models['xgboost'] = xgb_model\n        except Exception as e:\n            print(f\"XGBoost training or prediction failed: {e}\")\n            \n        # Train CatBoost model\n        cb_score = 0\n        cb_pred = np.zeros_like(y_val_final) # Initialize\n        try:\n            cb_model = self.train_catboost(X_train_final, y_train_final, X_val_final, y_val_final)\n            cb_pred = cb_model.predict(X_val_final)\n            cb_score = self.evaluate_model(y_val_final, cb_pred, \"CatBoost\")\n            self.models['catboost'] = cb_model\n        except Exception as e:\n            print(f\"CatBoost training or prediction failed: {e}\")\n            \n        # Ensemble logic\n        ensemble_predictions_list = []\n        ensemble_weights = []\n\n        if 'lightgbm' in self.models and lgb_score > 0:\n            ensemble_predictions_list.append(lgb_pred)\n            ensemble_weights.append(1/3) # Equal weight for 3 models\n\n        if 'xgboost' in self.models and xgb_score > 0:\n            ensemble_predictions_list.append(xgb_pred)\n            ensemble_weights.append(1/3) # Equal weight for 3 models\n\n        if 'catboost' in self.models and cb_score > 0:\n            ensemble_predictions_list.append(cb_pred)\n            ensemble_weights.append(1/3) # Equal weight for 3 models\n\n\n        if ensemble_predictions_list:\n            # Ensure all predictions have the same length before ensembling\n            min_len = min(len(p) for p in ensemble_predictions_list)\n            ensemble_predictions_list = [p[:min_len] for p in ensemble_predictions_list]\n            y_val_ensemble_aligned = y_val_final[:min_len] # Align true labels for evaluation\n\n            ensemble_pred = np.average(ensemble_predictions_list, axis=0, weights=ensemble_weights)\n            ensemble_score = self.evaluate_model(y_val_ensemble_aligned, ensemble_pred, \"Ensemble (LGBM + XGBoost + CatBoost)\")\n            print(f\"\\nEnsemble score: {ensemble_score:.4f}\")\n        else:\n            print(\"No successful models to include in ensemble. Ensemble score defaulting to 0.\")\n            ensemble_score = 0\n\n        print(f\"\\nBest individual model score: {max(lgb_score, xgb_score, cb_score):.4f}\")\n        print(f\"Final overall ensemble score: {ensemble_score:.4f}\")\n        \n        # Clean up memory after training\n        del X_train_final, y_train_final, X_val_final, y_val_final\n        gc.collect()\n        return self\n\n    def predict(self, test_data_raw_initial_load):\n        \"\"\"Generate predictions for test data with robust error handling\"\"\"\n        print(\"Generating predictions...\")\n\n        temp_df_for_id_map = test_data_raw_initial_load.copy()\n        if 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index.name == 'ID':\n            temp_df_for_id_map = temp_df_for_id_map.reset_index(names=['ID'])\n        elif 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index is not None and temp_df_for_id_map.index.name is None and len(temp_df_for_id_map.index) == len(temp_df_for_id_map):\n            temp_df_for_id_map = temp_df_for_id_map.reset_index()\n            temp_df_for_id_map.rename(columns={'index': 'ID'}, inplace=True)\n\n        id_to_original_index_map = pd.Series(temp_df_for_id_map.index.values, index=temp_df_for_id_map['ID'])\n        original_test_ids = temp_df_for_id_map['ID'].copy()\n\n        del temp_df_for_id_map\n        gc.collect()\n\n        basic_features_cols_test = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'ID']\n        X_n_cols_raw_test = [col for col in test_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        preselected_X_n_features_test = X_n_cols_raw_test[:min(self.top_X_features_to_preselect, len(X_n_cols_raw_test))]\n\n        columns_to_process_raw_for_predict = basic_features_cols_test + preselected_X_n_features_test\n\n        missing_columns = [col for col in columns_to_process_raw_for_predict if col not in test_data_raw_initial_load.columns]\n        if missing_columns:\n            raise KeyError(f\"The following required columns are missing from the test data: {missing_columns}. Please ensure your test.parquet file contains these columns.\")\n\n        print(f\"Selecting columns from initial raw test data (only {len(columns_to_process_raw_for_predict)} columns)...\")\n        test_df = test_data_raw_initial_load[columns_to_process_raw_for_predict].copy()\n\n        del test_data_raw_initial_load\n        gc.collect()\n\n        if 'ID' not in test_df.columns and test_df.index.name == 'ID':\n            test_df = test_df.reset_index()\n\n        if 'timestamp' in test_df.columns:\n            test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n\n        test_df = self.optimize_memory(test_df)\n        test_df = self.create_time_features(test_df)\n\n        print(f\"Test data shape after feature engineering: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n\n        if self.selected_features is None:\n            raise ValueError(\"Model must be fitted before making predictions (selected_features is None)\")\n\n        X_test_df_final = test_df[self.selected_features]\n\n        X_test_df_final = X_test_df_final.replace([np.inf, -np.inf], np.nan)\n        X_test_df_final = X_test_df_final.ffill().bfill().fillna(0)\n\n        X_test_scaled = self.scaler.transform(X_test_df_final)\n\n        if np.any(np.isnan(X_test_scaled)) or np.any(np.isinf(X_test_scaled)):\n            print(\"WARNING: Invalid values in test data after scaling, applying emergency cleanup.\")\n            X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test_df_final.index, columns=self.selected_features)\n\n        del X_test_df_final, test_df\n        gc.collect()\n\n        predictions = np.zeros(len(original_test_ids), dtype=np.float32)\n        indexed_predictions_by_internal_idx = {}\n\n        # LightGBM prediction\n        lgb_test_pred = np.zeros(len(X_test_scaled_df), dtype=np.float32) # Initialize\n        if 'lightgbm' in self.models:\n            try:\n                X_test_scaled_for_lgbm = X_test_scaled_df.copy()\n                if np.any(np.isnan(X_test_scaled_for_lgbm.values)) or np.any(np.isinf(X_test_scaled_for_lgbm.values)):\n                    print(\"WARNING: NaNs/Infs found in X_test_scaled_for_lgbm. Applying emergency cleanup.\")\n                    X_test_scaled_for_lgbm = pd.DataFrame(\n                        np.nan_to_num(X_test_scaled_for_lgbm.values, nan=0.0, posinf=1e6, neginf=-1e6),\n                        columns=X_test_scaled_for_lgbm.columns, index=X_test_scaled_for_lgbm.index\n                    )\n                lgb_test_pred = self.models['lightgbm'].predict(X_test_scaled_for_lgbm)\n                print(\"LightGBM predictions generated.\")\n                del X_test_scaled_for_lgbm\n                gc.collect()\n            except Exception as e:\n                print(f\"LightGBM prediction failed: {e}\")\n\n        # XGBoost prediction\n        xgb_test_pred = np.zeros(len(X_test_scaled_df), dtype=np.float32) # Initialize\n        if 'xgboost' in self.models:\n            try:\n                X_test_scaled_for_xgb = X_test_scaled_df.copy()\n                if np.any(np.isnan(X_test_scaled_for_xgb.values)) or np.any(np.isinf(X_test_scaled_for_xgb.values)):\n                    print(\"WARNING: NaNs/Infs found in X_test_scaled_for_xgb. Applying emergency cleanup.\")\n                    X_test_scaled_for_xgb = pd.DataFrame(\n                        np.nan_to_num(X_test_scaled_for_xgb.values, nan=0.0, posinf=1e6, neginf=-1e6),\n                        columns=X_test_scaled_for_xgb.columns, index=X_test_scaled_for_xgb.index\n                    )\n                xgb_test_pred = self.models['xgboost'].predict(X_test_scaled_for_xgb)\n                print(\"XGBoost predictions generated.\")\n                del X_test_scaled_for_xgb\n                gc.collect()\n            except Exception as e:\n                print(f\"XGBoost prediction failed: {e}\")\n\n        # CatBoost prediction\n        cb_test_pred = np.zeros(len(X_test_scaled_df), dtype=np.float32) # Initialize\n        if 'catboost' in self.models:\n            try:\n                X_test_scaled_for_cb = X_test_scaled_df.copy()\n                if np.any(np.isnan(X_test_scaled_for_cb.values)) or np.any(np.isinf(X_test_scaled_for_cb.values)):\n                    print(\"WARNING: NaNs/Infs found in X_test_scaled_for_cb. Applying emergency cleanup.\")\n                    X_test_scaled_for_cb = pd.DataFrame(\n                        np.nan_to_num(X_test_scaled_for_cb.values, nan=0.0, posinf=1e6, neginf=-1e6),\n                        columns=X_test_scaled_for_cb.columns, index=X_test_scaled_for_cb.index\n                    )\n                cb_test_pred = self.models['catboost'].predict(X_test_scaled_for_cb)\n                print(\"CatBoost predictions generated.\")\n                del X_test_scaled_for_cb\n                gc.collect()\n            except Exception as e:\n                print(f\"CatBoost prediction failed: {e}\")\n\n        # Ensemble predictions for final output\n        final_ensemble_predictions = []\n        final_ensemble_weights = []\n\n        if 'lightgbm' in self.models:\n            final_ensemble_predictions.append(lgb_test_pred)\n            final_ensemble_weights.append(1/3)\n        if 'xgboost' in self.models:\n            final_ensemble_predictions.append(xgb_test_pred)\n            final_ensemble_weights.append(1/3)\n        if 'catboost' in self.models:\n            final_ensemble_predictions.append(cb_test_pred)\n            final_ensemble_weights.append(1/3)\n\n        if final_ensemble_predictions:\n            # Ensure all prediction arrays have the same length for ensembling\n            min_len = min(len(p) for p in final_ensemble_predictions)\n            final_ensemble_predictions = [p[:min_len] for p in final_ensemble_predictions]\n            \n            # Use X_test_scaled_df's index to map predictions back to original IDs\n            # The indices of X_test_scaled_df correspond to the order of predictions.\n            # We need to map these to the original_test_ids.\n            \n            # The ensemble prediction is the average of the individual predictions\n            combined_predictions = np.average(final_ensemble_predictions, axis=0, weights=final_ensemble_weights)\n\n            # Map the combined predictions back to the original test IDs\n            # Iterate through the original_test_ids to place predictions in the correct overall submission order\n            # This requires aligning combined_predictions with X_test_scaled_df's index.\n            for i, current_id in enumerate(original_test_ids):\n                # Find the internal DataFrame index that corresponds to this 'ID'\n                original_idx_in_raw_df = id_to_original_index_map.get(current_id)\n\n                # Find the position of original_idx_in_raw_df within X_test_scaled_df's index\n                # This is a potentially slow operation if X_test_scaled_df.index is large and not a simple range.\n                # A direct lookup by index value is better.\n                if original_idx_in_raw_df is not None and original_idx_in_raw_df in X_test_scaled_df.index:\n                    # Get the positional index in X_test_scaled_df for this original_idx_in_raw_df\n                    pos_in_scaled_df = X_test_scaled_df.index.get_loc(original_idx_in_raw_df)\n                    if pos_in_scaled_df < len(combined_predictions): # Ensure we don't go out of bounds\n                        predictions[i] = combined_predictions[pos_in_scaled_df]\n                    else:\n                        predictions[i] = 0.0 # Default if out of bounds\n                else:\n                    predictions[i] = 0.0 # Default if original_idx not found\n\n            print(\"Ensemble predictions generated.\")\n        else:\n            print(\"No successful models to include in ensemble for prediction. Predictions defaulting to 0.\")\n            # If no models trained, predictions array remains all zeros.\n\n        del X_test_scaled_df # Clear prediction-specific data\n        gc.collect()\n\n        for i, current_id in enumerate(original_test_ids):\n            original_idx_in_raw_df = id_to_original_index_map.get(current_id)\n\n            if original_idx_in_raw_df is not None and original_idx_in_raw_df < len(predictions): # Check bounds\n                # If we used the indexed_predictions_by_internal_idx map, this is simpler:\n                predictions[i] = indexed_predictions_by_internal_idx.get(original_idx_in_raw_df, 0.0)\n            else:\n                predictions[i] = 0.0 # Default if original_idx not found or out of bounds\n\n        if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n            print(\"WARNING: Invalid predictions detected (NaN/Inf), cleaning...\")\n            predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n\n        print(f\"Generated {len(predictions)} predictions\")\n        print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n\n        return predictions\n    \n\n# Main execution function\ndef run_competition_pipeline():\n    \"\"\"Run the complete competition pipeline\"\"\"\n\n    print(\"Loading data...\")\n    # Using /kaggle/input/drw-crypto-market-prediction/train.parquet as per your last provided code\n    train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n\n    if 'timestamp' not in train_full_raw.columns:\n        if train_full_raw.index.name == 'timestamp':\n            train_full_raw = train_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n        else:\n            train_full_raw = train_full_raw.reset_index()\n            if 'index' in train_full_raw.columns and 'timestamp' not in train_full_raw.columns:\n                 train_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                 print(f\"DEBUG: Renamed 'index' to 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in train_full_raw.columns:\n        train_full_raw['timestamp'] = pd.to_datetime(train_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in train_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in train_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: train_full_raw columns are: {train_full_raw.columns.tolist()}\")\n\n    # Using /kaggle/input/drw-crypto-market-prediction/test.parquet as per your last provided code\n    test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n\n    if 'ID' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'ID':\n            test_full_raw = test_full_raw.reset_index(names=['ID'])\n            print(f\"DEBUG: Resetting index 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'ID' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'ID'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'timestamp':\n            test_full_raw = test_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'timestamp' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in test_full_raw.columns:\n        test_full_raw['timestamp'] = pd.to_datetime(test_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in test_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in test_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: test_full_raw columns are: {test_full_raw.columns.tolist()}\")\n\n    print(f\"\\nTrain shape: {train_full_raw.shape}\")\n    print(f\"Test shape: {test_full_raw.shape}\")\n\n    # Initialize and train model\n    predictor = CryptoMarketPredictor(\n        sequence_length=30, # Sequence length is still used for feature engineering, but not directly by XGBoost\n        top_features=100,\n        top_X_features_to_preselect=30\n    )\n    predictor.fit(train_full_raw)\n\n    predictions = predictor.predict(test_full_raw)\n\n    # Create submission\n    submission = pd.DataFrame({\n        'ID': test_full_raw['ID'],\n        'Prediction': predictions\n    })\n\n    # Save submission\n    submission.to_csv('/kaggle/working/submission.csv', index=False)\n    print(f\"Submission saved with {len(submission)} predictions\")\n    print(f\"Prediction statistics - Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n\n    return submission\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    submission = run_competition_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T07:29:24.305078Z","iopub.execute_input":"2025-07-05T07:29:24.305634Z","iopub.status.idle":"2025-07-05T07:36:04.938571Z","shell.execute_reply.started":"2025-07-05T07:29:24.30561Z","shell.execute_reply":"2025-07-05T07:36:04.937798Z"}},"outputs":[],"execution_count":null}]}