{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-nb153?scriptVersionId=248751774\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# DRW Crypto Market Prediction Competition Pipeline\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os # Import os for path checking\nimport gc # Import garbage collector\nimport math # For ceil in data generator\n\n# Memory optimization and data processing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\n# Deep learning and modeling\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (LSTM, ConvLSTM2D, Dense, Dropout,\n                                     BatchNormalization, Input, Conv1D, MaxPooling1D,\n                                     Flatten, Reshape, TimeDistributed)\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence # Import Keras Sequence for data generation\n\n# Tree-based models for ensemble\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\n# Set memory growth for GPU (if available) - still good practice\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n\nclass TimeSeriesSequence(Sequence):\n    \"\"\"\n    Keras Sequence for loading time-series data in batches from Parquet files,\n    preparing sequences for deep learning models.\n    \"\"\"\n    def __init__(self, X_filepath, y_filepath, sequence_length, batch_size, feature_cols, total_samples, original_indices_min, original_indices_max):\n        self.X_filepath = X_filepath\n        self.y_filepath = y_filepath\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.feature_cols = feature_cols # List of feature column names\n        self.original_indices_min = original_indices_min # Min value of 'original_row_index' for this split\n        self.original_indices_max = original_indices_max # Max value of 'original_row_index' for this split\n\n        # total_samples is the count of rows in the underlying file for this split\n        self.total_samples = total_samples\n\n        # Calculate the effective length of the dataset for this generator.\n        # This is the number of labels we can predict given the sequence length.\n        self.num_predictable_samples = max(0, self.total_samples - self.sequence_length + 1)\n        \n        print(f\"TimeSeriesSequence initialized for {X_filepath}: \"\n              f\"Total rows in file: {total_samples}, Predictable samples: {self.num_predictable_samples}, \"\n              f\"Batch size: {self.batch_size}, Sequence length: {self.sequence_length}, \"\n              f\"Original index range: [{self.original_indices_min}, {self.original_indices_max}]\")\n\n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch.\"\"\"\n        if self.num_predictable_samples == 0:\n            return 0\n        return math.ceil(self.num_predictable_samples / self.batch_size)\n\n    def __getitem__(self, idx):\n        \"\"\"Generates one batch of data.\"\"\"\n        # Calculate the range of original_row_index values for the labels in this batch\n        # The labels for this batch correspond to original_row_index values from:\n        # self.original_indices_min + (idx * self.batch_size)\n        # to self.original_indices_min + (idx * self.batch_size) + self.batch_size - 1\n        \n        # Determine the start and end of the *labels* for this batch based on original row indices\n        global_label_start_idx_value = self.original_indices_min + (idx * self.batch_size)\n        # The last label index value for this batch\n        global_label_end_idx_value = min(self.original_indices_max, global_label_start_idx_value + self.batch_size - 1)\n\n        # Determine the start and end of the *features* needed for these labels\n        # The first feature needed for the first label in this batch\n        global_feature_start_idx_value = global_label_start_idx_value - (self.sequence_length - 1)\n        # The last feature needed for the last sequence in this batch (which is the same as the last label)\n        global_feature_end_idx_value = global_label_end_idx_value\n\n        # Ensure we don't try to read before the actual min original index or beyond the max\n        read_start_original_idx = max(self.original_indices_min, global_feature_start_idx_value)\n        read_end_original_idx = min(self.original_indices_max, global_feature_end_idx_value)\n\n        # Handle cases where the calculated read range is invalid (e.g., at the very beginning of the data)\n        if read_start_original_idx > read_end_original_idx:\n            print(f\"Warning: Calculated read range [{read_start_original_idx}, {read_end_original_idx}] is invalid for batch {idx}. Returning empty.\")\n            return np.array([]).astype(np.float32).reshape(0, self.sequence_length, len(self.feature_cols)), \\\n                   np.array([]).astype(np.float32)\n\n        # Read the slice of X data using the 'original_row_index' column\n        # We need to read the 'original_row_index' column itself to re-index the DataFrame\n        X_batch_df = pd.read_parquet(self.X_filepath, columns=self.feature_cols + ['original_row_index'],\n                                     engine='pyarrow', \n                                     filters=[('original_row_index', '>=', read_start_original_idx), \n                                              ('original_row_index', '<=', read_end_original_idx)])\n        \n        # Read the slice of Y data using the 'original_row_index' column\n        y_batch_df = pd.read_parquet(self.y_filepath, columns=['label', 'original_row_index'],\n                                     engine='pyarrow', \n                                     filters=[('original_row_index', '>=', global_label_start_idx_value), \n                                              ('original_row_index', '<=', global_label_end_idx_value)])\n\n        # Set 'original_row_index' as the DataFrame index for proper alignment and slicing\n        X_batch_df = X_batch_df.set_index('original_row_index').sort_index()\n        y_batch_df = y_batch_df.set_index('original_row_index').sort_index()\n\n        # Convert to numpy arrays\n        X_batch_np = X_batch_df.values.astype(np.float32)\n        y_batch_np = y_batch_df['label'].values.flatten().astype(np.float32) # Ensure y is 1D\n\n        del X_batch_df, y_batch_df # Free memory\n        gc.collect()\n\n        # Prepare sequences from the loaded chunk (this logic remains similar to original prepare_sequences)\n        sequences = []\n        targets = []\n\n        # The indices within X_batch_np that correspond to the start of sequences for `y_batch_np` labels\n        # The first sequence in the loaded chunk starts at X_batch_np[0].\n        # Its corresponding label's original_row_index would be read_start_original_idx + sequence_length - 1.\n        # We need to align the sequences with the labels in y_batch_np.\n        \n        # The number of rows in X_batch_np that can form a full sequence\n        num_sequences_in_chunk = len(X_batch_np) - self.sequence_length + 1\n        \n        if num_sequences_in_chunk <= 0:\n            # Not enough data in this chunk to form any sequences\n            return np.array([]).astype(np.float32).reshape(0, self.sequence_length, len(self.feature_cols)), \\\n                   np.array([]).astype(np.float32)\n\n        # Create sequences from the loaded X_batch_np\n        X_sequences = np.array([X_batch_np[i : i + self.sequence_length] for i in range(num_sequences_in_chunk)])\n        \n        # The labels (y_sequences) correspond to the last element of each X_sequence.\n        # So, if X_sequences[0] is X_batch_np[0:sequence_length], its label is y_batch_np corresponding to X_batch_np[sequence_length-1].\n        # The y_batch_np contains labels for original_row_indices from `global_label_start_idx_value` to `global_label_end_idx_value`.\n        # We need to slice y_batch_np to match the generated X_sequences.\n        # The first label for X_sequences[0] is at global_label_start_idx_value.\n        # The y_batch_np loaded corresponds to original_row_indices from global_label_start_idx_value to global_label_end_idx_value.\n        y_sequences = y_batch_np[:num_sequences_in_chunk] # This assumes y_batch_np starts at the first label corresponding to the first sequence.\n\n        del X_batch_np, y_batch_np # Clear memory after forming sequences\n        gc.collect()\n\n        return X_sequences, y_sequences\n\n\nclass CryptoMarketPredictor:\n    def __init__(self, sequence_length=30, top_features=100, top_X_features_to_preselect=30):\n        self.sequence_length = sequence_length\n        self.top_features = top_features\n        self.top_X_features_to_preselect = top_X_features_to_preselect\n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None # Stores final selected feature names\n        self.models = {}\n        # Path for the initial processed data after feature engineering\n        self._engineered_data_checkpoint_path = './engineered_train_data_checkpoint.parquet'\n        # Paths for scaled and feature-selected data, to be read by the generator\n        self._scaled_X_path = './scaled_train_X.parquet'\n        self._scaled_y_path = './scaled_train_y.parquet'\n        self._scaled_val_X_path = './scaled_val_X.parquet'\n        self._scaled_val_y_path = './scaled_val_y.parquet'\n\n\n    def optimize_memory(self, df):\n        \"\"\"\n        Optimize Pandas DataFrame memory usage and clean data.\n        \"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n        # Clean data first\n        df = self.clean_data(df)\n\n        # Optimize numeric columns (Pandas directly)\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col == 'timestamp' or col == 'ID' or col == 'label':\n                continue # Don't downcast timestamp, ID, or label\n            df[col] = pd.to_numeric(df[col], downcast='float')\n\n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n\n    def clean_data(self, df):\n        \"\"\"\n        Clean Pandas DataFrame by handling inf, -inf, and extreme values.\n        \"\"\"\n        print(\"Cleaning data...\")\n\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        # Replace inf and -inf with NaN first\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n        # Fill NaN values with forward fill, then backward fill, then 0\n        for col in numeric_cols:\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        # Handle extreme outliers (values beyond 3*IQR)\n        for col in df.select_dtypes(include=[np.float32, np.float64]).columns:\n            if col in ['timestamp', 'ID', 'label']: continue\n            if df[col].nunique() > 1:\n                q25 = df[col].quantile(0.25)\n                q75 = df[col].quantile(0.75)\n                iqr = q75 - q25\n\n                if iqr != 0 and not pd.isna(iqr):\n                    lower_bound = q25 - 3 * iqr\n                    upper_bound = q75 + 3 * iqr\n                    df[col] = df[col].clip(lower_bound, upper_bound)\n        print(\"Data cleaning applied.\")\n        return df\n\n    def create_time_features(self, df):\n        \"\"\"\n        Create time-based features with robust calculations using Pandas.\n        Significantly reduced complexity for faster execution.\n        \"\"\"\n        print(\"Creating time-based features...\")\n\n        # Basic market features\n        df['mid_price'] = (df['bid_qty'] + df['ask_qty']) / 2\n        df['spread'] = df['ask_qty'] - df['bid_qty']\n\n        # Safe division for imbalance\n        denominator = df['bid_qty'] + df['ask_qty'] + 1e-10\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n\n        # Safe division for buy/sell ratio\n        df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-10)\n\n        # Rolling statistics - significantly reduced windows for speed\n        windows = [10, 30]\n        base_cols_for_rolling = ['volume', 'mid_price', 'buy_qty', 'sell_qty', 'imbalance']\n\n        for col in base_cols_for_rolling:\n            for window in windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)\n\n        # Lagged features - significantly reduced lags for speed\n        lags = [1, 5]\n        base_cols_for_lag = ['mid_price', 'imbalance']\n\n        for col in base_cols_for_lag:\n            for lag in lags:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n\n        # Technical indicators - reduced\n        df['rsi_proxy'] = self.calculate_rsi_proxy(df['mid_price'], window=10)\n        df['momentum'] = df['mid_price'] - df['mid_price'].shift(5)\n\n        # Final check for any inf/nan values that might have been introduced\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        print(f\"Time-based features created. Current shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n        return df\n\n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate RSI-like indicator with safe operations for Pandas Series.\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n\n        rs = gain / (loss + 1e-10)\n        rsi = 100 - (100 / (1 + rs))\n\n        rsi = rsi.replace([np.inf, -np.inf], np.nan).fillna(50)\n\n        return rsi\n\n    def select_features(self, X_df, y_df, method='mutual_info'):\n        \"\"\"\n        Feature selection to reduce dimensionality with robust handling.\n        Operates directly on Pandas DataFrames.\n        \"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n\n        print(\"Validating data before final feature selection...\")\n\n        # Check for any remaining inf/nan values\n        inf_mask = np.isinf(X_df).any(axis=1)\n        nan_mask = np.isnan(X_df).any(axis=1)\n        invalid_mask = inf_mask | nan_mask\n\n        if invalid_mask.sum() > 0:\n            print(f\"Removing {invalid_mask.sum()} rows with invalid values before final selection.\")\n            X_df = X_df[~invalid_mask]\n            y_df = y_df[~invalid_mask]\n\n        # Check for constant or near-constant features (can cause issues for some selectors)\n        feature_std = X_df.std()\n        constant_features_mask = feature_std < 1e-8\n\n        if constant_features_mask.all():\n            print(\"Warning: All features are constant. Cannot perform feature selection.\")\n            non_constant_features = X_df.columns.tolist()\n        else:\n            non_constant_features = X_df.columns[~constant_features_mask].tolist()\n            if constant_features_mask.sum() > 0:\n                print(f\"Removing {constant_features_mask.sum()} constant features.\")\n            X_df = X_df[non_constant_features]\n\n        print(f\"Final data shape for feature selection: {X_df.shape}\")\n\n        n_features_to_select = min(self.top_features, X_df.shape[1])\n\n        if method == 'mutual_info':\n            selector = SelectKBest(score_func=mutual_info_regression, k=n_features_to_select)\n        else:\n            selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n\n        X_selected = selector.fit_transform(X_df, y_df)\n        self.feature_selector = selector\n        self.selected_features = X_df.columns[selector.get_support()].tolist()\n\n        print(f\"\\n--- Selected Features ({len(self.selected_features)}) ---\")\n        for feature in self.selected_features:\n            print(f\"- {feature}\")\n        print(\"---------------------------------------\\n\")\n\n        # Return as NumPy array and Pandas Index to maintain alignment with y\n        return X_selected.astype(np.float32), X_df.index\n\n    # Removed original prepare_sequences, as it's now embedded in TimeSeriesSequence and won't be called directly by fit\n    # def prepare_sequences(self, data, target=None):\n    #     \"\"\"Prepare sequences for time series models\"\"\"\n    #     ...\n\n    # Removed build_convlstm_model\n    # def build_convlstm_model(self, input_shape):\n    #     \"\"\"Build ConvLSTM model for spatial-temporal patterns\"\"\"\n    #     ...\n\n    # Removed build_lstm_model\n    # def build_lstm_model(self, input_shape):\n    #     \"\"\"Build standard LSTM model\"\"\"\n    #     ...\n\n    def build_conv1d_model(self, input_shape):\n        \"\"\"Build a simplified Conv1D model for time series patterns.\"\"\"\n        model = Sequential([\n            Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape), # Reverted to 32 filters\n            MaxPooling1D(pool_size=2),\n            Flatten(),\n            Dense(32, activation='relu'), # Reverted to 32 units\n            Dropout(0.3),\n            Dense(1, activation='linear')\n        ])\n        model.compile(optimizer=Adam(learning_rate=0.001),\n                      loss='mae', metrics=['mae'])\n        return model\n\n    def build_cnn_lstm_model(self, input_shape):\n        print(\"CNN-LSTM model building skipped for current speed optimization, but can be reinstated later.\")\n        return None\n\n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train LightGBM model\"\"\"\n        print(\"Training LightGBM model...\")\n\n        # Robust NaN/Inf handling just before LightGBM training\n        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n            print(\"WARNING: NaNs/Infs found in X_train for LightGBM. Applying emergency cleanup.\")\n            X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(X_val)) or np.any(np.isinf(X_val)):\n            print(\"WARNING: NaNs/Infs found in X_val for LightGBM. Applying emergency cleanup.\")\n            X_val = np.nan_to_num(X_val, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_train)) or np.any(np.isinf(y_train)):\n            print(\"WARNING: NaNs/Infs found in y_train for LightGBM. Applying emergency cleanup.\")\n            y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n        if np.any(np.isnan(y_val)) or np.any(np.isinf(y_val)):\n            print(\"WARNING: NaNs/Infs found in y_val for LightGBM. Applying emergency cleanup.\")\n            y_val = np.nan_to_num(y_val, nan=0.0, posinf=1e6, neginf=-1e6)\n\n\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 1000\n        }\n\n        model = lgb.train(\n            params,\n            train_data,\n            valid_sets=[val_data],\n            num_boost_round=params['n_estimators'],\n            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n        )\n\n        return model\n\n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance\"\"\"\n        mae = mean_absolute_error(y_true, y_pred)\n        correlation, _ = pearsonr(y_true, y_pred)\n\n        print(f\"{model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n\n    def fit(self, train_data_raw_initial_load):\n        \"\"\"Main training pipeline with robust error handling\"\"\"\n        print(\"Starting training pipeline...\")\n\n        # Determine raw X_n columns from the initial load structure\n        X_n_cols_raw = [col for col in train_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        basic_features_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n\n        preselected_X_n_features = X_n_cols_raw[:min(self.top_X_features_to_preselect, len(X_n_cols_raw))]\n        print(f\"Initially selected {len(preselected_X_n_features)} X_n features for direct Pandas load.\")\n\n        columns_to_process_raw_for_fit = basic_features_cols + preselected_X_n_features + ['label']\n\n        train_df = None\n        if os.path.exists(self._engineered_data_checkpoint_path):\n            print(f\"Checkpoint found. Loading processed data from {self._engineered_data_checkpoint_path}...\")\n            train_df = pd.read_parquet(self._engineered_data_checkpoint_path)\n            if 'timestamp' in train_df.columns:\n                train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n        else:\n            print(f\"Selecting columns from initial raw training data...\")\n            train_df = train_data_raw_initial_load[columns_to_process_raw_for_fit].copy()\n            if 'timestamp' not in train_df.columns and train_df.index.name == 'timestamp':\n                train_df = train_df.reset_index(names=['timestamp'])\n            train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n            gc.collect()\n            train_df = self.optimize_memory(train_df)\n            train_df = self.create_time_features(train_df)\n            print(f\"Saving engineered data to checkpoint: {self._engineered_data_checkpoint_path}...\")\n            train_df.to_parquet(self._engineered_data_checkpoint_path, index=False)\n            print(\"Engineered data checkpoint saved.\")\n\n        print(f\"Data shape after feature engineering: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n        feature_cols_final = [col for col in train_df.columns\n                              if col not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols_final]\n        y_df = train_df['label']\n        print(f\"Features shape before final selection: {X_df.shape[0]} rows, {X_df.shape[1]} columns\")\n        print(f\"Target shape: {y_df.shape[0]} rows\")\n        \n        X_selected, valid_idx = self.select_features(X_df, y_df)\n        y_for_training = y_df.loc[valid_idx].astype(np.float32)\n\n        del X_df, y_df\n        gc.collect()\n\n        X_scaled = self.scaler.fit_transform(X_selected)\n        print(\"Final data validation (after scaling)...\")\n        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n            print(\"ERROR: Still have invalid values after preprocessing! Applying emergency cleanup.\")\n            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        print(f\"Final training data shape: {X_scaled.shape}\")\n        \n        del X_selected\n        gc.collect()\n\n        # Split data into training and validation sets\n        temp_train_full_timestamps = train_data_raw_initial_load[['timestamp']].copy()\n        if 'timestamp' not in temp_train_full_timestamps.columns and temp_train_full_timestamps.index.name == 'timestamp':\n            temp_train_full_timestamps = temp_train_full_timestamps.reset_index(names=['timestamp'])\n        temp_train_full_timestamps['timestamp'] = pd.to_datetime(temp_train_full_timestamps['timestamp'])\n        \n        VALIDATION_SPLIT_DATE = '2024-01-01'\n        original_train_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] < VALIDATION_SPLIT_DATE].index\n        original_val_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] >= VALIDATION_SPLIT_DATE].index\n        \n        del temp_train_full_timestamps\n        gc.collect()\n\n        X_scaled_temp_df = pd.DataFrame(X_scaled, index=valid_idx, columns=self.selected_features)\n        \n        del X_scaled, y_for_training\n        gc.collect()\n\n        actual_train_indices = original_train_indices.intersection(X_scaled_temp_df.index)\n        actual_val_indices = original_val_indices.intersection(X_scaled_temp_df.index)\n\n        # --- IMPORTANT CHANGE: Save index as a named column for pyarrow filtering ---\n        # Create DataFrames with the original_row_index as a named column\n        X_train_df_to_save = X_scaled_temp_df.loc[actual_train_indices].reset_index().rename(columns={'index': 'original_row_index'})\n        y_train_df_to_save = pd.DataFrame({'label': train_df['label'].loc[actual_train_indices].values}, index=actual_train_indices).reset_index().rename(columns={'index': 'original_row_index'})\n        X_val_df_to_save = X_scaled_temp_df.loc[actual_val_indices].reset_index().rename(columns={'index': 'original_row_index'})\n        y_val_df_to_save = pd.DataFrame({'label': train_df['label'].loc[actual_val_indices].values}, index=actual_val_indices).reset_index().rename(columns={'index': 'original_row_index'})\n        \n        # Save these DataFrames to Parquet files without the Pandas DataFrame index (it's now a column)\n        X_train_df_to_save.to_parquet(self._scaled_X_path, index=False)\n        y_train_df_to_save.to_parquet(self._scaled_y_path, index=False)\n        X_val_df_to_save.to_parquet(self._scaled_val_X_path, index=False)\n        y_val_df_to_save.to_parquet(self._scaled_val_y_path, index=False)\n        \n        print(\"Scaled training and validation data saved to separate Parquet files with 'original_row_index'.\")\n\n        # Delete the large DataFrames from memory right after saving\n        del X_train_df_to_save, y_train_df_to_save, X_val_df_to_save, y_val_df_to_save\n        del train_df, X_scaled_temp_df, valid_idx # Also clear original train_df\n        gc.collect()\n\n        # Create Data Generators - pass min/max of original indices for filtering\n        train_generator = TimeSeriesSequence(\n            X_filepath=self._scaled_X_path,\n            y_filepath=self._scaled_y_path,\n            sequence_length=self.sequence_length,\n            batch_size=16, # Current batch_size\n            feature_cols=self.selected_features,\n            total_samples=len(actual_train_indices),\n            original_indices_min=actual_train_indices.min(),\n            original_indices_max=actual_train_indices.max()\n        )\n        val_generator = TimeSeriesSequence(\n            X_filepath=self._scaled_val_X_path,\n            y_filepath=self._scaled_y_path,\n            sequence_length=self.sequence_length,\n            batch_size=16, # Current batch_size\n            feature_cols=self.selected_features,\n            total_samples=len(actual_val_indices),\n            original_indices_min=actual_val_indices.min(),\n            original_indices_max=actual_val_indices.max()\n        )\n        print(\"Keras Data Generators created.\")\n\n        try:\n            # LightGBM still needs in-memory data, so load from the new Parquet files\n            lgb_X_train = pd.read_parquet(self._scaled_X_path)\n            lgb_y_train = pd.read_parquet(self._scaled_y_path)['label'].values\n            lgb_X_val = pd.read_parquet(self._scaled_val_X_path)\n            lgb_y_val = pd.read_parquet(self._scaled_val_y_path)['label'].values\n\n            # Drop the 'original_row_index' column before passing to LightGBM\n            if 'original_row_index' in lgb_X_train.columns:\n                lgb_X_train = lgb_X_train.drop(columns=['original_row_index'])\n            if 'original_row_index' in lgb_X_val.columns:\n                lgb_X_val = lgb_X_val.drop(columns=['original_row_index'])\n\n            lgb_model = self.train_lightgbm(\n                lgb_X_train.values, # Pass values directly\n                lgb_y_train,\n                lgb_X_val.values, # Pass values directly\n                lgb_y_val\n            )\n            \n            # For prediction, use the same X_val data\n            lgb_pred = lgb_model.predict(lgb_X_val.values)\n            lgb_score = self.evaluate_model(lgb_y_val, lgb_pred, \"LightGBM\")\n            self.models['lightgbm'] = lgb_model\n\n            del lgb_X_train, lgb_y_train, lgb_X_val, lgb_y_val, lgb_pred # Clear LGBM-specific data\n            gc.collect()\n\n        except Exception as e:\n            print(f\"LightGBM training or prediction failed: {e}\")\n            lgb_score = 0\n\n        dl_predictions_list = []\n        dl_weights = []\n        \n        full_y_val_for_dl_eval = pd.read_parquet(self._scaled_val_y_path)['label'].values\n        y_val_dl_eval_aligned = full_y_val_for_dl_eval[self.sequence_length - 1:]\n\n        print(f\"Full Y_val for DL evaluation shape: {y_val_dl_eval_aligned.shape}\")\n\n        try:\n            if len(train_generator) > 0:\n                print(f\"Proceeding with Deep Learning models. Training generator has {len(train_generator)} batches.\")\n                callbacks = [\n                    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_mae'),\n                    ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6, monitor='val_mae')\n                ]\n                conv1d_score = 0 # Changed from convlstm_score\n                try:\n                    print(\"Training Conv1D model...\") # Changed print message\n                    dummy_X, _ = train_generator.__getitem__(0)\n                    input_shape_conv1d = dummy_X.shape[1:] # Changed from input_shape_convlstm\n                    del dummy_X\n                    gc.collect()\n\n                    conv1d_model = self.build_conv1d_model(input_shape_conv1d) # Changed to build_conv1d_model\n                    if conv1d_model:\n                        with tf.device('/GPU:0'): # Explicitly place on GPU\n                            conv1d_model.fit(\n                                train_generator,\n                                validation_data=val_generator,\n                                epochs=5, # Reduced epochs for faster testing\n                                callbacks=callbacks,\n                                verbose=2, # Suppress per-batch logging\n                                # workers=os.cpu_count(), use_multiprocessing=True # Reverted multiprocessing\n                            )\n                        with tf.device('/GPU:0'): # Explicitly place on GPU for prediction\n                            conv1d_pred = conv1d_model.predict(val_generator).flatten()\n                        conv1d_score = self.evaluate_model(y_val_dl_eval_aligned, conv1d_pred, \"Conv1D\") # Changed model name\n                        self.models['conv1d'] = conv1d_model # Changed model key\n                except Exception as e:\n                    print(f\"Conv1D training or prediction failed: {e}\") # Changed print message\n                    conv1d_score = 0\n                \n                # Removed LSTM training block\n                # lstm_score = 0\n                # try:\n                #     print(\"Training LSTM model...\")\n                #     ...\n                # except Exception as e:\n                #     print(f\"LSTM training or prediction failed: {e}\")\n                #     lstm_score = 0\n\n                # Ensemble logic now with LightGBM and Conv1D\n                if 'conv1d' in self.models and conv1d_score > 0: # Check if Conv1D was successful\n                    lgb_pred_aligned = lgb_pred[self.sequence_length - 1:]\n\n                    dl_predictions_list.append(conv1d_pred)\n                    dl_weights.append(0.6) # Adjust weight for single DL model\n\n                    dl_ensemble_weighted = np.average(dl_predictions_list, axis=0, weights=dl_weights)\n                    ensemble_pred = (lgb_pred_aligned * 0.4) + (dl_ensemble_weighted * 0.6)\n                    ensemble_score = self.evaluate_model(y_val_dl_eval_aligned, ensemble_pred, \"Ensemble\")\n                    print(f\"\\nEnsemble score: {ensemble_score:.4f}\")\n                else:\n                    print(\"No successful deep learning models to include in ensemble (only Conv1D was attempted).\")\n                    ensemble_score = lgb_score # Fallback to LGBM score if Conv1D failed\n\n                print(f\"\\nBest individual model score: {max(lgb_score, conv1d_score):.4f}\") # Adjusted max\n                print(f\"Final overall ensemble score: {ensemble_score:.4f}\")\n            else:\n                print(\"Skipping deep learning models as training generator is empty (e.g., due to insufficient data for sequence_length).\")\n                ensemble_score = lgb_score\n        except Exception as e:\n            print(f\"Deep learning training failed during overall process (likely model build or generator issue): {e}\")\n            ensemble_score = lgb_score\n        \n        for path in [self._scaled_X_path, self._scaled_y_path, self._scaled_val_X_path, self._scaled_val_y_path]:\n            if os.path.exists(path):\n                os.remove(path)\n                print(f\"Cleaned up temporary file: {path}\")\n        gc.collect()\n        return self\n\n    def predict(self, test_data_raw_initial_load):\n        \"\"\"Generate predictions for test data with robust error handling\"\"\"\n        print(\"Generating predictions...\")\n\n        temp_df_for_id_map = test_data_raw_initial_load.copy()\n        if 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index.name == 'ID':\n            temp_df_for_id_map = temp_df_for_id_map.reset_index(names=['ID'])\n        elif 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index is not None and temp_df_for_id_map.index.name is None and len(temp_df_for_id_map.index) == len(temp_df_for_id_map):\n            temp_df_for_id_map = temp_df_for_id_map.reset_index()\n            temp_df_for_id_map.rename(columns={'index': 'ID'}, inplace=True)\n\n        id_to_original_index_map = pd.Series(temp_df_for_id_map.index.values, index=temp_df_for_id_map['ID'])\n        original_test_ids = temp_df_for_id_map['ID'].copy()\n\n        del temp_df_for_id_map\n        gc.collect()\n\n        basic_features_cols_test = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'ID']\n        X_n_cols_raw_test = [col for col in test_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        preselected_X_n_features_test = X_n_cols_raw_test[:min(self.top_X_features_to_preselect, len(X_n_cols_raw_test))]\n\n        columns_to_process_raw_for_predict = basic_features_cols_test + preselected_X_n_features_test\n\n        missing_columns = [col for col in columns_to_process_raw_for_predict if col not in test_data_raw_initial_load.columns]\n        if missing_columns:\n            raise KeyError(f\"The following required columns are missing from the test data: {missing_columns}. Please ensure your test.parquet file contains these columns.\")\n\n        print(f\"Selecting columns from initial raw test data (only {len(columns_to_process_raw_for_predict)} columns)...\")\n        test_df = test_data_raw_initial_load[columns_to_process_raw_for_predict].copy()\n\n        del test_data_raw_initial_load\n        gc.collect()\n\n        if 'ID' not in test_df.columns and test_df.index.name == 'ID':\n            test_df = test_df.reset_index()\n\n        if 'timestamp' in test_df.columns:\n            test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n\n        test_df = self.optimize_memory(test_df)\n        test_df = self.create_time_features(test_df)\n\n        print(f\"Test data shape after feature engineering: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n\n        if self.selected_features is None:\n            raise ValueError(\"Model must be fitted before making predictions (selected_features is None)\")\n\n        X_test_df_final = test_df[self.selected_features]\n\n        X_test_df_final = X_test_df_final.replace([np.inf, -np.inf], np.nan)\n        X_test_df_final = X_test_df_final.ffill().bfill().fillna(0)\n\n        X_test_scaled = self.scaler.transform(X_test_df_final)\n\n        if np.any(np.isnan(X_test_scaled)) or np.any(np.isinf(X_test_scaled)):\n            print(\"WARNING: Invalid values in test data after scaling, applying emergency cleanup.\")\n            X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n\n        X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test_df_final.index, columns=self.selected_features)\n\n        del X_test_df_final, test_df\n        gc.collect()\n\n        predictions = np.zeros(len(original_test_ids), dtype=np.float32)\n        indexed_predictions_by_internal_idx = {}\n\n        if 'lightgbm' in self.models:\n            try:\n                # Ensure X_test_scaled_df is cleaned before prediction\n                X_test_scaled_for_lgbm = X_test_scaled_df.copy()\n                if np.any(np.isnan(X_test_scaled_for_lgbm.values)) or np.any(np.isinf(X_test_scaled_for_lgbm.values)):\n                    print(\"WARNING: NaNs/Infs found in X_test_scaled_for_lgbm. Applying emergency cleanup.\")\n                    X_test_scaled_for_lgbm = pd.DataFrame(\n                        np.nan_to_num(X_test_scaled_for_lgbm.values, nan=0.0, posinf=1e6, neginf=-1e6),\n                        columns=X_test_scaled_for_lgbm.columns, index=X_test_scaled_for_lgbm.index\n                    )\n\n                lgb_pred_full = self.models['lightgbm'].predict(X_test_scaled_for_lgbm)\n                for i, idx in enumerate(X_test_scaled_for_lgbm.index): # Use cleaned df for iteration\n                    indexed_predictions_by_internal_idx[idx] = lgb_pred_full[i]\n                print(\"LightGBM predictions generated.\")\n                del X_test_scaled_for_lgbm # Clear temporary\n                gc.collect()\n            except Exception as e:\n                print(f\"LightGBM prediction failed: {e}\")\n\n        if ('conv1d' in self.models and self.models['conv1d'] is not None): # Changed from convlstm/lstm\n            try:\n                # For prediction, we need to save test data to a temp file and use a generator\n                # to avoid OOM if test_data is also large.\n                # First, save X_test_scaled_df to a temporary parquet file with original_row_index\n                X_test_df_to_save = X_test_scaled_df.reset_index().rename(columns={'index': 'original_row_index'})\n                temp_test_X_path = './scaled_test_X.parquet'\n                X_test_df_to_save.to_parquet(temp_test_X_path, index=False)\n                del X_test_df_to_save\n                gc.collect()\n\n                # Create a generator for test prediction\n                test_generator = TimeSeriesSequence(\n                    X_filepath=temp_test_X_path,\n                    y_filepath=temp_test_X_path, # y_filepath is dummy for prediction, not used\n                    sequence_length=self.sequence_length,\n                    batch_size=16, # Current batch_size\n                    feature_cols=self.selected_features,\n                    total_samples=len(X_test_scaled_df),\n                    original_indices_min=X_test_scaled_df.index.min(),\n                    original_indices_max=X_test_scaled_df.index.max()\n                )\n\n                if len(test_generator) > 0:\n                    dl_predictions_list_test = []\n                    dl_weights_test = []\n\n                    if 'conv1d' in self.models and self.models['conv1d'] is not None: # Changed from convlstm\n                        try:\n                            with tf.device('/GPU:0'): # Explicitly place on GPU for prediction\n                                conv1d_pred = self.models['conv1d'].predict(test_generator).flatten() # Changed from convlstm_pred\n                            dl_predictions_list_test.append(conv1d_pred)\n                            dl_weights_test.append(0.6) # Weight for Conv1D\n                        except Exception as e:\n                            print(f\"Conv1D prediction during test failed: {e}\") # Changed print message\n\n                    if dl_predictions_list_test: # Check if any DL predictions were added\n                        dl_ensemble_weighted = np.average(dl_predictions_list_test, axis=0, weights=dl_weights_test)\n                        \n                        # Indices for DL predictions in the test set\n                        # These are the original_row_index values for which DL predictions were made\n                        # We need to load these indices from the temp_test_X_path to align\n                        temp_test_df_for_indices = pd.read_parquet(temp_test_X_path, columns=['original_row_index'])\n                        sequence_covered_indices_in_X_scaled_df = temp_test_df_for_indices['original_row_index'].values[self.sequence_length - 1:].tolist()\n                        del temp_test_df_for_indices\n                        gc.collect()\n\n                        for i, internal_idx in enumerate(sequence_covered_indices_in_X_scaled_df):\n                            lgbm_part = indexed_predictions_by_internal_idx.get(internal_idx, 0.0)\n                            indexed_predictions_by_internal_idx[internal_idx] = (lgbm_part * 0.4) + (dl_ensemble_weighted[i] * 0.6)\n\n                        print(\"Deep learning models included in predictions.\")\n                    else:\n                        print(\"No successful deep learning models to include in test predictions.\")\n                else:\n                    print(\"Test data too short for deep learning sequences. Using LightGBM only.\")\n            except Exception as e:\n                print(f\"Deep learning prediction setup failed: {e}\")\n                print(\"Falling back to LightGBM predictions only for test set.\")\n            finally:\n                # Clean up temporary test data file\n                if os.path.exists(temp_test_X_path):\n                    os.remove(temp_test_X_path)\n                    print(f\"Cleaned up temporary test file: {temp_test_X_path}\")\n        \n        del X_test_scaled_df\n        gc.collect()\n\n        for i, current_id in enumerate(original_test_ids):\n            original_idx_in_raw_df = id_to_original_index_map.get(current_id)\n\n            if original_idx_in_raw_df is not None and original_idx_in_raw_df in indexed_predictions_by_internal_idx:\n                predictions[i] = indexed_predictions_by_internal_idx[original_idx_in_raw_df]\n            else:\n                predictions[i] = 0.0\n\n        if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n            print(\"WARNING: Invalid predictions detected (NaN/Inf), cleaning...\")\n            predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n\n        print(f\"Generated {len(predictions)} predictions\")\n        print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n\n        return predictions\n    \n    # Removed _prepare_sequences_for_inference as it's replaced by TimeSeriesSequence for prediction\n    # def _prepare_sequences_for_inference(self, data):\n    #     \"\"\"Helper to prepare sequences for inference, similar to original prepare_sequences.\"\"\"\n    #     ...\n\n\n# Main execution function\ndef run_competition_pipeline():\n    \"\"\"Run the complete competition pipeline\"\"\"\n\n    print(\"Loading data...\")\n    train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n\n    if 'timestamp' not in train_full_raw.columns:\n        if train_full_raw.index.name == 'timestamp':\n            train_full_raw = train_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n        else:\n            train_full_raw = train_full_raw.reset_index()\n            if 'index' in train_full_raw.columns and 'timestamp' not in train_full_raw.columns:\n                 train_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                 print(f\"DEBUG: Renamed 'index' to 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in train_full_raw.columns:\n        train_full_raw['timestamp'] = pd.to_datetime(train_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in train_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in train_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: train_full_raw columns are: {train_full_raw.columns.tolist()}\")\n\n    test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n\n    if 'ID' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'ID':\n            test_full_raw = test_full_raw.reset_index(names=['ID'])\n            print(f\"DEBUG: Resetting index 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'ID' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'ID'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'timestamp':\n            test_full_raw = test_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else:\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'timestamp' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    if 'timestamp' in test_full_raw.columns:\n        test_full_raw['timestamp'] = pd.to_datetime(test_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in test_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in test_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: test_full_raw columns are: {test_full_raw.columns.tolist()}\")\n\n    print(f\"\\nTrain shape: {train_full_raw.shape}\")\n    print(f\"Test shape: {test_full_raw.shape}\")\n\n    # Initialize and train model\n    predictor = CryptoMarketPredictor(\n        sequence_length=30,\n        top_features=100,\n        top_X_features_to_preselect=30\n    )\n    predictor.fit(train_full_raw)\n\n    predictions = predictor.predict(test_full_raw)\n\n    # Create submission\n    submission = pd.DataFrame({\n        'ID': test_full_raw['ID'],\n        'Prediction': predictions\n    })\n\n    # Save submission\n    submission.to_csv('/kaggle/working/submission.csv', index=False)\n    print(f\"Submission saved with {len(submission)} predictions\")\n    print(f\"Prediction statistics - Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n\n    return submission\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    submission = run_competition_pipeline()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}