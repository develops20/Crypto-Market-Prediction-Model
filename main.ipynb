{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# DRW Crypto Market Prediction Competition Pipeline\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os # Import os for path checking\n\n# Memory optimization and data processing\nimport gc\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\n\n# Deep learning and modeling \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (LSTM, ConvLSTM2D, Dense, Dropout, \n                                     BatchNormalization, Input, Conv1D, MaxPooling1D,\n                                     Flatten, Reshape, TimeDistributed)\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\n# Tree-based models for ensemble\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\n# Set memory growth for GPU (if available) - still good practice\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\nclass CryptoMarketPredictor:\n    def __init__(self, sequence_length=30, top_features=100, top_X_features_to_preselect=30): # Reverted: sequence_length, top_features, top_X_features_to_preselect\n        self.sequence_length = sequence_length \n        self.top_features = top_features \n        self.top_X_features_to_preselect = top_X_features_to_preselect \n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None # Stores final selected feature names\n        self.models = {}\n        # This checkpoint path will remain local to the Colab session for temporary storage\n        self._checkpoint_path = './processed_train_data_checkpoint.parquet' \n        \n    def optimize_memory(self, df):\n        \"\"\"\n        Optimize Pandas DataFrame memory usage and clean data.\n        \"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        \n        # Clean data first\n        df = self.clean_data(df)\n        \n        # Optimize numeric columns (Pandas directly)\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col == 'timestamp' or col == 'ID' or col == 'label':\n                continue # Don't downcast timestamp, ID, or label\n            df[col] = pd.to_numeric(df[col], downcast='float')\n            \n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n    \n    def clean_data(self, df):\n        \"\"\"\n        Clean Pandas DataFrame by handling inf, -inf, and extreme values.\n        \"\"\"\n        print(\"Cleaning data...\")\n        \n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        # Replace inf and -inf with NaN first\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n        # Fill NaN values with forward fill, then backward fill, then 0\n        for col in numeric_cols:\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        # Handle extreme outliers (values beyond 3*IQR)\n        for col in df.select_dtypes(include=[np.float32, np.float64]).columns:\n            if col in ['timestamp', 'ID', 'label']: continue \n            if df[col].nunique() > 1:\n                q25 = df[col].quantile(0.25)\n                q75 = df[col].quantile(0.75)\n                iqr = q75 - q25\n                \n                if iqr != 0 and not pd.isna(iqr):\n                    lower_bound = q25 - 3 * iqr\n                    upper_bound = q75 + 3 * iqr\n                    df[col] = df[col].clip(lower_bound, upper_bound)\n        print(\"Data cleaning applied.\")\n        return df\n    \n    def create_time_features(self, df):\n        \"\"\"\n        Create time-based features with robust calculations using Pandas.\n        Significantly reduced complexity for faster execution.\n        \"\"\"\n        print(\"Creating time-based features...\")\n        \n        # Basic market features\n        df['mid_price'] = (df['bid_qty'] + df['ask_qty']) / 2\n        df['spread'] = df['ask_qty'] - df['bid_qty']\n        \n        # Safe division for imbalance\n        denominator = df['bid_qty'] + df['ask_qty'] + 1e-10\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / denominator\n        \n        # Safe division for buy/sell ratio\n        df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-10)\n        \n        # Rolling statistics - significantly reduced windows for speed\n        windows = [10, 30] # Reverted windows\n        base_cols_for_rolling = ['volume', 'mid_price', 'buy_qty', 'sell_qty', 'imbalance'] \n\n        for col in base_cols_for_rolling:\n            for window in windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0) \n        \n        # Lagged features - significantly reduced lags for speed\n        lags = [1, 5] # Reverted lags\n        base_cols_for_lag = ['mid_price', 'imbalance'] \n\n        for col in base_cols_for_lag:\n            for lag in lags:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n        \n        # Technical indicators - reduced\n        df['rsi_proxy'] = self.calculate_rsi_proxy(df['mid_price'], window=10) # Reverted window\n        df['momentum'] = df['mid_price'] - df['mid_price'].shift(5) # Reverted shift\n        \n        # Final check for any inf/nan values that might have been introduced\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'ID', 'label']]\n\n        for col in numeric_cols:\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n            df[col] = df[col].ffill().bfill().fillna(0)\n\n        print(f\"Time-based features created. Current shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n        return df\n    \n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate RSI-like indicator with safe operations for Pandas Series.\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n        \n        rs = gain / (loss + 1e-10)\n        rsi = 100 - (100 / (1 + rs))\n        \n        rsi = rsi.replace([np.inf, -np.inf], np.nan).fillna(50)\n        \n        return rsi\n    \n    def select_features(self, X_df, y_df, method='mutual_info'):\n        \"\"\"\n        Feature selection to reduce dimensionality with robust handling.\n        Operates directly on Pandas DataFrames.\n        \"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n        \n        print(\"Validating data before final feature selection...\")\n        \n        # Check for any remaining inf/nan values\n        inf_mask = np.isinf(X_df).any(axis=1)\n        nan_mask = np.isnan(X_df).any(axis=1)\n        invalid_mask = inf_mask | nan_mask\n        \n        if invalid_mask.sum() > 0:\n            print(f\"Removing {invalid_mask.sum()} rows with invalid values before final selection.\")\n            X_df = X_df[~invalid_mask]\n            y_df = y_df[~invalid_mask]\n        \n        # Check for constant or near-constant features (can cause issues for some selectors)\n        feature_std = X_df.std()\n        constant_features_mask = feature_std < 1e-8 \n        \n        if constant_features_mask.all():\n            print(\"Warning: All features are constant. Cannot perform feature selection.\")\n            non_constant_features = X_df.columns.tolist() \n        else:\n            non_constant_features = X_df.columns[~constant_features_mask].tolist()\n            if constant_features_mask.sum() > 0:\n                print(f\"Removing {constant_features_mask.sum()} constant features.\")\n            X_df = X_df[non_constant_features]\n\n\n        print(f\"Final data shape for feature selection: {X_df.shape}\")\n        \n        n_features_to_select = min(self.top_features, X_df.shape[1])\n        \n        if method == 'mutual_info':\n            selector = SelectKBest(score_func=mutual_info_regression, k=n_features_to_select)\n        else:\n            selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n        \n        X_selected = selector.fit_transform(X_df, y_df)\n        self.feature_selector = selector\n        self.selected_features = X_df.columns[selector.get_support()].tolist()\n        \n        print(f\"\\n--- Selected Features ({len(self.selected_features)}) ---\")\n        for feature in self.selected_features:\n            print(f\"- {feature}\")\n        print(\"---------------------------------------\\n\")\n\n        # Return as NumPy array and Pandas Index to maintain alignment with y\n        return X_selected.astype(np.float32), X_df.index\n    \n    def prepare_sequences(self, data, target=None):\n        \"\"\"Prepare sequences for time series models\"\"\"\n        sequences = []\n        targets = []\n        \n        for i in range(self.sequence_length, len(data)):\n            sequences.append(data[i-self.sequence_length:i])\n            if target is not None:\n                targets.append(target[i])\n        \n        # Ensure outputs are float32 for TensorFlow models to save memory\n        return np.array(sequences).astype(np.float32), np.array(targets).astype(np.float32) if target is not None else None\n    \n    def build_convlstm_model(self, input_shape):\n        \"\"\"Build ConvLSTM model for spatial-temporal patterns\"\"\"\n        model = Sequential([\n            # Reshape for ConvLSTM (samples, time_steps, rows, cols, channels)\n            # input_shape is (sequence_length, num_features)\n            # We need (sequence_length, 1, num_features, 1)\n            Reshape((self.sequence_length, 1, input_shape[1], 1), input_shape=input_shape),\n            \n            ConvLSTM2D(filters=64, kernel_size=(1, 3), # Reverted filters\n                       activation='tanh', recurrent_activation='sigmoid',\n                       return_sequences=True, dropout=0.2, padding='same'), \n            BatchNormalization(),\n            \n            ConvLSTM2D(filters=32, kernel_size=(1, 3), # Reverted filters\n                       activation='tanh', recurrent_activation='sigmoid',\n                       return_sequences=False, dropout=0.2, padding='same'), \n            BatchNormalization(),\n            \n            Flatten(),\n            Dense(50, activation='relu'), # Reverted dense units\n            Dropout(0.3),\n            Dense(1, activation='linear')\n        ])\n        \n        model.compile(optimizer=Adam(learning_rate=0.001), \n                      loss='mae', metrics=['mae'])\n        return model\n    \n    def build_lstm_model(self, input_shape):\n        \"\"\"Build standard LSTM model\"\"\"\n        model = Sequential([\n            LSTM(100, return_sequences=True, input_shape=input_shape, dropout=0.2), # Reverted LSTM units\n            BatchNormalization(),\n            LSTM(50, return_sequences=False, dropout=0.2), # Reverted LSTM units\n            BatchNormalization(),\n            Dense(50, activation='relu'), # Reverted dense units\n            Dropout(0.3),\n            Dense(1, activation='linear')\n        ])\n        \n        model.compile(optimizer=Adam(learning_rate=0.001), \n                      loss='mae', metrics=['mae'])\n        return model\n\n    def build_cnn_lstm_model(self, input_shape):\n        print(\"CNN-LSTM model building skipped for current speed optimization, but can be reinstated later.\")\n        return None \n    \n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train LightGBM model\"\"\"\n        print(\"Training LightGBM model...\")\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n        \n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31, # Reverted leaves\n            'learning_rate': 0.05,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 1000 # Reverted estimators\n        }\n        \n        model = lgb.train(\n            params,\n            train_data,\n            valid_sets=[val_data],\n            num_boost_round=params['n_estimators'], \n            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)] # Reverted early stopping rounds\n        )\n        \n        return model\n    \n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance\"\"\"\n        mae = mean_absolute_error(y_true, y_pred)\n        correlation, _ = pearsonr(y_true, y_pred)\n        \n        print(f\"{model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n    \n    def fit(self, train_data_raw_initial_load): \n        \"\"\"Main training pipeline with robust error handling\"\"\"\n        print(\"Starting training pipeline...\")\n        \n        # Determine raw X_n columns from the initial load structure\n        X_n_cols_raw = [col for col in train_data_raw_initial_load.columns if col.startswith('X') and col != 'label']   \n        basic_features_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n        \n        # --- Aggressive initial column selection for Pandas memory management ---\n        # We will ONLY select these columns from the already loaded raw DataFrame\n        preselected_X_n_features = X_n_cols_raw[:min(self.top_X_features_to_preselect, len(X_n_cols_raw))]\n        print(f\"Initially selected {len(preselected_X_n_features)} X_n features for direct Pandas load (due to memory constraints without Dask).\")\n\n        # Ensure 'timestamp' and 'label' are in columns_to_process for the fit method\n        columns_to_process_raw_for_fit = basic_features_cols + preselected_X_n_features + ['label']\n        \n        train_df = None\n        if os.path.exists(self._checkpoint_path):\n            print(f\"Checkpoint found. Loading processed data from {self._checkpoint_path}...\")\n            train_df = pd.read_parquet(self._checkpoint_path)\n            # Ensure timestamp is datetime for Pandas DataFrame if not already\n            if 'timestamp' in train_df.columns:\n                train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n        else:\n            print(f\"Selecting columns from initial raw training data...\")\n            train_df = train_data_raw_initial_load[columns_to_process_raw_for_fit].copy()\n            if 'timestamp' not in train_df.columns and train_df.index.name == 'timestamp':\n                train_df = train_df.reset_index(names=['timestamp'])\n            train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n            gc.collect()\n            train_df = self.optimize_memory(train_df)   \n            train_df = self.create_time_features(train_df)\n            print(f\"Saving processed data to checkpoint: {self._checkpoint_path}...\")\n            train_df.to_parquet(self._checkpoint_path, index=False) \n            print(\"Checkpoint saved.\")\n            \n        print(f\"Data shape after feature engineering: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n        feature_cols_final = [col for col in train_df.columns \n                              if col not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols_final]\n        y_df = train_df['label']\n        print(f\"Features shape before final selection: {X_df.shape[0]} rows, {X_df.shape[1]} columns\")\n        print(f\"Target shape: {y_df.shape[0]} rows\")\n        del train_df \n        gc.collect()\n        X_selected, valid_idx = self.select_features(X_df, y_df)    \n        y_for_training = y_df.loc[valid_idx].astype(np.float32)\n        del X_df, y_df \n        gc.collect()\n        X_scaled = self.scaler.fit_transform(X_selected)\n        print(\"Final data validation (after scaling)...\")\n        if np.any(np.isnan(X_scaled)) or np.any(np.isinf(X_scaled)):\n            print(\"ERROR: Still have invalid values after preprocessing! Applying emergency cleanup.\")\n            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        print(f\"Final training data shape: {X_scaled.shape}\")\n        temp_train_full_timestamps = train_data_raw_initial_load[['timestamp']].copy()\n        if 'timestamp' not in temp_train_full_timestamps.columns and temp_train_full_timestamps.index.name == 'timestamp':\n            temp_train_full_timestamps = temp_train_full_timestamps.reset_index(names=['timestamp'])\n        temp_train_full_timestamps['timestamp'] = pd.to_datetime(temp_train_full_timestamps['timestamp'])\n        VALIDATION_SPLIT_DATE = '2024-01-01' # Reverted validation split date\n        original_train_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] < VALIDATION_SPLIT_DATE].index\n        original_val_indices = temp_train_full_timestamps[temp_train_full_timestamps['timestamp'] >= VALIDATION_SPLIT_DATE].index\n        X_scaled_temp_df = pd.DataFrame(X_scaled, index=valid_idx, columns=self.selected_features)\n        actual_train_indices = original_train_indices.intersection(X_scaled_temp_df.index)\n        actual_val_indices = original_val_indices.intersection(X_scaled_temp_df.index)\n        X_train = X_scaled_temp_df.loc[actual_train_indices].values.astype(np.float32)\n        y_train = y_for_training.loc[actual_train_indices].values.astype(np.float32)\n        X_val = X_scaled_temp_df.loc[actual_val_indices].values.astype(np.float32)\n        y_val = y_for_training.loc[actual_val_indices].values.astype(np.float32)\n        del temp_train_full_timestamps, X_scaled_temp_df, y_for_training \n        gc.collect() \n        print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n        try:\n            lgb_model = self.train_lightgbm(X_train, y_train, X_val, y_val)\n            lgb_pred = lgb_model.predict(X_val)\n            lgb_score = self.evaluate_model(y_val, lgb_pred, \"LightGBM\")\n            self.models['lightgbm'] = lgb_model\n        except Exception as e:\n            print(f\"LightGBM training failed: {e}\")\n            lgb_score = 0\n        \n        # Initialize DL prediction lists and weights here to ensure they are always defined\n        dl_predictions_list = []\n        dl_weights = [] \n\n        try:\n            X_train_seq, y_train_seq = self.prepare_sequences(X_train, y_train)\n            X_val_seq, y_val_seq = self.prepare_sequences(X_val, y_val)\n            if len(X_train_seq) > 0:\n                print(f\"Sequence data - Train: {X_train_seq.shape}, Val: {X_val_seq.shape}\")\n                callbacks = [\n                    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_mae'), # Reverted patience\n                    ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6, monitor='val_mae') # Reverted patience\n                ]\n                convlstm_score = 0 \n                try:\n                    print(\"Training ConvLSTM model...\")\n                    convlstm_model = self.build_convlstm_model(X_train_seq.shape[1:])\n                    if convlstm_model: \n                        convlstm_model.fit(\n                            X_train_seq, y_train_seq,\n                            validation_data=(X_val_seq, y_val_seq),\n                            epochs=50, batch_size=64, # Reverted epochs and batch size\n                            callbacks=callbacks, verbose=1 \n                        )\n                        convlstm_pred = convlstm_model.predict(X_val_seq).flatten()\n                        convlstm_score = self.evaluate_model(y_val_seq, convlstm_pred, \"ConvLSTM\")\n                        self.models['convlstm'] = convlstm_model\n                except Exception as e:\n                    print(f\"ConvLSTM training failed: {e}\")\n                    convlstm_score = 0\n                lstm_score = 0 \n                try:\n                    print(\"Training LSTM model...\")\n                    lstm_model = self.build_lstm_model(X_train_seq.shape[1:])\n                    if lstm_model: \n                        lstm_model.fit(\n                            X_train_seq, y_train_seq,\n                            validation_data=(X_val_seq, y_val_seq),\n                            epochs=50, batch_size=64, # Reverted epochs and batch size\n                            callbacks=callbacks, verbose=1 \n                        )\n                        lstm_pred = lstm_model.predict(X_val_seq).flatten()\n                        lstm_score = self.evaluate_model(y_val_seq, lstm_pred, \"LSTM\")\n                        self.models['lstm'] = lstm_model\n                except Exception as e:\n                    print(f\"LSTM training failed: {e}\")\n                    lstm_score = 0\n                if len(self.models) > 1:\n                    lgb_pred_aligned = lgb_pred[self.sequence_length:]\n                    \n                    # Populate dl_predictions_list and dl_weights only if models were successful\n                    if 'convlstm' in self.models and convlstm_score > 0: \n                        dl_predictions_list.append(convlstm_pred)\n                        dl_weights.append(0.35)\n                    if 'lstm' in self.models and lstm_score > 0: \n                        dl_predictions_list.append(lstm_pred) \n                        dl_weights.append(0.25)\n\n                    if dl_predictions_list: # Check if any DL predictions were added\n                        dl_ensemble_weighted = np.average(dl_predictions_list, axis=0, weights=dl_weights)\n                        ensemble_pred = (lgb_pred_aligned * 0.4) + (dl_ensemble_weighted * 0.6)\n                        ensemble_score = self.evaluate_model(y_val_seq, ensemble_pred, \"Ensemble\")\n                        print(f\"\\nEnsemble score: {ensemble_score:.4f}\")\n                    else:\n                        print(\"No successful deep learning models to include in ensemble.\")\n                        ensemble_score = lgb_score \n                else:\n                    print(\"Skipping deep learning models as no sequences could be prepared.\")\n                    ensemble_score = lgb_score \n                print(f\"\\nBest individual model score: {max(lgb_score, convlstm_score, lstm_score) if 'convlstm' in self.models else lgb_score:.4f}\")\n                print(f\"Ensemble score: {ensemble_score:.4f}\") \n        except Exception as e:\n            print(f\"Deep learning training failed during ensemble: {e}\")\n            ensemble_score = lgb_score \n        del X_train, y_train, X_val, y_val \n        if 'X_train_seq' in locals() and X_train_seq is not None:\n             del X_train_seq, y_train_seq, X_val_seq, y_val_seq\n        gc.collect()\n        return self\n    \n    def predict(self, test_data_raw_initial_load): \n        \"\"\"Generate predictions for test data with robust error handling\"\"\"\n        print(\"Generating predictions...\")\n        \n        # --- Capture original ID to index mapping early ---\n        # Ensure test_data_raw_initial_load has 'ID' and is ready for mapping\n        temp_df_for_id_map = test_data_raw_initial_load.copy()\n        # Handle cases where 'ID' might be the index but not explicitly a column name\n        if 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index.name == 'ID':\n            temp_df_for_id_map = temp_df_for_id_map.reset_index(names=['ID'])\n        elif 'ID' not in temp_df_for_id_map.columns and temp_df_for_id_map.index is not None and temp_df_for_id_map.index.name is None and len(temp_df_for_id_map.index) == len(temp_df_for_id_map):\n            # Fallback if ID is an unnamed index that matches length\n            temp_df_for_id_map = temp_df_for_id_map.reset_index()\n            temp_df_for_id_map.rename(columns={'index': 'ID'}, inplace=True) # Rename the default 'index' column\n\n        # Create the series mapping 'ID' values to their corresponding *original DataFrame indices*\n        id_to_original_index_map = pd.Series(temp_df_for_id_map.index.values, index=temp_df_for_id_map['ID'])\n\n        # Store original test IDs for final submission mapping (these are the IDs from the input order)\n        original_test_ids = temp_df_for_id_map['ID'].copy()\n        \n        # Free up temporary mapping dataframe memory (now that we have the series and original IDs)\n        del temp_df_for_id_map\n        gc.collect()\n\n        # --- Aggressive initial column selection for Pandas memory management ---\n        basic_features_cols_test = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'ID']\n        X_n_cols_raw_test = [col for col in test_data_raw_initial_load.columns if col.startswith('X') and col != 'label']\n        preselected_X_n_features_test = X_n_cols_raw_test[:min(self.top_X_features_to_preselect, len(X_n_cols_raw_test))]\n\n        columns_to_process_raw_for_predict = basic_features_cols_test + preselected_X_n_features_test\n        \n        # --- Crucial Check before subsetting ---\n        missing_columns = [col for col in columns_to_process_raw_for_predict if col not in test_data_raw_initial_load.columns]\n        if missing_columns:\n            raise KeyError(f\"The following required columns are missing from the test data: {missing_columns}. Please ensure your test.parquet file contains these columns.\")\n\n        print(f\"Selecting columns from initial raw test data (only {len(columns_to_process_raw_for_predict)} columns)...\")\n        # Select columns directly from the already loaded test_data_raw_initial_load\n        test_df = test_data_raw_initial_load[columns_to_process_raw_for_predict].copy()\n        \n        # Now it's safe to delete the original raw load as its necessary info has been extracted\n        del test_data_raw_initial_load \n        gc.collect()\n\n        # Ensure 'ID' is a column if it was index (this should be handled by temp_df_for_id_map now but a safety check)\n        if 'ID' not in test_df.columns and test_df.index.name == 'ID':\n            test_df = test_df.reset_index()\n            \n        # Ensure timestamp is datetime\n        if 'timestamp' in test_df.columns:\n            test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n\n        # Memory optimization and data cleaning (now on Pandas DataFrame)\n        test_df = self.optimize_memory(test_df)\n        \n        # Feature engineering (same as training, now on Pandas DataFrame)\n        test_df = self.create_time_features(test_df)\n        \n        print(f\"Test data shape after feature engineering: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n        \n        # Prepare features for prediction\n        if self.selected_features is None:\n            raise ValueError(\"Model must be fitted before making predictions (selected_features is None)\")\n        \n        # Filter test_data to include only the final selected features (from training)\n        # This is now safe as test_df has all engineered features\n        X_test_df_final = test_df[self.selected_features] # Ensure order matches training features\n\n        # Handle any remaining invalid values before scaling\n        X_test_df_final = X_test_df_final.replace([np.inf, -np.inf], np.nan)\n        X_test_df_final = X_test_df_final.fillna(method='ffill').fillna(method='bfill').fillna(0)\n        \n        # Scale features\n        X_test_scaled = self.scaler.transform(X_test_df_final)\n        \n        # Final validation\n        if np.any(np.isnan(X_test_scaled)) or np.any(np.isinf(X_test_scaled)):\n            print(\"WARNING: Invalid values in test data after scaling, applying emergency cleanup.\")\n            X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e6, neginf=-1e6)\n        \n        # Create a DataFrame from X_test_scaled to maintain original indices for prediction mapping\n        X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test_df_final.index, columns=self.selected_features)\n        \n        del X_test_df_final, test_df # Clean up\n        gc.collect()\n\n        # Initialize full predictions array based on the number of original IDs\n        predictions = np.zeros(len(original_test_ids), dtype=np.float32) \n\n        # Dictionary to store predictions mapped by the internal DataFrame index after sequence preparation\n        indexed_predictions_by_internal_idx = {}\n        \n        # LightGBM predictions (baseline/fallback)\n        if 'lightgbm' in self.models:\n            try:\n                lgb_pred_full = self.models['lightgbm'].predict(X_test_scaled_df)\n                # Map LGBM predictions to their internal DataFrame indices\n                for i, idx in enumerate(X_test_scaled_df.index):\n                    indexed_predictions_by_internal_idx[idx] = lgb_pred_full[i]\n                print(\"LightGBM predictions generated.\")\n            except Exception as e:\n                print(f\"LightGBM prediction failed: {e}\")\n                \n        # Deep learning prediction logic for ensemble\n        if ('convlstm' in self.models and self.models['convlstm'] is not None) or \\\n           ('lstm' in self.models and self.models['lstm'] is not None):\n            try:\n                X_test_seq, _ = self.prepare_sequences(X_test_scaled) \n                \n                if len(X_test_seq) > 0:\n                    dl_predictions_list_test = [] # Use distinct name for clarity in predict method\n                    dl_weights_test = [] # Use distinct name for clarity in predict method\n                    \n                    if 'convlstm' in self.models and self.models['convlstm'] is not None:\n                        try:\n                            convlstm_pred = self.models['convlstm'].predict(X_test_seq).flatten()\n                            dl_predictions_list_test.append(convlstm_pred)\n                            dl_weights_test.append(0.6) # Weight for ConvLSTM in DL ensemble\n                        except Exception as e:\n                            print(f\"ConvLSTM prediction during test failed: {e}\")\n                    \n                    if 'lstm' in self.models and self.models['lstm'] is not None:\n                        try:\n                            lstm_pred = self.models['lstm'].predict(X_test_seq).flatten()\n                            dl_predictions_list_test.append(lstm_pred)\n                            dl_weights_test.append(0.4) # Weight for LSTM in DL ensemble\n                        except Exception as e:\n                            print(f\"LSTM prediction during test failed: {e}\")\n                    \n                    # Combine deep learning predictions\n                    if dl_predictions_list_test: # Check if any DL predictions were added\n                        dl_ensemble_weighted = np.average(dl_predictions_list_test, axis=0, weights=dl_weights_test)\n                        \n                        # Apply ensemble to the part of the test data covered by sequences\n                        # These predictions map to indices starting from `self.sequence_length`\n                        sequence_covered_indices_in_X_scaled_df = X_test_scaled_df.index[self.sequence_length:].tolist()\n                        \n                        for i, internal_idx in enumerate(sequence_covered_indices_in_X_scaled_df):\n                            # Combine LGBM's initial prediction for this internal_idx with DL ensemble\n                            lgbm_part = indexed_predictions_by_internal_idx.get(internal_idx, 0.0)    \n                            indexed_predictions_by_internal_idx[internal_idx] = (lgbm_part * 0.4) + (dl_ensemble_weighted[i] * 0.6)\n                        \n                        print(\"Deep learning models included in predictions.\")\n                    else:\n                        print(\"No successful deep learning models to include in test predictions.\")\n                else:\n                    print(\"Test data too short for deep learning sequences. Using LightGBM only.\")\n            except Exception as e:\n                print(f\"Deep learning prediction setup failed: {e}\")\n                print(\"Falling back to LightGBM predictions only for test set.\")\n        \n        # Populate the final predictions array using the original_test_ids order\n        # Iterate through the original_test_ids to place predictions in the correct overall submission order\n        for i, current_id in enumerate(original_test_ids):\n            # Find the internal DataFrame index that corresponds to this 'ID'\n            # We created id_to_original_index_map earlier, which maps ID to its original dataframe index\n            # And then indexed_predictions_by_internal_idx stores predictions keyed by that original dataframe index\n            original_idx_in_raw_df = id_to_original_index_map.get(current_id)\n            \n            if original_idx_in_raw_df is not None and original_idx_in_raw_df in indexed_predictions_by_internal_idx:\n                predictions[i] = indexed_predictions_by_internal_idx[original_idx_in_raw_df]\n            else:\n                predictions[i] = 0.0 # Default to 0.0 if no prediction found (e.g., due to data cleaning/filtering)\n        \n        # Final validation of predictions\n        if np.any(np.isnan(predictions)) or np.any(np.isinf(predictions)):\n            print(\"WARNING: Invalid predictions detected (NaN/Inf), cleaning...\")\n            predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n        \n        print(f\"Generated {len(predictions)} predictions\")\n        print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n        \n        return predictions\n\n# Main execution function\ndef run_competition_pipeline():\n    \"\"\"Run the complete competition pipeline\"\"\"\n    \n    # Load data (initial raw load to get shape and display head)\n    print(\"Loading data...\")\n    # train_full_raw = pd.read_parquet('/content/drive/MyDrive/CryptoPrediction/train.parquet') \n    train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet') \n    \n    # Ensure 'timestamp' is always a column in train_full_raw\n    if 'timestamp' not in train_full_raw.columns:\n        if train_full_raw.index.name == 'timestamp':\n            train_full_raw = train_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n        else: # Attempt to reset generic index if 'timestamp' not found as column or named index\n            train_full_raw = train_full_raw.reset_index()\n            # If after reset, 'timestamp' is still not a column, and 'index' is, try renaming 'index'\n            if 'index' in train_full_raw.columns and 'timestamp' not in train_full_raw.columns:\n                 train_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                 print(f\"DEBUG: Renamed 'index' to 'timestamp' for train_full_raw. New columns: {train_full_raw.columns.tolist()}\")\n    \n    # Convert 'timestamp' to datetime, assuming it's now a column\n    if 'timestamp' in train_full_raw.columns:\n        train_full_raw['timestamp'] = pd.to_datetime(train_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in train_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in train_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: train_full_raw columns are: {train_full_raw.columns.tolist()}\")\n\n    # test_full_raw = pd.read_parquet('/content/drive/MyDrive/CryptoPrediction/test.parquet')\n    test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n    \n    # Ensure 'ID' is always a column in test_full_raw\n    if 'ID' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'ID':\n            test_full_raw = test_full_raw.reset_index(names=['ID'])\n            print(f\"DEBUG: Resetting index 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else: # Attempt to reset generic index if 'ID' not found as column or named index\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'ID' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'ID'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'ID' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    # Ensure 'timestamp' is always a column in test_full_raw\n    if 'timestamp' not in test_full_raw.columns:\n        if test_full_raw.index.name == 'timestamp':\n            test_full_raw = test_full_raw.reset_index(names=['timestamp'])\n            print(f\"DEBUG: Resetting index 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n        else: # Attempt to reset generic index if 'timestamp' not found as column or named index\n            test_full_raw = test_full_raw.reset_index()\n            if 'index' in test_full_raw.columns and 'timestamp' not in test_full_raw.columns:\n                test_full_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n                print(f\"DEBUG: Renamed 'index' to 'timestamp' for test_full_raw. New columns: {test_full_raw.columns.tolist()}\")\n\n    # Convert 'timestamp' to datetime for test_full_raw, assuming it's now a column\n    if 'timestamp' in test_full_raw.columns:\n        test_full_raw['timestamp'] = pd.to_datetime(test_full_raw['timestamp'])\n        print(f\"DEBUG: 'timestamp' column found and converted to datetime in test_full_raw.\")\n    else:\n        print(\"CRITICAL WARNING: 'timestamp' column still not found in test_full_raw after all attempts. This will likely cause issues.\")\n        print(f\"DEBUG: test_full_raw columns are: {test_full_raw.columns.tolist()}\")\n    \n    print(f\"\\nTrain shape: {train_full_raw.shape}\")\n    print(f\"Test shape: {test_full_raw.shape}\")\n    \n    # Initialize and train model\n    predictor = CryptoMarketPredictor(\n        sequence_length=30, # Reverted\n        top_features=100,    # Reverted\n        top_X_features_to_preselect=30 # Reverted\n    )    \n    predictor.fit(train_full_raw) \n    \n    predictions = predictor.predict(test_full_raw) \n    \n    # Create submission\n    submission = pd.DataFrame({\n        'ID': test_full_raw['ID'], # Use the 'ID' column from the original test DataFrame\n        'Prediction': predictions\n    })\n    \n    # Save submission\n    # Saving submission.csv to the same Google Drive folder as data\n    # submission.to_csv('/content/drive/MyDrive/CryptoPrediction/submission.csv', index=False)\n    submission.to_csv('/kaggle/working/submission.csv', index=False)\n    print(f\"Submission saved with {len(submission)} predictions\")\n    print(f\"Prediction statistics - Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n    \n    return submission\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    submission = run_competition_pipeline()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T00:30:32.642828Z","iopub.execute_input":"2025-06-27T00:30:32.643189Z"}},"outputs":[{"name":"stderr","text":"2025-06-27 00:30:35.676004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750984235.865805      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750984235.925563      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-27 00:30:54.104321: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Loading data...\nDEBUG: Resetting index 'timestamp' for train_full_raw. New columns: ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X107', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X121', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X149', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X188', 'X189', 'X190', 'X191', 'X192', 'X193', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X233', 'X234', 'X235', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X268', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X289', 'X290', 'X291', 'X292', 'X293', 'X294', 'X295', 'X296', 'X297', 'X298', 'X299', 'X300', 'X301', 'X302', 'X303', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X330', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X347', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X381', 'X382', 'X383', 'X384', 'X385', 'X386', 'X387', 'X388', 'X389', 'X390', 'X391', 'X392', 'X393', 'X394', 'X395', 'X396', 'X397', 'X398', 'X399', 'X400', 'X401', 'X402', 'X403', 'X404', 'X405', 'X406', 'X407', 'X408', 'X409', 'X410', 'X411', 'X412', 'X413', 'X414', 'X415', 'X416', 'X417', 'X418', 'X419', 'X420', 'X421', 'X422', 'X423', 'X424', 'X425', 'X426', 'X427', 'X428', 'X429', 'X430', 'X431', 'X432', 'X433', 'X434', 'X435', 'X436', 'X437', 'X438', 'X439', 'X440', 'X441', 'X442', 'X443', 'X444', 'X445', 'X446', 'X447', 'X448', 'X449', 'X450', 'X451', 'X452', 'X453', 'X454', 'X455', 'X456', 'X457', 'X458', 'X459', 'X460', 'X461', 'X462', 'X463', 'X464', 'X465', 'X466', 'X467', 'X468', 'X469', 'X470', 'X471', 'X472', 'X473', 'X474', 'X475', 'X476', 'X477', 'X478', 'X479', 'X480', 'X481', 'X482', 'X483', 'X484', 'X485', 'X486', 'X487', 'X488', 'X489', 'X490', 'X491', 'X492', 'X493', 'X494', 'X495', 'X496', 'X497', 'X498', 'X499', 'X500', 'X501', 'X502', 'X503', 'X504', 'X505', 'X506', 'X507', 'X508', 'X509', 'X510', 'X511', 'X512', 'X513', 'X514', 'X515', 'X516', 'X517', 'X518', 'X519', 'X520', 'X521', 'X522', 'X523', 'X524', 'X525', 'X526', 'X527', 'X528', 'X529', 'X530', 'X531', 'X532', 'X533', 'X534', 'X535', 'X536', 'X537', 'X538', 'X539', 'X540', 'X541', 'X542', 'X543', 'X544', 'X545', 'X546', 'X547', 'X548', 'X549', 'X550', 'X551', 'X552', 'X553', 'X554', 'X555', 'X556', 'X557', 'X558', 'X559', 'X560', 'X561', 'X562', 'X563', 'X564', 'X565', 'X566', 'X567', 'X568', 'X569', 'X570', 'X571', 'X572', 'X573', 'X574', 'X575', 'X576', 'X577', 'X578', 'X579', 'X580', 'X581', 'X582', 'X583', 'X584', 'X585', 'X586', 'X587', 'X588', 'X589', 'X590', 'X591', 'X592', 'X593', 'X594', 'X595', 'X596', 'X597', 'X598', 'X599', 'X600', 'X601', 'X602', 'X603', 'X604', 'X605', 'X606', 'X607', 'X608', 'X609', 'X610', 'X611', 'X612', 'X613', 'X614', 'X615', 'X616', 'X617', 'X618', 'X619', 'X620', 'X621', 'X622', 'X623', 'X624', 'X625', 'X626', 'X627', 'X628', 'X629', 'X630', 'X631', 'X632', 'X633', 'X634', 'X635', 'X636', 'X637', 'X638', 'X639', 'X640', 'X641', 'X642', 'X643', 'X644', 'X645', 'X646', 'X647', 'X648', 'X649', 'X650', 'X651', 'X652', 'X653', 'X654', 'X655', 'X656', 'X657', 'X658', 'X659', 'X660', 'X661', 'X662', 'X663', 'X664', 'X665', 'X666', 'X667', 'X668', 'X669', 'X670', 'X671', 'X672', 'X673', 'X674', 'X675', 'X676', 'X677', 'X678', 'X679', 'X680', 'X681', 'X682', 'X683', 'X684', 'X685', 'X686', 'X687', 'X688', 'X689', 'X690', 'X691', 'X692', 'X693', 'X694', 'X695', 'X696', 'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X718', 'X719', 'X720', 'X721', 'X722', 'X723', 'X724', 'X725', 'X726', 'X727', 'X728', 'X729', 'X730', 'X731', 'X732', 'X733', 'X734', 'X735', 'X736', 'X737', 'X738', 'X739', 'X740', 'X741', 'X742', 'X743', 'X744', 'X745', 'X746', 'X747', 'X748', 'X749', 'X750', 'X751', 'X752', 'X753', 'X754', 'X755', 'X756', 'X757', 'X758', 'X759', 'X760', 'X761', 'X762', 'X763', 'X764', 'X765', 'X766', 'X767', 'X768', 'X769', 'X770', 'X771', 'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780', 'X781', 'X782', 'X783', 'X784', 'X785', 'X786', 'X787', 'X788', 'X789', 'X790', 'X791', 'X792', 'X793', 'X794', 'X795', 'X796', 'X797', 'X798', 'X799', 'X800', 'X801', 'X802', 'X803', 'X804', 'X805', 'X806', 'X807', 'X808', 'X809', 'X810', 'X811', 'X812', 'X813', 'X814', 'X815', 'X816', 'X817', 'X818', 'X819', 'X820', 'X821', 'X822', 'X823', 'X824', 'X825', 'X826', 'X827', 'X828', 'X829', 'X830', 'X831', 'X832', 'X833', 'X834', 'X835', 'X836', 'X837', 'X838', 'X839', 'X840', 'X841', 'X842', 'X843', 'X844', 'X845', 'X846', 'X847', 'X848', 'X849', 'X850', 'X851', 'X852', 'X853', 'X854', 'X855', 'X856', 'X857', 'X858', 'X859', 'X860', 'X861', 'X862', 'X863', 'X864', 'X865', 'X866', 'X867', 'X868', 'X869', 'X870', 'X871', 'X872', 'X873', 'X874', 'X875', 'X876', 'X877', 'X878', 'X879', 'X880', 'X881', 'X882', 'X883', 'X884', 'X885', 'X886', 'X887', 'X888', 'X889', 'X890', 'label']\nDEBUG: 'timestamp' column found and converted to datetime in train_full_raw.\nDEBUG: Resetting index 'ID' for test_full_raw. New columns: ['ID', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X107', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X121', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X149', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X188', 'X189', 'X190', 'X191', 'X192', 'X193', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X233', 'X234', 'X235', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X268', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X289', 'X290', 'X291', 'X292', 'X293', 'X294', 'X295', 'X296', 'X297', 'X298', 'X299', 'X300', 'X301', 'X302', 'X303', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X330', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X347', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X381', 'X382', 'X383', 'X384', 'X385', 'X386', 'X387', 'X388', 'X389', 'X390', 'X391', 'X392', 'X393', 'X394', 'X395', 'X396', 'X397', 'X398', 'X399', 'X400', 'X401', 'X402', 'X403', 'X404', 'X405', 'X406', 'X407', 'X408', 'X409', 'X410', 'X411', 'X412', 'X413', 'X414', 'X415', 'X416', 'X417', 'X418', 'X419', 'X420', 'X421', 'X422', 'X423', 'X424', 'X425', 'X426', 'X427', 'X428', 'X429', 'X430', 'X431', 'X432', 'X433', 'X434', 'X435', 'X436', 'X437', 'X438', 'X439', 'X440', 'X441', 'X442', 'X443', 'X444', 'X445', 'X446', 'X447', 'X448', 'X449', 'X450', 'X451', 'X452', 'X453', 'X454', 'X455', 'X456', 'X457', 'X458', 'X459', 'X460', 'X461', 'X462', 'X463', 'X464', 'X465', 'X466', 'X467', 'X468', 'X469', 'X470', 'X471', 'X472', 'X473', 'X474', 'X475', 'X476', 'X477', 'X478', 'X479', 'X480', 'X481', 'X482', 'X483', 'X484', 'X485', 'X486', 'X487', 'X488', 'X489', 'X490', 'X491', 'X492', 'X493', 'X494', 'X495', 'X496', 'X497', 'X498', 'X499', 'X500', 'X501', 'X502', 'X503', 'X504', 'X505', 'X506', 'X507', 'X508', 'X509', 'X510', 'X511', 'X512', 'X513', 'X514', 'X515', 'X516', 'X517', 'X518', 'X519', 'X520', 'X521', 'X522', 'X523', 'X524', 'X525', 'X526', 'X527', 'X528', 'X529', 'X530', 'X531', 'X532', 'X533', 'X534', 'X535', 'X536', 'X537', 'X538', 'X539', 'X540', 'X541', 'X542', 'X543', 'X544', 'X545', 'X546', 'X547', 'X548', 'X549', 'X550', 'X551', 'X552', 'X553', 'X554', 'X555', 'X556', 'X557', 'X558', 'X559', 'X560', 'X561', 'X562', 'X563', 'X564', 'X565', 'X566', 'X567', 'X568', 'X569', 'X570', 'X571', 'X572', 'X573', 'X574', 'X575', 'X576', 'X577', 'X578', 'X579', 'X580', 'X581', 'X582', 'X583', 'X584', 'X585', 'X586', 'X587', 'X588', 'X589', 'X590', 'X591', 'X592', 'X593', 'X594', 'X595', 'X596', 'X597', 'X598', 'X599', 'X600', 'X601', 'X602', 'X603', 'X604', 'X605', 'X606', 'X607', 'X608', 'X609', 'X610', 'X611', 'X612', 'X613', 'X614', 'X615', 'X616', 'X617', 'X618', 'X619', 'X620', 'X621', 'X622', 'X623', 'X624', 'X625', 'X626', 'X627', 'X628', 'X629', 'X630', 'X631', 'X632', 'X633', 'X634', 'X635', 'X636', 'X637', 'X638', 'X639', 'X640', 'X641', 'X642', 'X643', 'X644', 'X645', 'X646', 'X647', 'X648', 'X649', 'X650', 'X651', 'X652', 'X653', 'X654', 'X655', 'X656', 'X657', 'X658', 'X659', 'X660', 'X661', 'X662', 'X663', 'X664', 'X665', 'X666', 'X667', 'X668', 'X669', 'X670', 'X671', 'X672', 'X673', 'X674', 'X675', 'X676', 'X677', 'X678', 'X679', 'X680', 'X681', 'X682', 'X683', 'X684', 'X685', 'X686', 'X687', 'X688', 'X689', 'X690', 'X691', 'X692', 'X693', 'X694', 'X695', 'X696', 'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X718', 'X719', 'X720', 'X721', 'X722', 'X723', 'X724', 'X725', 'X726', 'X727', 'X728', 'X729', 'X730', 'X731', 'X732', 'X733', 'X734', 'X735', 'X736', 'X737', 'X738', 'X739', 'X740', 'X741', 'X742', 'X743', 'X744', 'X745', 'X746', 'X747', 'X748', 'X749', 'X750', 'X751', 'X752', 'X753', 'X754', 'X755', 'X756', 'X757', 'X758', 'X759', 'X760', 'X761', 'X762', 'X763', 'X764', 'X765', 'X766', 'X767', 'X768', 'X769', 'X770', 'X771', 'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780', 'X781', 'X782', 'X783', 'X784', 'X785', 'X786', 'X787', 'X788', 'X789', 'X790', 'X791', 'X792', 'X793', 'X794', 'X795', 'X796', 'X797', 'X798', 'X799', 'X800', 'X801', 'X802', 'X803', 'X804', 'X805', 'X806', 'X807', 'X808', 'X809', 'X810', 'X811', 'X812', 'X813', 'X814', 'X815', 'X816', 'X817', 'X818', 'X819', 'X820', 'X821', 'X822', 'X823', 'X824', 'X825', 'X826', 'X827', 'X828', 'X829', 'X830', 'X831', 'X832', 'X833', 'X834', 'X835', 'X836', 'X837', 'X838', 'X839', 'X840', 'X841', 'X842', 'X843', 'X844', 'X845', 'X846', 'X847', 'X848', 'X849', 'X850', 'X851', 'X852', 'X853', 'X854', 'X855', 'X856', 'X857', 'X858', 'X859', 'X860', 'X861', 'X862', 'X863', 'X864', 'X865', 'X866', 'X867', 'X868', 'X869', 'X870', 'X871', 'X872', 'X873', 'X874', 'X875', 'X876', 'X877', 'X878', 'X879', 'X880', 'X881', 'X882', 'X883', 'X884', 'X885', 'X886', 'X887', 'X888', 'X889', 'X890', 'label']\nDEBUG: Renamed 'index' to 'timestamp' for test_full_raw. New columns: ['timestamp', 'ID', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X72', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X93', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X107', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X121', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X149', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X188', 'X189', 'X190', 'X191', 'X192', 'X193', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X233', 'X234', 'X235', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X268', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X289', 'X290', 'X291', 'X292', 'X293', 'X294', 'X295', 'X296', 'X297', 'X298', 'X299', 'X300', 'X301', 'X302', 'X303', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X330', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X347', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X381', 'X382', 'X383', 'X384', 'X385', 'X386', 'X387', 'X388', 'X389', 'X390', 'X391', 'X392', 'X393', 'X394', 'X395', 'X396', 'X397', 'X398', 'X399', 'X400', 'X401', 'X402', 'X403', 'X404', 'X405', 'X406', 'X407', 'X408', 'X409', 'X410', 'X411', 'X412', 'X413', 'X414', 'X415', 'X416', 'X417', 'X418', 'X419', 'X420', 'X421', 'X422', 'X423', 'X424', 'X425', 'X426', 'X427', 'X428', 'X429', 'X430', 'X431', 'X432', 'X433', 'X434', 'X435', 'X436', 'X437', 'X438', 'X439', 'X440', 'X441', 'X442', 'X443', 'X444', 'X445', 'X446', 'X447', 'X448', 'X449', 'X450', 'X451', 'X452', 'X453', 'X454', 'X455', 'X456', 'X457', 'X458', 'X459', 'X460', 'X461', 'X462', 'X463', 'X464', 'X465', 'X466', 'X467', 'X468', 'X469', 'X470', 'X471', 'X472', 'X473', 'X474', 'X475', 'X476', 'X477', 'X478', 'X479', 'X480', 'X481', 'X482', 'X483', 'X484', 'X485', 'X486', 'X487', 'X488', 'X489', 'X490', 'X491', 'X492', 'X493', 'X494', 'X495', 'X496', 'X497', 'X498', 'X499', 'X500', 'X501', 'X502', 'X503', 'X504', 'X505', 'X506', 'X507', 'X508', 'X509', 'X510', 'X511', 'X512', 'X513', 'X514', 'X515', 'X516', 'X517', 'X518', 'X519', 'X520', 'X521', 'X522', 'X523', 'X524', 'X525', 'X526', 'X527', 'X528', 'X529', 'X530', 'X531', 'X532', 'X533', 'X534', 'X535', 'X536', 'X537', 'X538', 'X539', 'X540', 'X541', 'X542', 'X543', 'X544', 'X545', 'X546', 'X547', 'X548', 'X549', 'X550', 'X551', 'X552', 'X553', 'X554', 'X555', 'X556', 'X557', 'X558', 'X559', 'X560', 'X561', 'X562', 'X563', 'X564', 'X565', 'X566', 'X567', 'X568', 'X569', 'X570', 'X571', 'X572', 'X573', 'X574', 'X575', 'X576', 'X577', 'X578', 'X579', 'X580', 'X581', 'X582', 'X583', 'X584', 'X585', 'X586', 'X587', 'X588', 'X589', 'X590', 'X591', 'X592', 'X593', 'X594', 'X595', 'X596', 'X597', 'X598', 'X599', 'X600', 'X601', 'X602', 'X603', 'X604', 'X605', 'X606', 'X607', 'X608', 'X609', 'X610', 'X611', 'X612', 'X613', 'X614', 'X615', 'X616', 'X617', 'X618', 'X619', 'X620', 'X621', 'X622', 'X623', 'X624', 'X625', 'X626', 'X627', 'X628', 'X629', 'X630', 'X631', 'X632', 'X633', 'X634', 'X635', 'X636', 'X637', 'X638', 'X639', 'X640', 'X641', 'X642', 'X643', 'X644', 'X645', 'X646', 'X647', 'X648', 'X649', 'X650', 'X651', 'X652', 'X653', 'X654', 'X655', 'X656', 'X657', 'X658', 'X659', 'X660', 'X661', 'X662', 'X663', 'X664', 'X665', 'X666', 'X667', 'X668', 'X669', 'X670', 'X671', 'X672', 'X673', 'X674', 'X675', 'X676', 'X677', 'X678', 'X679', 'X680', 'X681', 'X682', 'X683', 'X684', 'X685', 'X686', 'X687', 'X688', 'X689', 'X690', 'X691', 'X692', 'X693', 'X694', 'X695', 'X696', 'X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717', 'X718', 'X719', 'X720', 'X721', 'X722', 'X723', 'X724', 'X725', 'X726', 'X727', 'X728', 'X729', 'X730', 'X731', 'X732', 'X733', 'X734', 'X735', 'X736', 'X737', 'X738', 'X739', 'X740', 'X741', 'X742', 'X743', 'X744', 'X745', 'X746', 'X747', 'X748', 'X749', 'X750', 'X751', 'X752', 'X753', 'X754', 'X755', 'X756', 'X757', 'X758', 'X759', 'X760', 'X761', 'X762', 'X763', 'X764', 'X765', 'X766', 'X767', 'X768', 'X769', 'X770', 'X771', 'X772', 'X773', 'X774', 'X775', 'X776', 'X777', 'X778', 'X779', 'X780', 'X781', 'X782', 'X783', 'X784', 'X785', 'X786', 'X787', 'X788', 'X789', 'X790', 'X791', 'X792', 'X793', 'X794', 'X795', 'X796', 'X797', 'X798', 'X799', 'X800', 'X801', 'X802', 'X803', 'X804', 'X805', 'X806', 'X807', 'X808', 'X809', 'X810', 'X811', 'X812', 'X813', 'X814', 'X815', 'X816', 'X817', 'X818', 'X819', 'X820', 'X821', 'X822', 'X823', 'X824', 'X825', 'X826', 'X827', 'X828', 'X829', 'X830', 'X831', 'X832', 'X833', 'X834', 'X835', 'X836', 'X837', 'X838', 'X839', 'X840', 'X841', 'X842', 'X843', 'X844', 'X845', 'X846', 'X847', 'X848', 'X849', 'X850', 'X851', 'X852', 'X853', 'X854', 'X855', 'X856', 'X857', 'X858', 'X859', 'X860', 'X861', 'X862', 'X863', 'X864', 'X865', 'X866', 'X867', 'X868', 'X869', 'X870', 'X871', 'X872', 'X873', 'X874', 'X875', 'X876', 'X877', 'X878', 'X879', 'X880', 'X881', 'X882', 'X883', 'X884', 'X885', 'X886', 'X887', 'X888', 'X889', 'X890', 'label']\nDEBUG: 'timestamp' column found and converted to datetime in test_full_raw.\n\nTrain shape: (525887, 897)\nTest shape: (538150, 898)\nStarting training pipeline...\nInitially selected 30 X_n features for direct Pandas load (due to memory constraints without Dask).\nSelecting columns from initial raw training data...\nMemory usage before optimization: 148.45 MB\nCleaning data...\nData cleaning applied.\nMemory usage after optimization: 78.24 MB\nCreating time-based features...\nTime-based features created. Current shape: 525887 rows, 67 columns\nSaving processed data to checkpoint: ./processed_train_data_checkpoint.parquet...\nCheckpoint saved.\nData shape after feature engineering: 525887 rows, 67 columns\nFeatures shape before final selection: 525887 rows, 65 columns\nTarget shape: 525887 rows\nSelecting top 100 features from 65 features...\nValidating data before final feature selection...\nFinal data shape for feature selection: (525887, 65)\n","output_type":"stream"}],"execution_count":null}]}